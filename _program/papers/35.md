---
layout: paper
title: "Spatial Action Maps for Mobile Manipulation"
invisible: true
---
*[Jimmy Wu](https://www.cs.princeton.edu/~jw60/), [Xingyuan Sun](https://people.csail.mit.edu/xingyuan/), [Andy Zeng](https://andyzeng.github.io), [Shuran Song](https://shurans.github.io), [Johnny Lee](http://johnnylee.net), [Szymon Rusinkiewicz](https://www.cs.princeton.edu/~smr/), [Thomas Funkhouser](https://www.cs.princeton.edu/~funk/)*
{: style="color:black; font-size: 120%; text-align: center;"}

<table width="40%"> <tr>
<td style="width: 20%; text-align: center;"><a href="http://www.roboticsproceedings.org/rss16/p035.pdf"><img src="{{ site.baseurl }}/images/paper_link.png"
width = "50"  height = "60"/> </a> </td>

<td style="width: 20%; text-align: center;"><a href="https://spatial-action-maps.cs.princeton.edu"><img src="{{ site.baseurl }}/images/video_link.png"
width = "50"  height = "60"/> </a> </td>

<td style="width: 20%; text-align: center;"><a href="https://spatial-action-maps.cs.princeton.edu"><img src="{{ site.baseurl }}/images/website_link.png"
width = "50"  height = "60"/> </a> </td>

<td style="width: 20%; text-align: center;"><a href="https://spatial-action-maps.cs.princeton.edu"><img src="{{ site.baseurl }}/images/software_link.png"
width = "50"  height = "60"/> </a> </td>

<td style="width: 20%; text-align: center;"><a href="nan"><img src="{{ site.baseurl }}/images/pheedloop_link.png"
width = "70"  height = "60"/> </a> </td>

</tr></table>

### Abstract
<html><p style="color:gray; font-size: 120%; text-align: justified;">
Typical end-to-end formulations for learning robotic navigation involve predicting a small set of steering command actions (e.g., step forward, turn left, turn right, etc.) from images of the current state (e.g., a bird's-eye view of a SLAM reconstruction). Instead, we show that it can be advantageous to learn with dense action representations defined in the same domain as the state. In this work, we present "spatial action maps," in which the set of possible actions is represented by a pixel map (aligned with the input image of the current state), where each pixel represents a local navigational endpoint at the corresponding scene location. Using ConvNets to infer spatial action maps from state images, action predictions are thereby spatially anchored on local visual features in the scene, enabling significantly faster learning of complex behaviors for mobile manipulation tasks with reinforcement learning. In our experiments, we task a robot with pushing objects to a goal location, and find that policies learned with spatial action maps achieve much better performance than traditional alternatives.
</p></html>

### Paper Reviews
<details><summary style="font-size:20px; color:#438BCA"><b> Review 1</b></summary>
<p style="color:gray; font-size: 120%; text-align: justified; white-space: pre-line">
This paper proposes an action representation (called ‘spatial action maps’) for robots learning to manipulate objects using deep reinforcement learning. This work is inspired by previous works using dense action representations. An agent is trained, using simulation, to push objects to a target location. While a standard algorithm is used for training (DDQN), the policy is represented using a Fully Convolutional neural net. Experimental results using a few baselines show some promise of the proposed approach.

The paper in general is well-written, and I was very excited when reading sections I and II. This excitement decreased from section III. For example, the reward function assumes that the distance between objects and the target location is known — which may not be the case in more complex scenarios. In addition and most importantly, the experimental setting is very toy-like environment. This environment assumes that the action representation is available in one image, which is a strong assumption in more complex  (real-world) environments. Last but not least, the experiments do not take into account strong baselines to compare against. It would have been interesting to see methods X, Y and Z with and without spatial action maps, where the use of spatial action maps makes a substantial difference — not only in a toy environment but in a more realistic one. 
</p> </details>

<details><summary style="font-size:20px; color:#438BCA"><b> Review 2</b></summary>
<p style="color:gray; font-size: 120%; text-align: justified; white-space: pre-line">
Originality

The authors propose a novel representation for actions in mobile manipulation settings and discuss several advantages of the proposed representation. They also present an empirical study that showcases the advantages of the approach. The action representation is indeed novel, to the best of my knowledge.

Quality

The results are impressive and the evaluation is thorough, especially the ablations. Figure 7 clearly shows the value of the proposed action space. Taken together, Tables 1,2,3, and 4 all show the effect of different kinds of ablations (using straight line paths instead of shortest paths in the movement primitives, using a fixed step size, etc). A consistent trend is that the design choices make the most difference in the Large Divider environment - this makes sense since the agent must navigate around the large divider in order to push all of the blocks successfully.

The section on limitations of the approach is an important inclusion and is appreciated. As for the supplementary website, the videos are useful to watch. The emergent behavior of grouping items against the wall and then sweeping multiple objects with long trajectory is indeed interesting, as discussed by the authors.

Clarity

Overall, the authors provide a very thorough explanation of their method, the state and action representations used, and their results. One minor point - the paper could use more details on how gradients are passed only through the state pixel corresponding to a selected action pixel.

Significance

Overall, this is a good paper that proposes a nice idea for an action space, and presents thorough validation that the proposed action space outperforms other choices.
</p> </details>

<table width="100%"><tr><td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/34"> <img src="{{ site.baseurl }}/images/previous_icon.png" width = "120"  height = "80"/> </a> </td>

<td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers"> <img src="{{ site.baseurl }}/images/overview_icon.png" width = "120"  height = "80"/> </a> </td> 

<td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/36"> <img src="{{ site.baseurl }}/images/next_icon.png" width = "100"  height = "80"/> </a> </td> 

</tr></table>

