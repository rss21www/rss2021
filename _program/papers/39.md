---
layout: paper
title: "Reinforcement Learning based Control of Imitative Policies for Near-Accident Driving"
invisible: true
---
*[Zhangjie Cao](https://caozhangjie.github.io/), [Erdem Biyik](http://stanford.edu/~ebiyik/), [Woodrow Z. Wang](https://www.linkedin.com/in/woodrow-wang-214043150/), [Allan Raventos](https://www.linkedin.com/in/allan-ravent%C3%B3s-19b962138/), [Adrien Gaidon](https://www.linkedin.com/in/adrien-gaidon-63ab2358/), [Guy Rosman](http://people.csail.mit.edu/rosman/), [Dorsa Sadigh](https://dorsa.fyi/)*
{: style="color:black; font-size: 120%; text-align: center;"}

<table width="30%"> <tr>
<td style="width: 20%; text-align: center;"><a href="http://www.roboticsproceedings.org/rss16/p039.pdf"><img src="{{ site.baseurl }}/images/paper_link.png"
width = "50"  height = "60"/> </a> </td>

<td style="width: 20%; text-align: center;"><a href="https://github.com/Stanford-ILIAD/CARLO"><img src="{{ site.baseurl }}/images/software_link.png"
width = "50"  height = "60"/> </a> </td>

<td style="width: 20%; text-align: center;"><a href="nan"><img src="{{ site.baseurl }}/images/pheedloop_link.png"
width = "70"  height = "60"/> </a> </td>

</tr></table>

### Abstract
<html><p style="color:gray; font-size: 120%; text-align: justified;">
Autonomous driving has achieved significant progress in recent years, but autonomous cars are still unable to tackle high-risk situations where a potential accident is likely. In such near-accident scenarios, even a minor change in the vehicle's actions may result in drastically different consequences. To avoid unsafe actions in near-accident scenarios, we need to fully explore the environment. However, reinforcement learning (RL) and imitation learning (IL), two widely-used policy learning methods, cannot model rapid phase transitions and are not scalable to fully cover all the states. To address driving in near-accident scenarios, we propose a hierarchical reinforcement and imitation learning (H-ReIL) approach that consists of low-level policies learned by IL for discrete driving modes, and a high-level policy learned by RL that switches between different driving modes. Our approach exploits the advantages of both IL and RL by integrating them into a unified learning framework. Experimental results and user studies suggest our approach can achieve higher efficiency and safety compared to other methods. Analyses of the policies demonstrate our high-level policy appropriately switches between different low-level policies in near-accident driving situations.
</p></html>

### Supplementary Video
<iframe width="700" height="400" src="https://www.youtube.com/embed/CY24zlC_HdI " frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<table width="100%"><tr><td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/38"> <img src="{{ site.baseurl }}/images/previous_icon.png" width = "120"  height = "90"/> </a> </td>

<td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers"> <img src="{{ site.baseurl }}/images/overview_icon.png" width = "120"  height = "90"/> </a> </td> 

<td style="width: 30%; text-align: center;"><a href="{{ site.baseurl }}/program/papers/40"> <img src="{{ site.baseurl }}/images/next_icon.png" width = "100"  height = "80"/> </a> </td> 

</tr></table>

