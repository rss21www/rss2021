Abstract,AgreementFile,AuthorLinks,AuthorNames,Files,PaperFile,PaperID,PaperOrder,PaperPheedLink,PaperSession,PaperTitle,PaperWebpage,RAND,Review1,Review1Consent,Review2,Review2Consent,Review3,Review3Consent,Review4,Review4Consent,ReviewAuthorConsent,StartTime,StopTime,SupplementarySoftware,SupplementaryVideo,Unnamed: 16,Unnamed: 18,Unnamed: 2,Youtube
"Models used in modern planning problems to simulate outcomes of real world action executions are becoming increasingly complex, ranging from simulators that do physics-based reasoning to precomputed analytical motion primitives. However, robots operating in the real world often face situations not modeled by these models before execution. This imperfect modeling can lead to highly suboptimal or even incomplete behavior during execution. In this paper, we propose CMAX an approach for interleaving planning and execution. CMAX adapts its planning strategy online during real-world execution to account for any discrepancies in dynamics during planning, without requiring updates to the dynamics of the model. This is achieved by biasing the planner away from transitions whose dynamics are discovered to be inaccurately modeled, thereby leading to robot behavior that tries to complete the task despite having an inaccurate model. We provide provable guarantees on the completeness and efficiency of the proposed planning and execution framework under specific assumptions on the model, for both small and large state spaces. Our approach CMAX is shown to be efficient empirically in simulated robotic tasks including 4D planar pushing, and in real robotic experiments using PR2 involving a 3D pick-and-place task where the mass of the object is incorrectly modeled, and a 7D arm planning task where one of the joints is not operational leading to discrepancy in dynamics",RSS2020_Author_Agreement.pdf,"[Anirudh Vemula](https://vvanirudh.github.io/), [Yash Oza](https://www.ri.cmu.edu/ri-people/yash-oza/), [J. Andrew Bagnell](http://robotwhisperer.org/), [Maxim Likhachev](http://www.cs.cmu.edu/~maxim/index.html)",Anirudh Vemula (Carnegie Mellon University)*; Yash Oza (CMU); J. Bagnell (Aurora Innovation); Maxim Likhachev (CMU),RSS2020_Author_Agreement.pdf (112293 bytes); camera_ready.pdf (8434450 bytes),camera_ready.pdf,1284.0,1.0,,1.0,Planning and Execution using Inaccurate Models with Provable Guarantees,https://vvanirudh.github.io/blog/cmax/,0.93485668449403,"This paper presents a method for planning in discrete state and action space when the transition function is initially unknown. Specifically, it focuses on fully observed problems where the transition function is deterministic but its model can be initially unknown/wrong and revealed only when the action is executed from the particular state. At each execution step, if the resulting next step differs from the estimated next step based on the transition function, then the cost of the state-action pair is changed to a very high value and adapt RTAA* (with neural network to estimate cost to go) is used to update the solution. Assuming that there is at least one solution with all state-action pairs in the solution having initially correct transition model, the proposed method provides a guarantee that the goal state is reached within a certain number of steps dependent on the size of the state space and the covering number of the set of state-action pairs known to have initially wrong transition function. Tests are performed on physical robot and compared with Q-learning and model-based learning.

The above problem can be viewed as a subclass of reinforcement learning problem (usually reinforcement learning assumes non-deterministic transition function). The purely planning solution (aside from the cost to go estimator) to the problem is interesting. 
 
I think the idea of ""removing"" state-action pairs with initially wrong dynamic model, by setting the associated cost to be high and replanning, is useful and interesting, though as the authors mentioned, is a bit limited. But, I think the method could have merit  for safe operation when the robot has minor fault, e.g., small accident might cause a mobile robot or UAV to have larger error at certain state.   

The paper is thorough in its analysis.

Comments on technical details:

*/ The guarantee requires the estimated cost-to-go to be admissible and consistent. It is not clear to me how the neural-network to estimate cost-to-go guarantees that this estimate is admissible and consistent.

*/ For problems whose underlying state space is continuous, the effect of \delta, which is a parameter of the guarantee, is likely to depend on the resolution of discretisation. It would be useful to elaborate this relation. 

*/ The problem description does not specify if the state space must be discrete. But, it is quite clear from the paper that this requirement  is required (at the very least by RTAA*). It would be useful to state this requirement in sec. II.

*/ In sec. III and IV before IVA, the set of incorrect state-action pairs is denoted as \chi* but in IVA, this notation becomes \chi. It doesn't matter the exact notation used but, please make it consistent.
",Agreement accepted,"This paper proposes an approach that interleaves planning and execution while representing and considering state-action pairs that are not possible to be used given discrepancies in dynamics during execution. The paper is well written and the proposed algorithms are well defined and formulated with provable guarantees on their completeness and efficiency.

Hyperspheres in KD tress are used to mark states which are discovered to lead to a discrepancy with respect to the robot motion. Then, during planning new considered states can be checked to be in the set of incorrect state-action pairs, and if so a much higher transition cost can be used to favor alternative, safer and more reliable, transitions.

Results are presented comparing against alternative learning methods and the proposed method shows superior results. 

Overall the paper presents useful techniques and results that highlight the potential of the approach. While the proposed algorithm follows a somewhat standard approach of interleaving planning and execution with on-line marking of invalid states, the paper provides a new formulation to the approach.

With respect to the results, since the proposed method is a discrete search method, it would be useful to compare it against other popular search-based techniques. For instance it would be more revealing to compare the method against a traditional on-line RRT implementation that would only sample states from valid regions of the state space, using the same techniques proposed in the paper to mark the invalid regions with hyperspheres around states with discrepancies. 

In terms of presentation, while the paper is well written, minor improvements could be considered in the submitted version of this paper. I find the notation X* for incorrect state-action pairs to be inconsistent with the use of the ‘*’ symbol to denote optimality in planning algorithms. The beginning of the Approach section is a bit repetitive, and perhaps having the related work section at the beginning of the paper as usual would make the reading more fluid.
",Agreement accepted,"The paper tackles an important and interesting problem in robotics where imperfect modeling of the underlying dynamics can lead to undesired behavior during execution. A typical approach in such setups is to update the dynamics of the model from observations. Instead, this work utilizes an online real-time planner and biases it away from transitions whose dynamics are discovered to be inaccurately modeled. The proposed method updates the cost function to penalize executing state-action pairs that are observed to be inaccurate. 

Function approximations are suggested - and guarantees are argued for reaching the goal under assumptions - to make the solution tractable in large state spaces. In particular, the algorithm maintains sets of hyper-spheres in the state space for each action, implemented via kd-trees, to indicate the region of unknown dynamics. The parameters of the cost-to-go function approximators are updated after placing a hypersphere when a discrepancy in dynamics was observed.

Overall, the paper’s approach of updating the cost function instead of trying to learn a dynamics model has desirable features. It is desirable to have a method with guarantees under the imperfect modeling setup considered. This assumes that a solution always exists at least a certain distance away from the imperfectly modeled part of the space. 

There are some concerns, however, regarding the proposed solution and manuscript. In most scenarios, using a binary categorization of imperfect/perfect modeling in terms of the dynamics is limiting. This makes the formulation rather restrictive. In most problems, there is some level of error in the modeling of the dynamics through the entire state space.  On a similar note, the paper assumes that in all cases the unknown regions can be perfectly and immediately detected, which may not be realistic in many setups. Furthermore, the approach also raises safety concerns, as an unsafe region must be first visited in order to be detected. 

The paper argues that its contribution includes both the idea of dealing with imperfectly modeled dynamics via updates in the underlying cost function as well as the application of function approximators to deal with large state spaces. For the first part, the paper applies in a rather straightforward manner the RTAA* algorithm, which belongs in a family of search methods developed for problems like these, where there are errors in the underlying model. The novelty of the paper regarding this aspect is rather limited. The paper’s contribution lies primarily on the second component and the function approximation, which depends critically on the underlying smoothness assumptions about the state space. 

There are also some gaps in the experimental evaluation. In the context of the experiment involving “3D pick and Place with a heavy object”: No quantitative results are reported, which is disappointing. Planning is performed for the end-effector location in a discretized 3D state-space (20 bins in each dimension).  No function approximation is utilized here given this discretization, which is however quite coarse and results in a motion that is not smooth. Would the function approximation work in this setup? It is important to understand the significance of this problem, how well does it match the assumptions of this work  and the repeatability of the solution.

The “7D arm planning with a non-operational joint” case seems rather engineered so as to satisfy the requirement of a binary classification of the state space. It is also similarly coarsely discretized and there are no sufficient quantitative results.  The algorithm seems rather sensitive to the parameters depicted in Figure 5.

For the icy states scenario (Table 2), the approach does not seem to outperform alternatives or provide any desirable tradeoff. 

In terms of Table 1 related to the “simulated planar pushing” experiments: What data points are used for the comparison learning-based approaches?  
",,,,,07/14 15:00,07/14 17:00,https://github.com/vvanirudh/CMAX,https://youtu.be/eQmAeWIhjO8,,,1.0,S7tLYBgZzUM
"Truly intelligent agents need to capture the interplay of all their senses to build a rich physical understanding of their world.  In  robotics,  we  have  seen  tremendous  progress  in  using visual  and  tactile  perception;  however  we  have  often  ignored  a key  sense:  sound.  This  is  primarily  due  to  lack  of  data  that captures  the  interplay  of  action  and  sound.  In  this  work,  we perform  the  first  large-scale  study  of  the  interactions  between sound  and  robotic  action.  To  do  this,  we  create  the  largest available sound-action-vision dataset with 15,000 interactions on60 objects using our robotic platform Tilt-Bot. By tilting objects and  allowing  them  to  crash  into  the  walls  of  a  robotic  tray,  we collect  rich  four-channel  audio  information.  Using  this  data,  we explore  the  synergies  between  sound  and  action,  and  present three   key   insights.   First,   sound   is   indicative   of   fine-grained object  class  information,  e.g.,  sound  can  differentiate  a  metal screwdriver  from  a  metal  wrench.  Second,  sound  also  contains information  about  the  causal  effects  of  an  action,  i.e.  given  the sound  produced,  we  can  predict  what  action  was  applied  on the  object.  Finally,  object  representations  derived  from  audio embeddings  are  indicative  of  implicit  physical  properties.  We demonstrate that on previously unseen objects, audio embeddings generated through interactions can predict forward models 24%better  than  passive  visual  embeddings.",RSS2020_Author_Agreement.pdf,Dhiraj Gandhi (Carnegie Mellon University)*; Abhinav Gupta (Carnegie Mellon University); Lerrel Pinto (NYU/Berkeley),Dhiraj Gandhi (Carnegie Mellon University)*; Abhinav Gupta (Carnegie Mellon University); Lerrel Pinto (NYU/Berkeley),RSS2020_Author_Agreement.pdf (378966 bytes); _RSS_2020_SoundAction_2_compressed.pdf (542597 bytes),_RSS_2020_SoundAction_2_compressed.pdf,1174.0,2.0,,1.0,Swoosh! Rattle! Thump! - Actions that Sound,[Not Answered],0.761728241077182,"This paper provides insights on the importance of using sound for object classification, inverse and forward model predictions. To do this, the paper first present the data collection procedure used to sound-action-vision dataset available with 15,000 interactions on over 60 objects using a Sawyer robot. Then this dataset is used to explore explore the synergy between sound and action to gain insight into what sound can be used for. The paper reports on a number of evaluations including 1) object classification, 2) inverse-model learning, 3) multi-task audio embedding learning, 4) few shot learning, and 5) forward model learning.

Pros:
* The insights provided in the paper are very useful for the research community.
* Relatively thorough experimental evaluation.
* The authors mentioned they are planning to open-source the dataset.
* The paper is easy to read.

Cons:
* The distributions of the objects are not described in the paper. Although some of the objects used for data collection are shown in Fig 4, but this is quite an important aspect of the paper that needs to be described in more details. Specifically, these distributions should be reported for both training and test datasets:
  1) Object distribution based on MATERIAL (e.g. metal, plastic, glass, ...).
  2) Object distribution based on SHAPE (e.g. small, medium, large).
  3) Object distribution based on WEIGHT (e.g. light, medium, heavy).
  4) Object distribution based on HARDNESS (e.g. soft, firm, hard).
It would be also interesting to report on the accuracy based on these distributions. This might provide more insights on the effectiveness or in-effectiveness of using sound based on the object category.

* Some claims in the paper are too generic and not well-supported and should be tuned-down. For example:
(1) In Section IV-B: "" This shows that audio data contains fine-grained information about objects.  Although ...,  our results show for the first time (to our knowledge) that audio information generated through action gives instance-level information like screwdriver, scissor, tennis ball etc"" Though I believe this claim for the limited object dataset considered in this paper, one should avoid over-generalizing the results. 

(2) Although the collected dataset is useful for research, it is not a representative of real-life scenarios a robot may face. The proposed setup makes a number of exaggerated movements that produces loud noise. This plays an important role in improving the results in using sound. The paper briefly reports an experiment along this line in Section IV-G, however, this section is not well described missing critical details such objects being used and more thorough quantitative experiments.

Suggestions:
- The paper provides comparison with a visual-only baseline. I am wondering how much one may gain by using audio-vision vs force-vision. The latter refers to the case where one may use to force/torque value from contact to infer information.

- Kids used toys with exaggerated noise to build their sound-action synergy. But, when they grow they could use the built skill during childhood in their adulthood. I was wondering if you could use the knowledge gained by tilt-bot robot and apply it to the pushing experiment?

Further comments:
- Please provide details on the action distribution used to tilt the box.
- Section IV-C: It seems the visual model does a better generalization from seen to unseen objects compared to an audio model. Please elaborate this in the paper.
- Fig 6: Some of the action prediction in Fig 6 are pretty off. This need to be discussed in the paper.
- Fig 2 is missing in the paper.
- What if you tilt the box but the object doesn't hit the wall (though does a small movement)? Do you still use this data?
- There is a typo in the caption of Fig 6: ""For each for images, ...""
- Table 1: What does the arrow up and down correspond to? I assume it means whether a higher or lower value is desired. Please clarify this in the caption.",Agreement accepted,"Overall comments: The submission is highly original. I do not know of any other work aiming to provide an open-sourced dataset that includes action, vision and sound. I believe such a dataset would be useful to the robotics community, as well as other communities more specifically interested in sound.

There is room for improvement in the quality of the submission on the experimental side. As a dataset paper, I think it is very strong. However, the authors do not provide convincing experiments to show that sound is a useful perceptual input above vision. I give several specific points of (hopefully constructive) criticism on this below.

Nitpicky writing points for the introduction
“A truly intelligent agent would need to capture the interplay of all the three senses to build a physical understanding of the world.”
--this really is not obviously true. Many truly intelligent humans and animals lack at least one sense, sometimes two, and are still capable of understanding the physical world. Consider removing this sentence, as I don’t think it is needed to still underscore the importance of sound as a sense.

Consider splitting related work into two sections: one that is about datasets (and prior datasets), and one about how sound can be successfully leveraged for different tasks. As it currently reads, these two are conflated, as previous methods are mentioned that also introduce both datasets and applications, but their differences are only mentioned in the dataset axis but not the applications axis. “Multi-modal learning with sound” as a section should only compare to the application axis of previous work - the dataset section can separately compare to datasets in previous work.

Constructive criticism for experiments:
General criticism: not nearly enough detail is provided in the paper to be able to replicate any of the experiments. Architectures for the models are not described, there is no mention of how the models are trained, and hyperparameters are ignored. As it stands, this paper could not be reproduced by a reader.

Fine-grained audio classification:
--The authors state that they create a dataset by taking 80% of their data and testing on a held out set of 20% of their data, for each object. They provide a random embedding baseline, but not the standard “nearest neighbours in pixel space” baseline. Without this nearest neighbours baseline (in pixel space), it is difficult to get a sense of how difficult this problem is. This is a common baseline for other perceptual datasets.

Inverse model learning
--In Figure 6, it would be helpful to see for these same examples what the predictions from the “vision-only” model are. In addition, how much sound is being provided as input? Is it from the full 4 seconds? If so, it seems that a fair comparison might be to provide the full 4 second video from the visual domain, which I would expect could perform better than using the sound.

Multi-task audio embedding learning 
--The authors first say that “training is performed on set A objects, while testing is done on set A held-out interactions and unseen set B objects”. However, they then go on to say that they see performance improvements from 73.8% to 78.6% when training on the set A objects only, and 76.1 to 79.5 when training on both set A and set B objects. If you were training on set A and set B objects, what was being tested?
--for the inverse-model learning, is joint learning performed on both set A and set B? If so, it’s not surprising that you see improvement for set B regression but not set A. This needs to be stated more clearly in the manuscript. Similarly, when comparing to the visual baseline, it seems like the authors are just reporting the previous number that did not use joint training on both tasks. If you would like to compare to this baseline, it should also be jointly trained with the classification task.
--Figure 8 could be improved or possibly removed. Perhaps color coding by physical similarity would help the visualization. At the moment, it’s not clear that physically different objects are well separated in the embedding space since the colors don’t represent a gradient along any property.

Few-shot learning
--The authors say they use a nearest neighbours method to perform few-shot learning. How many neighbours are used?
--The authors show that using a ResNet embedding does better than the audio embeddings by a wide margin. Was any fine-tuning done to the ResNet?

Forward model learning. 
--Authors say that audio embedding is based on a random interaction. Is this random interaction *significantly different* from the action that is then input to the forward model for prediction? The figure suggests these are in the same general direction, and it would be helpful to know how the accuracy varies as a function of the similarity between the “probe” action and the predicted action.
--I am particularly concerned about this because of how badly the ResNet performs, even though it did a *much* better job at few-shot object classification in the previous section. Since the oracle is doing very well, and is defined as using the true “class” label, I would have expected that the method which most accurately captures the class would perform best in this task. However, the ResNet, which performs much better than the audio embeddings, is somehow worse for forward model prediction. Could the authors comment on why this is?

Overall, I think this dataset will be a nice contribution to the community, but the paper as it is currently written does not perform clear experiments to show that audio is necessary above and beyond visual information. If the authors can address the criticisms above, I believe the paper could be improved and would help spur the adoption of this dataset within the community.",Agreement accepted,"This is a very nice, very clear paper. It makes a compelling case for the value of the provided dataset for research, the value of using joint sensory inputs in embeddings, and that achieving some degree of generalization from audio data is actually attainable without a large amount of data/modeling/engineering. This last point is a bit surprising, and opens up new research directions (how much generalization? can we use 'pretrained' audio embeddings?",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,https://dhiraj100892.github.io/sound_with_action/,[Not Answered],,,2.0,CLubmYsZsPM
"In this paper, we propose a deep convolutional recurrent neural network that predicts action sequences for task and motion planning (TAMP) from an initial scene image. Typical TAMP problems are formalized by combining reasoning on a symbolic, discrete level (e.g. first-order logic) with continuous motion planning such as nonlinear trajectory optimization.Due to the great combinatorial complexity of possible discrete action sequences, a large number of optimization/motion planning problems have to be solved to find a solution, which limits the scalability of these approaches.To circumvent this combinatorial complexity, we develop a neural network which, based on an initial image of the scene, directly predicts promising discrete action sequences such that ideally only one motion planning problem has to be solved to find a solution to the overall TAMP problem.A key aspect is that our method generalizes to scenes with many and varying number of objects, although being trained on only two objects at a time.This is possible by encoding the objects of the scene in images as input to the neural network, instead of a fixed feature vector.Results show runtime improvements of several magnitudes.",release.pdf,"[Danny Driess](https://dannydriess.github.io/)Jung-Su Ha,  [Marc Toussaint](http://www.marc-toussaint.net/)","Danny Driess (Machine Learning and Robotics Lab, University of Stuttgart)*; Jung-Su Ha (); Marc Toussaint ()",rss2020.pdf (5274789 bytes); release.pdf (90256 bytes),rss2020.pdf,1313.0,3.0,,1.0,Deep Visual Reasoning: Learning to Predict Action Sequences for Task and Motion Planning from an Initial Scene Image,,0.04670143950020901,"This paper addresses an important issue in manipulation planning, namely the fact that there's a combinatorial explosion as a function of plan length.  This is traditionally addressed by a heuristic function and there is a growing body of work on learning heuristics for manipulation planning (aka TAMP).  In this paper, the key novelty is formulating this learning problem as learning a convolutional RNN based on an image representation of the start state (a depth map with separate channels for object masks).  Care has been taken with the learning setting so that the learned heuristic generalizes over number of objects in the scene, something which has been problematic for some earlier approaches.

One observation is that the learning is being done with a very large data set of plans (for 30,000 scenes of two objects).  Presumably because the appearances have to span the range of placements in the workspace and the relationship of the two objects.  I note that in a more realistic setting, e.g. a mobile manipulation robot in a household, might require a prohibitive number of images to ""span"" it's operating space.  In any case, this training is a substantial investment, so the question is does it pay back?  That is, how well does it generalize?  The authors show that adding other objects to the scene does not have a substantial impact on performance and they test for multiple goal locations.  But all of these tasks have a very similar structure, i.e. the number of solution sequences (ignoring the discrete grasp choice) is relatively small, I believe (if it were a single arm there's only up to 3 copies of [grasp, place]).  Most of the combinatorics comes from the choice of grasps (and arm).  The paper shows that the vast majority of sequences are infeasible - can you give us some insight as to why?  Is it due to kinematic limits?  Presumably not due to motion planning failures in that simple setting. What is the network learning?  

The paper stresses that the approach mostly does away with search altogether.  This seems an overly strong claim based on the limited testing.  Yes, in their experiments there is little search needed, but the setting is limited.  I would recommend toning down the claims to a more realistic level.

Clarifications:
1. I'm assuming that the odd length sequences involve grasps with different arms, so the arm is another discrete parameter in the action.  This also explains why there are 8 sequences of length 2 - 2 arms with 4 grasps each?
2. The initial handover illustration in Figure 1 does not seem to fit into the class of Fig 3, unless you have different grasps for each arm on the objects?
3. When counting sequences, is the assumption that only two objects can matter (the goal object and the one blocking the target)?
4. In Table I, is this the size of the search space or the number of solutions?  The title of the table makes it sound as if it's the number of solutions, but huge numbers of solutions would argue that the problem is easy.
5. You need a ""perfect"" object detector as part of the framework; you should make it clear, especially when comparing to other methods for planning for image input.  You are using images as a flexible representation for state, not really addressing realistic sensor-based manipulation.
6. Clarify the discussion on the relation to Q-functions.  There's no uncertainty in action or effects being modeled, right?  So, presumably it's a POMDP because the discrete actions are only partially specified?
",Agreement accepted,"The paper is well written and presents an interesting variation of prior TAMP heuristic learning methods. Rather than learning feasibility of actions, this approach learns whether an action leads to the goal, and uses this as a task-level search heuristic instead. The learned model uses an image-space representation of the scene along with a clever parameterization of the action, manipulated object, and goal, which allows the method to generalize to arbitrary numbers of objects.

The experiments are clear and show a significant benefit to using the proposed approach, but involve rather limited object-object interactions (cuboids and cylinders) in toy rearrangement scenarios. It is not hard to generate heuristics manually in this case, as all the images are top-down and all the objects can be grasped from top-down with very little interaction. I can imagine the approach breaking down when, for example, the goal requires multiple objects to be packed tightly together. The paper could be improved by more discussion about the limitations of learning.

The real-robot experiments are not very illuminating, since real images are not being used. Basically, this is equivalent to a playback of a plan generated offline. It would be helpful to discuss how the approach could be used with real images.

Minor comments:
- ""loosing"" => ""losing""
- Fig 6 should have a legend, as it is not clear what bars are from which comparison group (especially if printed in B&W).",Agreement accepted,"This paper presents an interesting solution to overcome the exponential increase of computational complexity of TAMP approaches for long sequence lengths and large numbers of objects. 

The proposed method uses a recurrent neural network to predict sequences of high-level actions given an image of the scene at the first time-step and the goal. To generate the training data for the neural network, a large number of scenes and goal-states are generated programmatically and the corresponding problems are solved using an existing state-of-the-art TAMP approach.
While the dataset generation is offline, allowing for significantly larger computational budgets than in the case of online-planning, it might still require unreasonable amounts of computation to generate plans in complex, real-world scenarios -- even if computation is offline. 

The authors should clarify whether physics simulation was used when running the quantitative evaluations or if only kinematics were considered. This would be especially interesting to know with regards to the generalization experiments with cylinders. To justify the claim of generalization to slightly different geometries, it would be good if the authors could add a real-world experiment with cylinders.

It would be good if the authors could also discuss how the presented framework could be extended to objects with more complex shapes, without dramatically increasing the required amount of computation.
",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,https://youtu.be/i8yyEbbvoEk,,,3.0,hoKA9csoJNU
"Most current methods for learning from demonstrations assume that those demonstrations alone are sufficient to learn the underlying task. This is often untrue, especially if extra safety specifications exist which were not present in the original demonstrations. In this paper, we allow an expert to elaborate on their original demonstration with additional specification information using linear temporal logic (LTL). Our system converts LTL specifications into a differentiable loss. This loss is then used to learn a dynamic movement primitive that satisfies the underlying specification, while remaining close to the original demonstration. Further, by leveraging adversarial training, our system learns to robustly satisfy the given LTL specification on unseen inputs, not just those seen in training. We show our method is expressive enough to work across a variety of common movement specification patterns such as obstacle avoidance, patrolling, keeping steady, and speed limitation. In addition, we show our system can modify a base demonstration with complex specifications by incrementally composing multiple simpler specifications. We also implement our system on a PR-2 robot to show how a demonstrator can start with an initial (sub-optimal) demonstration, then interactively improve task success by including additional specifications enforced with our differentiable LTL loss.",RSS2020_Author_Agreement.pdf,"[Craig Innes](http://www.craiginnes.com/), [Subramanian Ramamoorthy](http://rad.inf.ed.ac.uk/)",Craig Innes (University of Edinburgh)*; Subramanian Ramamoorthy (University of Edinburgh),RSS_2020_Camera_Ready.pdf (5618147 bytes); RSS2020_Author_Agreement.pdf (263244 bytes),RSS_2020_Camera_Ready.pdf,44.0,4.0,,1.0,Elaborating on Learned Demonstrations with Temporal Logic Specifications,https://sites.google.com/view/ltl-dmp-rss-2020/,0.36634574915249496,"•	Originality: The paper makes an original connection between multiple existing work, in particular, metrics for satisfiability of propositional logic formulas and signal temporal logic formulas, learning DMPs from demonstrations using neural networks, learning from a combination of demonstrations and high-level specifications, and robustification using adversarial training. The authors do a good job in citing the papers on which they have built their work.
•	Quality: The overall quality is satisfactory. However, the writing lacks the expected technicality and formality at many instances. There are inconsistencies in the writing such as switching between using a term and its acronym, changing the order of arguments of an operator, missing notation definition, and some typos.
•	Clarity: The overall writing, the description of the algorithm, and the presentation of the results are clear.
•	Significance: average
The individual components of the proposed framework are borrowed from existing work. Nonetheless, integrating these components together may be interesting for the robotics community.
The work does not have theoretical contribution and its empirical contributions could be much stronger had the authors compared their work with existing algorithms, for example, the inverse reinforcement learning algorithm of reference 26.
•	Miscellaneous:
o	Figure 1 is not referred to.
o	There is an overuse of “our” in the paper.
o	It is better to use separate variable names for initial condition and canonical system.
o	It is not clear whether there is an assumption on the length of the demonstrations being the same.
o	Define variables ‘T_i’ in equation 5.
o	The loss function is almost everywhere differentiable.
o	It is worth mentioning that the proposed metric can only evaluate finite-horizon tasks. For example, a formula such as “always eventually an event” requires evaluation over an infinite trajectory and hence, cannot be evaluated.
o	The constraint loss definition in equation 8 is confusing for the “until” operator and its negation (the commas near “or” operators).
o	The soft approximations of “max” and “min” operator are also required in equation 7.
o	Algorithm 1 also takes ‘\gamma’, ‘\eta’, number of epochs, and number of iterations as inputs.
o	There should be a discussion on how the hyperparameters are selected. Also, include the values of all hyperparameters.
o	There is no axis label in figure 3.
o	The difference in loss shown in figure 5 (imitation) for simple training and adversarial training is negligible. It may need to be averaged over multiple runs.
o	Include the variance of the numerical results as well (in the tables).
",,"The paper is well written and structured.  The exposition and problem statement are clear.  However, there is a serious issue regarding the originality of one of the main claimed contributions, the loss function.

The paper proposes a loss function for LTL specifications over finite traces similar to robustness to STL and MTL.  However, a very similar reward function was already proposed in [1] in a reinforcement learning setting, and with the softmax approximation in [2].  The proposed loss function is just a negated and capped version of the them.  Moreover, there are many recent approaches to define differentiable objective functions in the literature [3, 4, 5], most for STL.  However, by the same mechanism employed in this paper and [1], these can be used for LTL as well.

The softmax approximation used in the paper does not guarantee satisfaction of the specification, and for optimization the parameter of the $\gamma$ can not be taken arbitrary close to zero without incurring severe numerical issues.  To alleviate the approximation issue, some methods precompute a slack value that (positive lower bounds) for the robustness that depends on the specification and $\gamma$ parameter.

Another issue with the technical approach is with respect to ensuring robustness of solutions to bounded-norm perturbations.  One result regarding time-robustness for STL is that any perturbation smaller in norm than the robustness score is guaranteed to satisfy the specification.  The proposed solution, on the other hand, cannot guarantee robustness, unless the perturbations are similar to the sampled ones (Alg.1, line 5).  This also seems a limitation that should be addressed.

Lastly, most methods for control problems with STL consider optimization-based approaches.  While their goals are different from the one in this paper (learning from demonstration with side TL information), the technical aspects are similar.  The paper cites relevant STL/MTL robustness work ([5] and [6] in the paper), but not the ones that actually use the quantitative robustness in optimization problems, see [3, 4, 5, 6] below.  The paper should include comments with regard to relevant literature.

[1] X. Li, C.-I., Vasile, C. Belta, Reinforcement Learning with Temporal Logic Rewards, IEEE/RSJ International Conference on Intelligent Robots and Systems, Vancouver, BC, Canada, 2017.

[2] X. Li, Y. Ma, C. Belta, A Policy Search Method For Temporal Logic Specified Reinforcement Learning Tasks, 2018 American Control Conference (ACC), Wisconsin, Milwaukee, USA, 2018.

[3] L. Lindemann, D. V. Dimarogonas, Robust control for signal temporal logic specifications using discrete average space robustness, Automatica, vol. 101, pp. 377–387, 2019.

[4] N. Mehdipour, C.-I. Vasile, C. Belta, Arithmetic-Geometric Mean Robustness for Control from Signal Temporal Logic Specifications, American Control Conference (ACC), 2019.

[5] T. Akazaki, I. Hasuo, Time robustness in MTL and expressivity in hybrid system falsification,” in International Conference on Computer Aided Verification. Springer, 2015, pp. 356–374.

[6] S. Farahani, V. Raman, R. M. Murray, Robust Model Predictive Control for Signal Temporal Logic Synthesis, ADHS 2015.",,"This paper addresses a failing of the traditional learning from demonstration (LfD) paradigm, in which a human demonstrates a skill once or a few times and then the robot learns to generalize the skill.  The paper observes that often there are a set of other constraints that apply implicitly even if they are not obviously part of the demonstration.  Since these constraints are both subtle and important, it may take a large number of demonstrations to learn them accurately.  The contribution is to represent an LTL specification in a differentiable form and combine it with a conventional LfD framework so that the robot can learn a representation of the skill that obeys the specification with maximum likelihood when generalizing.

The paper is well organized and clearly written.  The paper effectively leverages and merges the work from the authors and others into a novel whole.

LTL has seemingly become a very popular in robotics as a means of expressing specifications or restrictions on robot behavior.  The trouble with learning skills subject to hard constraints is that it forces us to solve challenging constrained optimization problems.  The paper's contribution transforms this into an unconstrained, multiobjective optimization, which is much easier to solve.

The paper uses adversarial learning to increase the robustness of the skills when generalizing to novel configurations.  This feature particularly helps improve compliance with the specifications.

One nagging question in my mind is that since the specification was turned into a soft constrained, it is no longer exactly satisfied.  Can this effect be empirically quantified?  Does it effectively promote solutions on the boundary of the specification space?  Does it produce many solutions that just barely meet the specification, or just barely fail to satisfy it?  For many constraints, like not tipping a cup, the soft constraint is good enough.  For some others, like touching an object, could it be insufficient?

Regarding the touching of objects, the paper writes specifications like EVENTUALLY p_xyz = x_i,3.  That is, the end-effector pose eventually visits the green cube.  However, this does not take into account the trajectory taken to reach that pose or the inverse kinematic solution.  That creates the likelihood that the robot's end-effector or elbow will collide with the green cube or some other object before reaching the specified configuration.  Should we expect that in a more realistic scenario, the specification would include such restrictions as well?

The method is validated on a simple 2D problem in simulation and also on a real PR2 robot.  There is definitely opportunity to do a more exhaustive and compelling validation of the method that would look at other metrics besides loss and end-effector trajectories in 6D.  For instance, how often was the specification violated?  How close to meeting the specification was the trajectory on average (maybe use a signed distance field)?  How much longer/more costly was the generated trajectory compared to baseline methods?  I don't really find loss to be a particularly compelling metric because it stands to reason that if you optimize something explicitly, the loss function for that thing will be smaller.  If there were one thing I could change about this paper, it would be to use more application-relevant metrics for the results.

Nevertheless, the method is novel and interesting and relevant to robotics.
",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,https://github.com/craigiedon/ltl_diff,https://www.youtube.com/watch?v=Te989To-0Rw,,,4.0,ZqIsY5Y2cOg
"A theoretically complete solution to the optimal Non-revisiting Coverage Path Planning (NCPP) problem of any arbitrarily-shaped object with a non-redundant manipulator is proposed in this work. Given topological graphs of surface cells corresponding to feasible and continuous manipulator configurations, the scheme is aimed at ensuring optimality with respect to the number of surface discontinuities, and extends the existing provable solution attained for simply-connected configuration cell topologies to any arbitrary shape. This is typically classified through their genus, or the number of ""holes"" which appear increasingly as configurations are further constrained with the introduction of additional metrics for the task at hand, e.g., manipulability thresholds, clearance from obstacles, end-effector orientations, tooling force/torque magnitudes, etc. The novel contribution of this paper is to show that no matter what the resulting topological shapes from such quality cell constraints may be, the graph is finitely solvable, and a multi-stage iterative solver is designed to find all such optimal solutions. ",yt002.pdf,Tong Yang (Zhejiang University)*; Jaime Valls Miro (University of Technology Sydney); Yue Wang (Zhejiang University); Rong Xiong (Zhejiang University),Tong Yang (Zhejiang University)*; Jaime Valls Miro (University of Technology Sydney); Yue Wang (Zhejiang University); Rong Xiong (Zhejiang University),min_removal_RSS.pdf (4726069 bytes); yt002.pdf (371029 bytes),min_removal_RSS.pdf,1332.0,5.0,,1.0,Non-revisiting Coverage Task with Minimal Discontinuities for Non-redundant Manipulators,,0.193273692670221,"
Contribution:

The main contribution of this paper lies in the detailed analysis on ways to convert genus 1, 2, n continuous sets into genus 0 sets. The paper shows that there is a finite number of ways of doing so, even though the number is exponential to the number of edges and sets. The analysis of converting genus 1 set into genus 0 sets is the most interesting and significant one while the cases of decomposing sets with genus 2 & n are straightforward. 

Quality: 

While the analyses provided in this paper is intriguing, the end results are not very exiting as the proposed method is essentially a brute force method and has the same time complexity as integer programming. 

The results shown in the figures 2, 10~13 are interesting and has practical significance but they do not seem to connect well with the analyses of the proposed method. The cutting paths connecting the ""holes"" to the outer boundary do not seem to play any important role in these examples. The holes seem to be simply removed because they are covered by other sets. The experiments also provide no comparison and do not report any statistical information about the proposed method. Decompositions from these examples are also not demonstrated.

Clarity:

The paper have much room for improvement in terms of the presentation. 
Several key terms are used without (clear) definition, including U_m in Eq. 6, and ""cutting paths"".

All images are very small and the tiny text are not really legible. Many figures, such as fig 3, fig 6, fig 7 are not well explained. 

Significance: 

This work is significant as the first work that deals with more practical and complex surfaces. The case analyses investigated in this paper are of theoretical interest. However, the proposed method based on these analyses did not seem to provide any practical advantages over integer programming based methods. The paper should provide more comprehensive experiments to demonstrate the benefits of converting high genus cells into genus 0 cells. 
",Agreement accepted,"This paper presents a theoretical and rigorous treatment of NCPP problem. I hope that the authors did some work to provide more insight and intuition before delving into mathematical rigor. It was not an easy read, and requires a few repeats. One of the reasons why I had a difficulty was that even though provided illustrations are useful, but do not provide clear legends. However, I liked the mathematical rigor and several interesting benchmarking results.",,"This paper focuses on achieving full non-revisiting coverage of the surface of an object with non-redundant manipulator (non-revisiting coverage path planning - NCPP). The objective is for the end effector to cover the entire space while minimizing the number of reconfigurations (lift-offs) where the arm breaks contact with the surface, since lift-offs would be suboptimal in most of the applications. Prior work in this area tackled object surfaces with simply-connected topologies. This paper leverages those results by decomposing non-simply-connected topologies into simply-connected topologies. 

Representing the surface as a topological graph composed of cells, the paper proposes an iterative algorithm that transforms higher-genus cells into lower-genus cells. This method is performed inductively, reducing a genus-n cell into two subcells each with a genus no more than n-1, until the base case of a genus-one cell (Sec VII). Given a genus-one cell, the surface has one hole with an inner and outer component. Sec VI focuses on two methods: (1) treating the inner and outer parts as independent problems which can be solved for because each part is a simply-connected cell or (2) transforming the genus-one cell into a simply connected cell (thus removing the hole) and then further dividing the cell. 

The paper concludes with an experimental section (Sec VIII) that provides examples of the algorithms performance on a number of different, complex surfaces. The algorithm minimizes the number of lift-offs necessary and adapts to various manipulability thresholds. 

My primary source of confusion related to understanding many of the figures. For example, Fig. 3 is a large complex figure and it is not immediately apparent in what direction to read it. While the text directs us to see that examples of the different topological divisions described by Eq (15) and (16) are visualized in Fig. 3 the separation is not clear. The reader may need a more detailed caption or further annotations to fully appreciate Fig. 3. While there are pairs of figures that share similar style, i.e. Fig. 5b with Fig 7 and Fig. 6 with Fig 8., the reader may need more details (in caption or in text) to interpret them properly. For example, the distinguishing factor between dotted lines and full lines is not delineated. Many theoretical ideas were illustrated via the figures and I think more illustrative description could further solidify the understanding. 

Given the large number of symbols and terminology, it may be helpful to introduce a table of symbols to aide the reader in keep track of everything. Likewise, a flowchart of the algorithm may help summarize the algorithmic contribution and help guide the reader. 

Below are a few other minor questions/suggestions/comments: 
- One of the assumptions detailed in Sec. III is that all points on surface are within the robot's dexterous workspace, in that we do not need to consider singularities. This assumption was only made clear to me when watching the supplementary video. How limiting is this assumption? 
- Sec V states that phi and psi, from Eq. 13, will be defined later. However, in Sec VI.D, the text refers phi as defined in Eq. 13. It is not clear where these quantities are defined. 
- Eq. 25 brings together Eq. 24 and Eq. 13. Given that Eq. 13 establishes a bound on phi, should the second equals sign in Eq. 25 also be a bound?  
- The term ""cutting path"", while used frequently, is never formally defined.  
- Given that the method focuses on the end effector path, it would be nice to briefly mention in the experimental section how the arm path, that achieves the end effector path, is computed. 
- Sec III, mentions a metric in joint space d(.,.) - is this the euclidean distance? 
- The last sentence of Sev IV.A does an excellent job summarizing the key takeaway of that subsection. The subsections of Sec VII would benefit from similar summaries.
- Is there a way to provide insight into how to interpret the various manipulability thresholds? 
- In Eq. 9 it may add clarity to add a set of parenthesis or brackets around the middle term (in between the inequalities). 
- The text in Fig. 6 and Fig. 8 is too small to read. 
- In Sec II's second paragraph ""chossing""->""choosing"", ""unifrom""->uniform"" and ""WAS""->""was"". 
- In Sec VI.A's first paragraph ""mappingfrom""->""mapping from""",,,,Agreement accepted,07/14 15:00,07/14 17:00,,https://youtu.be/TqFzqGGM06Y,,,5.0,j9u2-fZCQC4
"Deep convolutional neural networks (CNNs) have shown outstanding performance in the task of semantically segmenting images. Applying the same methods on 3D data still poses challenges due to the heavy memory requirements and the lack of structured data. Here, we propose LatticeNet, a novel approach for 3D semantic segmentation, which takes raw point clouds as input. A PointNet describes the local geometry which we embed into a sparse permutohedral lattice. The lattice allows for fast convolutions while keeping a low memory footprint. Further, we introduce DeformSlice, a novel learned data-dependent interpolation for projecting lattice features back onto the point cloud. We present results of 3D segmentation on multiple datasets where our method achieves state-of-the-art performance.",RSS2020_Author_Agreement-1_filled.pdf,Radu Alexandru Rosu (University of Bonn)*; Peer  Schütt (University of Bonn); Jan Quenzel (University of Bonn); Sven Behnke (University of Bonn),Radu Alexandru Rosu (University of Bonn)*; Peer  Schütt (University of Bonn); Jan Quenzel (University of Bonn); Sven Behnke (University of Bonn),RSS_2020_Rosu.pdf (5862389 bytes); RSS2020_Author_Agreement-1_filled.pdf (90964 bytes),RSS_2020_Rosu.pdf,95.0,6.0,,1.0,LatticeNet: Fast Point Cloud Segmentation Using Permutohedral Lattices,http://www.ais.uni-bonn.de/videos/RSS_2020_Rosu/,0.8189827645870771,"This paper extends the well noticed Splatnet [31] method with original contributions by introducing learned operations for splatting and slicing. It is shown in the paper that this extensions lead to significantly better performance in point cloud segmentation.

The paper is clearly written and reads very well. The important concepts are illustrated as figures. 

The new method is thoroughly evaluated on relevant data sets and compared to other state-of-the-art methods. The proposed method achieves state-of-the-art performance at faster speeds and with a lower memory footprint. On the KITTI data set it outperforms all other state-of-the-art methods.

The contribution is significant in improving 3D point cloud segmentation which is an important component in robotics. However, now real link to robotics is provided in the paper except by using the KITTI data set. I would be nice to have experiments directly pointing out the importance for robotics scenarios. Point cloud processing is I guess more interesting in the robotics community as there is a larger number of sensors producing point clouds. So, nevertheless such an algorithm is an important component for robotics.",Agreement accepted,"The authors introduce four new operations on permutohedral lattices that are novel and significant for reasons explained in the following: Instead of summing the vertex features features  are concatenated. A different distribuet function does the same for coordinates. The ""distribute"" significance is that it enables the learning of the splatting with a Pointnet. 
The second is a downsampler (strided convolution) that facilitates more context. The third is an upsampler (transposed convolution) and the fourth is a deformslicing which maps back to point clouds but bu subtracting from the maximum of the weighted values, an operation that guarantees equivariance to point permutations.  The authors beautifully visualize the new splatter and slicer. 

Extensive experiments are performed in ShapeNet, ScanNet and Semantic KITTI. The experiments contain extensive ablation studies as well as comparison to other point cloud segmentation algorithms. In summary, the experimentation is thorough, the methodological contributions are novel and the problem is hard.

",Agreement accepted,"In this paper, a LatticeNet is proposed for the problem of 3D semantic segmentation with the raw point cloud as input. The points are transformed into permutohedral lattices structure and features on the lattice are projected back to the point cloud. Also, the PointNet is leveraged to obtain local features and several new operations are proposed on the pemutohedral lattice. The structure of the paper is organized and it is clear to follow. Extensive experiments have demonstrated that the proposed method is able to obtain state of the art results on multiple datasets with high computation efficiency. The video is clear and helps to understand the idea of the paper.

As the main contribution, the paper introduces the design of the DeformSlicing operator in details, which is good. While some intuitive explanation of why the DerformSlicing is superior to Slicing lacks. Also, in the ablation experiments, the authors do not provide sufficient analysis to illustrate how DeformSlice could positively affect the segmentation results. Another concern is that from the experimental results, the improvements is not very obvious comparing to other state of the art methods. Especially results in Table I. It is suggested the author could add some more comparison with different methods in the aspect of computation efficiency. 

A minor problem is that in Table V, a legend should be given to illustrate what the red and green colors indicate respectively. Typo: undefitting -> underfitting in page 6.  

",,,,Agreement accepted,07/14 15:00,07/14 17:00,https://github.com/AIS-Bonn/lattice_net,http://www.ais.uni-bonn.de/videos/RSS_2020_Rosu/,,,6.0,503Z5Vw9a90
"Accurate rotation estimation is at the heart of robot perception tasks such as visual odometry and object pose estimation. Deep neural networks have provided a new way to perform these tasks, and the choice of rotation representation is an important part of network design. In this work, we present a novel symmetric matrix representation of the 3D rotation group, SO(3), with two important properties that make it particularly suitable for learned models: (1) it satisfies a smoothness property that improves convergence and generalization when regressing large rotation targets, and (2) it encodes a symmetric Bingham belief over the space of unit quaternions, permitting the training of uncertainty-aware models. We empirically validate the benefits of our formulation by training deep neural rotation regressors on two data modalities. First, we use synthetic point-cloud data to show that our representation leads to superior predictive accuracy over existing representations for arbitrary rotation targets. Second, we use image data collected onboard ground and aerial vehicles to demonstrate that our representation is amenable to an effective out-of-distribution (OOD) rejection technique that significantly improves the robustness of rotation estimates to unseen environmental effects and corrupted input images, without requiring the use of an explicit likelihood loss, stochastic sampling, or an auxiliary classifier. This capability is key for safety-critical applications where detecting novel inputs can prevent catastrophic failure of learned models.",RSS2020_Author_Agreement_signed.pdf,"[Valentin Peretroukhin](https://valentinp.com), [Matthew Giamou](https://starslab.ca/people/matthew-giamou/), [David M. Rosen](https://scholar.google.com/citations?user=EZWbedcAAAAJ), [W. Nicholas Greene](https://wngreene.github.io/), [Nicholas Roy](https://www.csail.mit.edu/person/nicholas-roy), [Jonathan Kelly](http://stars.utias.utoronto.ca/~jkelly/)",Valentin Peretroukhin (University of Toronto)*; Matthew Giamou (University of Toronto); W. Nicholas Greene (MIT); David Rosen (MIT Laboratory for Information and Decision Systems); Jonathan Kelly (University of Toronto); Nicholas Roy (MIT),RSS2020_Author_Agreement_signed.pdf (78673 bytes); bingham_so3_learning_camera_ready_june_1.pdf (2164305 bytes),bingham_so3_learning_camera_ready_june_1.pdf,61.0,7.0,,1.0,A Smooth Representation of Belief over SO(3) for Deep Rotation Learning with Uncertainty,https://papers.starslab.ca/bingham-rotation-learning/,0.138399915671618,"The representation of rotations via continuous differentiable forms is essential for pose estimation methods based on learnable differentiable maps, such as deep neural networks. This paper provides a valuable contribution by providing a method for learning rotations which should be easily implementable with any modern deep learning framework. Theoretical results and empirical evidence strengthen the contribution. The paper, however, has a few issues, which I highlight below.

Major issues:
- A clearer distinction and discussion of advantages/disadvantages of the proposed approach with respect to the work in [42] is needed. Despite the simplicity of Problem 3 and its solution when compared to the formulations in [42, Sec. 4.2], the empirical gains seem to be quite marginal in the experiments to justify a 10D rather than 6D representation for rotations in SO(3).

Minor issues:
- There are a few acronyms and math symbols which should be defined before their first mention in the text. Examples (and my guess): QCQP (quadratically constrained quadratic program), SO(n) (special orthogonal group), \mathbb{S}^4 (real symmetric matrices?), S^3 (3D sphere), \mathbb{RP}^n (real projection), the Log and norm in Eq. 22, which is ambiguous, etc. Despite some of these terms and notation being common in some specific fields, RSS is still a general robotics conference with a broad audience. A paragraph defining the main mathematical spaces and the associated notation, as in [42, Sec. 3], should suffice.

- The claim ""most approaches to regressing rotations using data-driven models are unable to effectively model uncertainty"" is slightly strong and lacks a citation.

- In Eq. 25/26, ""f"" as defined by Problem 3 should map to a rotation matrix C^*, not the corresponding quaternion q^*.

- A comparison in terms of measured computation time is missing. It should clarify whether the extra computations required to solve Problem 3 add too much overhead when compared to the method proposed by [42].

Other details:
- Some sentences are quite long, extending for more than 4 lines, and should be broken up/revised.
- Why not ""\mathbf{I}"" instead of ""\mathbf{1}"" to denote the identity matrix? The latter symbol causes confusion with a vector/matrix of ones.",Agreement accepted,"This paper is very well-written. It builds upon the work in reference [42] significantly with the technical novelties being the connection to the Bingham distribution for uncertainty estimation and out-of-distribution detection. The experiments are impressive.

Some minor comments:

1. It is a bit hard to judge how difficult it is to train the matrix A(theta) and how well it generalizes across datasets which would be a highly desirable quality for a rotation estimation method.
2. Perhaps the authors could also consider incorporating this approach in a typical visual-inertial-odometry (VIO) system",Agreement accepted,"The authors present a novel rotation estimation model for robot perception tasks. They claim that their model improves convergence on large rotation targets, it is singularity-free and it is robust against uncertainties or corrupted images, providing experiments with synthetic and real data. They are going to publish the code with the paper for reproducibility.

Minor comments:
- It would be interesting to explain a little bit more how the method avoids singularities
- Quadratically- Constrained Quadratic Program (QCQP) is mentioned for the firt time in page 3, but QCQP appears in previous pages.",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,https://github.com/utiasSTARS/bingham-rotation-learning,,,,7.0,iEp6amPkkKw
"Effective multi-agent teaming requires knowledgeable robots to have the capability of influencing their teammates. Robots are able to possess information that their human and other agent teammates do not, such as by scouting ahead in dangerous areas. To work as an effective team, robots must be able to influence their teammates when necessary and adapt to changing situations in order to move to goal positions that only they may be aware of, while remaining connected as a team. In this paper, we propose the problem of multiple robot teammates tasked with leading a multi-agent team to multiple goal positions while maintaining the ability to communicate with one another. We define utilities of making progress towards goals, maintaining communications with followers, and maintaining communications with fellow leaders. In addition, we introduce a novel regularized optimization formulation that balances these utilities and utilizes structured sparsity inducing norms to focus the leaders' attention on specific goals and followers over time. The dynamically learned utility allows our approach to generate an action for each leader at each time step, which allows the leaders to reach goals without sacrificing communication. We show through extensive synthetic and high-fidelity simulations that our method effectively enables multiple robotic leaders to guide a multi-agent team to different goals while maintaining communication.",RSS2020_Author_Agreement.pdf,"[Brian Reily](https://people.mines.edu/breily/)Christopher Reardon,  [Hao Zhang](http://inside.mines.edu/~hzhang/)",Brian Reily (Colorado School of Mines)*; Christopher Reardon (ARL); Hao Zhang (Colorado School of Mines),RSS2020_Author_Agreement.pdf (107174 bytes); RSS_Leading_Reily_Camera_Ready.pdf (7205183 bytes),RSS_Leading_Reily_Camera_Ready.pdf,58.0,8.0,,1.0,Leading Multi-Agent Teams to Multiple Goals While Maintaining Communication,,0.21918846412065,"This paper contributes a method for leading multi agent teams to multiple goals while maintaining communication. This is important in scenarios such as disaster response, when robots might need to lead multiple agents to multiple goals while keeping in contact with each other. The idea is for leaders to maintain communication with each other, and for leaders to also maintain communication with their own followers. 

One statement the authors make about the contribution is a bit confusing. ""...the challenge of maintaining communication while leading a team of followers to multiple goals has not yet been addressed..."" This statement is a bit misleading, as the leaders are not leading their group of followers to different goals, they are leading their followers to a single goal, while other leaders might be taking their followers to a different goal. The sentence continues ""...either from the perspective of ensuring leaders stay connected to their followers or ensuring leaders stay connected to other leaders."" This statement is misleading as well, and doesn't quite read the way the authors probably intended. There is PLENTY of work on leader-follower control or groups of robots maintaining communication as they navigate a space (calling them leaders does not really affect the algorithm, you can always add followers to those agents). In fact they have cited some of that work. I think they should reconsider this sentence as a whole, quite carefully, to make sure they are making the proper claim they intend to make. 

WIth respect to citing existing work, the authors have done a very thorough literature review, but at times it is too thorough. An entire section on choosing leaders (Sec IIA) is almost irrelevant, as the leaders and followers in this paper are generated as such, and are not assigned or decided later, the paper doesn't explicitly deal with humans and animals (although they could possibly be the agents). So this quite long section of the literature review can be easily shortened. Sec II-A shouldn't really be much longer (if longer at all) than Sec II-B. 

The authors do a good job of showing how their approach, which balances the weights of the different objectives and uses hyperparameters to control the importance of the regularization terms, does better than the baseline (without the hyperparameters), greedy, and random approaches. One thing that is confusing about the choice of algorithms to compare to is why the authors would compare to a random approach? this reviewer feels that the choice of random as a comparison could be better justified. 

One area that could use the extra space created by shortening the literature review is providing more intuition behind W, V, U, and the hyperparameters. There is a lot of notation in this paper, and it's hard to keep up with what everything means and how it affects the outcome. 

There are two lingering questions, though, that would be helpful to address. It's not clear why the authors would like the leaders to go to different goals. Based on the scenario presented, is it really that important to make sure that agents end up evenly spread around the available goals? This was not clear and left me wondering in the evaluation. 
Also, it's not clear what the high-fidelity Unity simulations add to the paper. Especially in light of the fact that the approach is evaluated based on the number of time steps to reach goal positions, but there is no tie-in of those steps to an actual platform, or mention of velocities. I don't see the value of the Unity simulations at all. There is no low-level control policy that is being tested, and the description and discussion of this simulation is brief at best.  

Finally, the conference paper citations are quite sparse, with bare minimum identifying information. I'm not sure if this is a style file issue, or if the authors have omitted the other information. 

Overall this is an interesting paper that could perhaps shift focus from the literature review and the high-fidelity simulations to giving readers a better understanding and intuition of the inner workings of the algorithm. 
",Agreement accepted,"The paper presents a solution to a suitably defined optimization problem that accurately captures the objectives stated by the authors. Generally speaking the paper is written well, modulo some notes I provide below. However, there are a couple of points I am skeptical about. 

The first concerns the practical applicability of the algorithm, given that it is based on a flawed model, i.e., a deterministic circular communication range. While such model is often used in scholar works in robotics, it is known to be unrealistic b/c communication is a much more complex phenomenon. The sensor network community has pinpointed this limitation since long (see for example Cerpa's IPSN 2005 paper just to name one). In fact, such assumption does not hold even at short range. The authors do not provide an adequate discussion on how their framework and algorithm could be extended to manage such extension. This greatly limits the applicability of the solution to the proposed problem.

Second, the very sophisticated mathematical model put forward, but the associated algorithm do not obviously generalize (see also the previous point about applicability). It would be perhaps more valuable if the authors considered a problem where this method is applicable without assuming an unrealistic communication model.  

Algorithm 1 is shown to converge to the optimal solution but what about the rate of convergence?

Other comments:
- is Eq. (2) correct? It does not look like a derivative.
- At the end of section III you state that the overall complexity of the algorithm is O(X^3) where X=max(N,G,F). However that is not the complexity of the algorithm, but rather the complexity of one iteration.",,"In the following my main concerns and comments are listed:

* The overall paper clarity is good but could be further improved. Some sentences are not very clear and should be revised, e.g. the following sentences are very long at unclear :
    ** ""Existing human-robot interaction research has focused on robots simply responding to human actions, and while recent research introduces the problem of how to influence a team to follow a robot, it has focused on a single robot leader.""
    ** ""Additionally, robots may not be able to explicitly relay instructions to their teammates, as the robots may not have authority to control their machine teammates, or civilians being rescued may not have the training necessary to interact with the robot"" 
    ** ""We then design a mathematical formulation based on the unified framework of regularized optimization to find weights that optimally balance these competing utilities, which incorporates new structured sparsity inducing norms to focus each leader’s attention on specific goals and sub-teams of followers, as well as regularization terms to enforce temporal consistency as time passes."".

* The two goals of maintaining communication while leading a team of followers to multiple goals should be better motivated with respect to an application point of view. In practice, having a constant communication rely may not be needed, a periodic or asynchronous communication may be enough.

* In section II-B, the authors should already mention which kind of communication constraints they consider.

* The authors do not highlight one of the major drawbacks of the proposed method, namely the fact of being centralized. In fact, looking at the computational complexity, it is clear that the problem became unfeasible for a large number of robots and goals. To address these problems the community already tried to decentralize algorithms for the communication maintenance (in the following I list two examples). The authors are invited to discuss this point.
    ** ""Decentralized Simultaneous Multi-target Exploration using
a Connected Network of Multiple Robots"" T. Nestmeyer and P. Robuffo Giordano and H. H. Bülthoff and
A. Franchi
    ** ""Distributed connectivity control of mobile networks"" M. M. Zavlanos, G. J. Pappas.

* Regarding the cost function, it is not straight forward to understand why the authors left the scaling gains as optimization variables. This allows maximizing the utility but the latter is not linked to the desired behavior anymore. What if in one scenario the fact of reaching the goals is more important than preserving the connectivity? One can notice this problem also in the proposed simulations (Fig. 2) where 2 out of 3 goals are not reached to preserve the connectivity. The authors are invited to better describe and clarify this aspect.  

* The format chosen to express matrices R, Q, S in equations 2, 3, 4, respectively, is a bit confusing. I would suggest leaving only the term [r_ij] = ... Similarly in 3 and 4.

* As mentioned before, in my opinion, the proposed simulations results are not fully complete because:
    ** One of the main objectives of the proposed investigation is to preserve the connectivity. However, no metrics or results related to the connectivity of the robotic team are provided. Thus the reader cannot verify that the proposed method to preserve the connectivity actually works or not.
    ** I did not understand the relevance of the comparison with a ""random"" approach. Clearly the latter would lead to very poor results, I would say by definition.
    ** The authors claim that their method is consistently able to lead followers to goal positions. Nevertheless, the followers are lead only close to the goal, as it is clear from the plots. A more detailed discussion should be added.
    ** The authors should also mention the performance of the proposed approach with respect to the computational complexity. In how much time the solution to the optimization problem is computed? In other words, every how much time a new command is sent to the leaders? If this is too high, the entire system may get unstable. It might be interesting to understand which are the limits in terms of the maximum number of robots and goals.",,,,,07/14 15:00,07/14 17:00,,,,,8.0,rSBXoqBqQy8
"Simultaneous localization and mapping (SLAM) is a fundamental capability required by most autonomous systems. In this paper, we address the problem of loop closing for SLAM based on 3D laser scans recorded by autonomous cars. Our approach utilizes a deep neural network exploiting different cues generated from LiDAR data for finding loop closures. It estimates an image overlap generalized to range images and provides a relative yaw angle estimate between pairs of scans. Based on such predictions, we tackle loop closure detection and integrate our approach into an existing SLAM system to improve its mapping results. We evaluate our approach on sequences of the KITTI odometry benchmark and the Ford campus dataset. We show that our method can effectively detect loop closures surpassing the detection performance of state-of-the-art methods. To highlight the generalization capabilities of our approach, we evaluate our model on the Ford campus dataset while using only KITTI for training. The experiments show that the learned representation is able to provide reliable loop closure candidates, also in unseen environments.",20200529_Robotics_ Science &Systems Foundation Contributing Auth.pdf,"[Xieyuanli Chen](https://www.ipb.uni-bonn.de/people/xieyuanli-chen/) [Thomas Läbe](https://www.ipb.uni-bonn.de/people/thomas-laebe/) [Andres Milioto](https://www.ipb.uni-bonn.de/people/andres-milioto/) [Timo Röhling](https://www.ipb.uni-bonn.de/people/timo-rohling/)Olga Vysotska, Alexandre Haag,  [Jens Behley](https://www.ipb.uni-bonn.de/people/jens-behley/) [Cyrill Stachniss](https://www.ipb.uni-bonn.de/people/cyrill-stachniss/)","Xieyuanli Chen (Photogrammetry & Robotics Lab, University of Bonn)*; Thomas Läbe (Institute for Geodesy and Geoinformation, University of Bonn); Andres Milioto (University of Bonn); Timo Röhling (Fraunhofer FKIE); Olga Vysotska (Autonomous Intelligent Driving GmbH); Alexandre Haag (AID); Jens Behley (University of Bonn); Cyrill Stachniss (University of Bonn)",chen2020rss.pdf (6485306 bytes); 20200529_Robotics_ Science &Systems Foundation Contributing Auth.pdf (250684 bytes),chen2020rss.pdf,80.0,9.0,,1.0,OverlapNet: Loop Closing for LiDAR-based SLAM,,0.183270526152848,"This paper proposes an interesting solution to loop closure detection in 3D range data. It is very clearly written and generally easy to follow. The reported results are significant, with high recall rates and accurate angle and position estimates, in what appears to be a competitive run-time. I appreciate that you perform ablation studies and compare to different input modalities (Table V) and different variants (Table III).

The related work section is mostly relevant and complete. However, I would recommend to also relate to recent work on learning-based place recognition of Sun et al. (Li Sun et al., ""Localising Faster: Efficient and precise lidar-based robot localisation in large-scale environments"", ICRA 2020, 	arXiv:2003.01875) Furthermore, I think that the claim that methods using handcrafted features are ""therefore often scene-specific"" needs to be better substantiated. Can you cite evidence that they do not generalise to different environments? In particular, your evaluation is on two similar urban datasets, so I think you cannot claim that OverlapNet is less scene specific without further substantiation.

In Sec III-A you state that overlap percentage may be a better measure of loop closure than using the estimated spatial distance. This is certainly true, and is also the rationale for the many appearance-based methods (that you also cite in Sec II).

In Fig 3, I was surprised to see that there is not even a local maximum at the correct position in (a). Could you please clarify why that is so?

Regarding the comparison to learning-based methods (OREOS) it would be interesting to see not only KITTI sequence 00, but also other experiments from the OREOS paper; e.g., the NCLT dataset. 

As you also state in the paper, it appears that it is the use of additional sensor modalities that gives the greatest benefits for OverlapNet. Looking at the results in Tables IV and V, the depth-only OverlapNet has similar performance as OREOS (and higher variance in the yaw estimate) and lower than the method of Sun et al. (above). This is an interesting conclusion.
Looking at the results for geometry only (and using prior pose information), the difference between OverlapNet and the baselines is not significant. The precision/recall curves for the Ford dataset in Fig 5b are indistinguishable (""ours"", M2DP, and ""histogram""), and rather close for KITTI (5a). In Table II, the numbers are also nearly indistinguishable.

In Fig 7, what are the precision rates? (Reporting recall without precision is not very informative.)

Regarding the execution time, please clarify if the reported 630/550 ms is for the whole dataset, and what the processing time is as a function of map size.

Minor edits/typos:
1) Below eq (3): misformed sentence ""using the maximum overall these overlaps""
2) Fig 3(c): ""Groud"" -> ""Ground""
3) ""the most 100 recent scans"" -> ""the 100 most recent scans""
4) Check capitalisation in the reference list (""Pointnetvlad"")
5) Some arXiv URLs are missing in the reference list.
",Agreement accepted,"The paper presents an interesting new technique for estimating the overlap of 3D lidar scans.  This is an important problem for localization of automated vehicles in urban and suburban environment, and is made challenging due to the highly dynamic nature of these scenes, and susceptibility for aliasing due to many urban roads and intersections looking alike (e.g. Manhattan road layouts).  The new method provides good initializations for ICP and integrates readily with pose graph optimization, yielding a capable system. Strong results are obtained for Kitti and impressively the technique operates successfully on the Ford campus dataset without retraining.

The paper is written very clearly and conveys strong technical strength, with novel insights in the design of the Siamese network and the use of the spherical projection representation for lidar scan matching to achieve a lightweight approach. 

The paper has an extensive evaluation and this is one of the strengths of the paper.  Each claim for the new technique is carefully buttressed with appropriate experimental results, including strong precision-recall curves, comparison against other methods, interesting qualitative results and a careful analysis of the yaw estimation errors and ICP registration errors. 

(A minor point, I'm a little surprised that the Kitti yaw estimation errors are as high as 20 degrees for the lower overlap percentage while only as high as 5 degrees for the Ford campus data set, this must be a property of one data set vs. the other?)

The figures and tables are all prepared with great care, and the video is nicely done.  

It would be ideal to see how this technique worked in a true Manhattan-style environment (larger-scale map with repetitive grid-structure) and to see how it degrades when the aliasing is really bad (e.g. nested turns around and around parallel and perpendicular linear city blocks). ",Agreement accepted,"The work takes a step toward removing the need for using state estimates in place recognition. I subscribe to the idea and do think SLAM approaches that rely on the current estimate and geometric nearest neighbor search, despite impressive results, are not a true general solution to the problem. This is simply due to the fact that if the drift is large, the algorithm is doomed to fail. In contrast, the detection of a match between two frames, here using LIDAR, has a better chance of adding useful loop-closures to the SLAM graph.

My main concern is the claim of 3D SLAM which is not verified. The datasets and results are for cars. Therefore, estimating the relative yaw angle is sufficient to have a good result in ICP. I expect the method to be useful for a more general 3D SLAM problem where all motion axes are excited, but the paper claims it without showing it. But of course, the 3D LIDAR data is processed as input and name can be used anyway. 

I'm also interested in seeing the side views of Figure 6. Is the height estimation as good as the top view?",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,https://github.com/PRBonn/OverlapNet,https://youtu.be/YTfliBco6aw,,,9.0,t3TF4mwp6ho
"In the past years, research on the embodiment of interactive social agents has been focused on comparisons between robots and virtually-displayed agents. Our work contributes to this line of research by providing a comparison between social robots and disembodied agents exploring the role of embodiment within group interactions. We conducted a user study where participants formed a team with two agents to play a Collective Risk Dilemma (CRD). Besides having two levels of embodiment as between-subjects ---physically-embodied and disembodied---, we also manipulated the agents' degree of cooperation as a within-subjects variable ---one of the agents used a prosocial strategy and the other used selfish strategy. Our results show that while trust levels were similar between the two conditions of embodiment, participants identified more with the team of embodied agents. Surprisingly, when the agents were disembodied, the prosocial agent was rated more positively and the selfish agent was rated more negatively, compared to when they were embodied. The obtained results support that embodied interactions might improve how humans relate with agents in team settings. However, if the social aspects can positively mask selfish behaviours, as our results suggest, a dark side of embodiment may emerge.",RSSpaperCopyright.pdf,"[Filipa Correia](https://pipzcorreiaz.github.io), [Samuel Gomes](https://github.com/SamGomes), [Samuel Mascarenhas](https://gaips.inesc-id.pt/component/gaips/people/displayPerson/8/12), [Francisco Melo](https://gaips.inesc-id.pt/~fmelo), [Ana Paiva](https://ana-paiva.com)",Filipa Correia (INESC-ID & University of Lisbon)*; Samuel Gomes (IST/INESC-ID); Samuel Mascarenhas (INESC-ID); Francisco S. Melo (IST/INESC-ID); Ana Paiva (INESC-ID U of Lisbon),RSSpaperCopyright.pdf (551983 bytes); RSS20___The_Dark_Side_of_Embodiment(7).pdf (2426178 bytes),RSS20___The_Dark_Side_of_Embodiment(7).pdf,81.0,10.0,,1.0,The Dark Side of Embodiment - Teaming Up With Robots VS Disembodied Agents,,0.06776374896146299,"There is a lot that I liked about this paper. I commend the authors on designing and running a complex experimental protocol that evaluated autonomous robots and agents in a team game setting. The paper is well written and clearly laid out, describing the hypotheses and findings extensively and providing a good discussion of the findings. Furthermore I liked the honest and thoughtful discussion of the negative results found in this study. 

Overall, this research aims at a very interesting question related to agent embodiment in team settings. 

I wish that the authors would have more clearly developed theories and predictions for clearly identified and motivated issues surrounding these topics. Herein lies my main critique of the current paper. 

The authors spend almost three pages setting up topics such as embodiment and team dynamics, but do not converge on a clear theory that their experiment is trying to test. What is the relationship between the studied constructs? What effects would you predict? And importantly, why does it matter to future HRI? In other words, what is the main research question that the authors have? 
 
Instead the lengthy introduction and related work moves straight into a list of hypotheses, which seem overreaching (in the sense that there are too many of them) and unfocused. My sense is that this experiment actually gets at some really interesting relationships between trust, group success, and embodiment, but the authors never clearly state these questions. A more focused description of the research question would make this work much stronger. 

As a result of this lack of focus, the authors combine a lot of different manipulations and measured dependent variable, which also contributed to the lack of focus in this work. Clear experimental manipulations targeting one or two sharp research questions would be preferred. 

For example, I was left confused about the hypothesis that the perception of the pro-social agent will be more positive in the embodied condition. Do authors thing that it will be less positive in the disembodied condition? A ""compared to"" phrase might have disambiguated this.  

An additional issue with the paper is that the authors do not explain the mechanism of the game. This is a crucial part to understanding the experiment and the ability to judge it and possibly replicate it in the future. 

The sample diversity is commendable, in comparison to many HRI studies. However, I found the significant differences between the two main conditions (embodiment) to be too large. The confounds are striking, as can be seen - for example - in the large drop-out rate for the online participants. Why not have people in the lab experiment the disembodied condition? 

I would also have liked to know how the agents' behavior was monitored in the online condition (and, in fact, in the lab condition as well). 

As a smaller point, the Abstract can be much shortened and focus on the findings. 

In summary, this research gets at some very interesting questions, but the research questions are not well set up, resulting in a large number of confounding manipulations and somewhat unfocused dependent variables. The authors make many good choices, but the lack of theory and clear research questions, along with a lack of detail about the actual procedure, are strong drawbacks of the currently submitted work.  
",Agreement accepted,"This paper is overall very well written and makes a relevant contribution to the HRI community by discussing the implication of embodiment in human-agent teams in a mixed-motive context. While I am overall in favor of accepting the paper, I have some comments, mostly related to the readability and organization of arguments in the paper and the presentation of the results.

I. Introduction
The introduction could be a bit more concise in motivating the main contributions of the paper, which are (a) the comparison of an unembodied unimodal agent and an embodied multimodal agent and (b) the interaction with two agents in a mixed-motive task. Right now, the introduction is too detailed in motivating both contributions. This overlaps partially with section II and III, but it also makes the introduction harder to follow because one loses focus.
I feel the information that is conveyed in Fig. 1 is rather confusing and could be more understandable conveyed inside the text. If a picture were to be added for this, I would merge it with Fig. 2 to make it more apparent what is considered the environment and what modalities of interaction exist. 

II. Embodiment
This section in turn would benefit from adding some of the details that were given in the introduction to make it more understandable. Specifically, the first part of the argument is hard to follow because the ‘social embodiment’ is the only one of the six embodiments that is explicitly mentioned, but it is difficult to grasp this concept without mentioning and then properly discussing the differences to the other embodiment definitions. 
I also believe this section would become clearer if it is discussed with the specific embodiments that are chosen for conducting the experiments later in this paper. Especially the importance of discussing structural coupling only becomes evident when reading the paper for a second time, because then the reader knows that both robots can interact with the game interface which is part of the environment in this scenario. Here, it would also be of great help to have Fig. 2 as a reference.

IV. A Task / B Independent Variables
I think it would be helpful to explicitly name the mixed-motive that was mentioned in the introduction here again. Specifically, what are the mixed objectives of each player in the game? Is there one individual that wins the game? If so, is the manipulation of the winner related to the team winning or losing or the human player winning in comparison to the robots as well? Did participants have an incentive to win the game?

IV. D Dependent Measures
How were the two different agents differentiated in the post-game questionnaire? Were they given names or other identifiers? Were all participants asked to rate the cooperative or the defective agent first, so could there potentially be an ordering effect, or was that randomized?

IV. F Sample
Bartneck et al. [0] found a difference between lab-based experiments and experiments conducted online. Even though they argue that practical implications are small, they name the broader range of demographics to be found on AMT  as one potential influence factor on their results. This goes much beyond the distribution of age and gender and is more related to the interest in the subject, level of education or academic background. Since the sample for the lab-based study in this paper was recruited “at the facilities of an energy company”, it is likely that the population for the lab-based study was more homogeneous than the AMT sample. This should be further discussed in the paper. 
[0] Bartneck, Christoph, et al. ""Comparing the similarity of responses received from studies in Amazon’s Mechanical Turk to studies conducted online and with direct recruitment."" PloS one 10.4 (2015). 

V. Results
Figure 3 is misleading because it communicates that all of the diagrams are using the same scale for the y-axis, when it is in fact different scales for all of them. The specific scale needs to be added to each axis, so this becomes evident. It is also very difficult to read the individual captions because there is no gap between (a), (b) and (c). 
For Figure 4, I would advice to use percent instead of number of participants, because with the uneven number of participants per condition it now looks like more people picked the pro-social one in the embodied condition (because the bar is higher), when if the opposite is true. 

VI. Discussion
I was wondering if another explanation for the results could be that the two robots looked exactly the same and were thus also expected to behave the same, while people might have had less expectations towards the two unembodied agents because they didn’t communicate similarity due to their appearance? This might be an interesting consideration for a future study as well.
Another consideration that came to my mind was the ethical implication coming from a robot being easily able to disguise its selfish behavior just by having an embodiment. This could potentially be misused by designers in the future to cover potentially harmful behavior from a robot. 
I would suggest adding the considerations for human-robot teams (that is currently in the conclusions) to the discussion section, because this would make the flow of the argument more readable to me.  ",Agreement accepted,"The paper is mostly well written in terms of language but lacks a lot of methodological clarifications. It appropriately motivates the need for further research in comparisons of embodiment in human-robot interactions. The related work is comprehensive and works are reported adequately. The discussion raised on the definition of embodiment is interesting and well researched, but would be curious to extend the discussion also among the anthropomorphic elements of voice.

I think a comparison in a mixed-design experiment where embodiment is manipulated within subjects and agent degree of cooperation manipulated between subjects would be also interesting. It would not allow the use of in-lab and crowdsourcing platforms, but would further highlight embodiment effects.

I am also curious on the choice of text output for communication with the disembodied agent. Would subjects’ behaviour shift when they have to focus on the text? Why not choosing a disembodied agent that can communicate only with voice?

I did not see anywhere reporting the scale of the questionnaires (only in figures). Also, the measures deserve a bit more description than citing the papers to inform the reader what is going to be compared across conditions.

The experimental setup involves a collaborative game framing a collective risk dilemma. There is something I was not able to understand well on the paper: if the choice through digital dice is predetermined, how can players decide to be either pro-social or selfish?

Furthermore, more information on the verbal and non-verbal behaviour of the robots is needed, for graduate students that wish to replicate or extend this study, and to control on what robot behaviour affects the reported results.

How do subjects in the disembodied condition know who of the two agents is speaking, and is it clear that there are two agents, especially when this condition is evaluated online? How did authors control for this?

The within-subjects variable description is also a bit confusing. Are both agents either pro-social or selfish or only one at the time? If one at the time, what is the other agent’s behaviour?

If I understand correctly the design of the experiment, authors should run Two-way repeated-measures ANOVAs indicating the existence of two between subject factors and one within. Is that correct? For instance, in subsection A of the results, the within factor is not mentioned. This should be made clear.

Minor: a few grammatical mistakes, and a few cases of a mix of British and US English are used throughout the paper. I would suggest the authors have another look before the next paper version.

Overall, the paper presents an interesting problem and has potential, as long as clarifications in method are met. It appears to be theoretically sound and relevant to the conference.",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,https://github.com/SamGomes/for-the-record,,,,10.0,iffv8gV8XQc
"Assistive robots enable people with disabilities to conduct everyday tasks on their own. However, these tasks can be complex, containing both coarse reaching motions and fine-grained manipulation. For example, when eating, not only does one need to move to the correct food item, but they must also precisely manipulate the food in different ways (e.g., cutting, stabbing, scooping). Shared autonomy methods make robot teleoperation safer and more precise by arbitrating user inputs with robot controls. However, these works have focused mainly on the high-level task of reaching a goal from a discrete set, while largely ignoring manipulation of objects at that goal. Meanwhile, dimensionality reduction techniques for teleoperation map useful high-dimensional robot actions into an intuitive low-dimensional controller, but it is unclear if these methods can achieve the requisite precision for tasks like eating. Our insight is that---by combining intuitive embeddings from learned latent actions with robotic assistance from shared autonomy---we can enable precise assistive manipulation. In this work, we adopt learned latent actions for shared autonomy by proposing a new model structure that changes the meaning of the human's input based on the robot's confidence of the goal. We show convergence bounds on the robot's distance to the most likely goal, and develop a training procedure to learn a controller that is able to move between goals even in the presence of shared autonomy. We evaluate our method in simulations and an eating user study. See videos of our experiments here: https://youtu.be/7BouKojzVyk.",RSS2020_Author_Agreement.pdf,"Hong Jun, [Dylan Losey](http://dylanlosey.com/)  [Dorsa Sadigh](https://dorsa.fyi/)",Hong Jun Jeon (Stanford University)*; Dylan Losey (Stanford University); Dorsa Sadigh (Stanford),RSS2020_Author_Agreement.pdf (216365 bytes); Jeon_RSS20.pdf (4273304 bytes),Jeon_RSS20.pdf,1269.0,11.0,,1.0,Shared Autonomy with Learned Latent Actions,,0.461893505245908,"Overall, this paper is well-written and makes a significant contribution with solid evaluation.  The main weakness of this paper as written is that it doesn't provide a good sense of its own limitations (perhaps because the authors decided to eliminate a discussion section in favor of including more results).  In combination with the confusing use of the terms ""goal"" and ""preference"", this means that the authors are effectively substantially overstating the generality of the work in almost every section.  More detail on a few key examples of this:


1. As mentioned above, the goals vs preferences language is quite confusing -- and it doesn't really make the distinction the authors want it to.  The example is also confusing, since a task like ""cut off a piece of tofu and pick it up with the fork"" could either be discrete options (cut vs. stab vs. lift) or it could be the kind of continuous motion that I think the authors are trying to talk about, and a task like ""reach the tofu"" could be discrete options (as the authors intend) but could also involve continuous preference (for example moving along an arc to avoid knocking over a glass of water).  Perhaps the authors mean something closer to the target of the motion and the shape of the motion? Overall, I would say the way that the authors describe their work in terms of ""goals"" and ""preferences"" feels like an over-reach.

2. As far as I can tell, this paper does not include any input from or testing with users with disabilities.  Not every technical paper needs to take a fully participatory design approach, but it's bad form to not even mention this as a limitation (if nothing else, it means that the subjective results need to be taken with a grain of salt).  The authors need to at the very least include a discussion of how things might change if evaluated with target users.  As a start, consider how things might change with:
- participants who use a wheelchair mounted arm full-time and are therefore extremely expert
- participants who have limited ability to provide input (for example, who find it easier to move a joystick in one direction than another)
- disabled participants who are particularly sensitive to having their autonomy curtailed
- participants who are familiar with one method of controlling the arm (e.g., mode-switching) and are given this new method
- participants with multiple disabilities (e.g., low vision or cognitive impairments)
Alternatively, if I have misunderstood, then the authors should provide significantly more detail on the profile of the participants (what type of disability, their level of familiarity with assistive arms, etc.).


3. I found the description of the remapping function to be a bit glib; it makes sense that you can change reference frame for many physical manipulation actions, but the authors should provide more description of the limitations of this approach. For example, how would you know how to remap from opening a door (side hinge) to opening an oven (bottom hinge)? Remapping from picking up an espresso cup to picking up a large coffee mug? For a remapping more complicated than the location of an object this is not a trivial problem (arguably, this type of affordance remapping/transfer learning is still an open problem in robotics).

Minor comments and questions:

What were the demonstrations for the simulated reaching task? Was there a set of reaching demonstrations for one object that were then remapped to the different goals?  Or were there demonstrations provided for each target?

University name is included in study description.
",Agreement accepted,"

This is an interesting paper presenting solid work, and contains many well-thought-through aspects of assistive teleoperation for reaching and grasping tasks. I liked the breadth of the presentation, which included a good motivation, new computational methods, and two kinds of analysis. The paper is also well written and the Figures are clear. 

The authors make a convincing case that their method of switching control modes based on the confidence of the coarse shared autonomy is useful and beneficial for successful assisted teleoperation. I do not have major comments on this paper. 

In terms of clarity, I would recommend better distinguishing the so-called ""Goals"" from ""Preferences"". In the second half of the paper, it is not always clear if the goals of the controller include the ""preferences"" or just the ""goals"". Moreover, do preferences have a temporal aspect to them or are they static orientations, as it sometimes seems in the evaluation part of the paper. Revising this for more clarity would help readers. 

The user study has quite a small sample. This is understandable, since the main contribution of this work is the method and algorithm. Still, a remark on the statistical limitation of such a small sample is necessary. 

Finally, authors could have done a better job anonymizing. A central citation is to an unpublished ArXiv paper that is quite similar to the submission. ",Agreement accepted,"Summary:
This work proposes an approach that enables robots to reach high-level goals as well as adapt to human preferences. They combine shared autonomy and latent actions so that humans can provide inputs with different meanings (e.g., move towards the left or right vs. adjust the fork orientation). The authors include a theoretical analysis on the robot’s convergence to the human’s goal and the robot’s adaptation to changing and new goals. Experiments on both a simulated robot and a real robot show that both latent actions and shared autonomy together lead to higher efficiency on the task. The authors also conduct a user study to determine how the method works with real human users. They found that the time taken to complete the task was lowest for their method LA+SA and users were most comfortable when the robot used their approach.

Originality:
I think the method is quite novel as it allows a human to provide input with different meaning, even with the same joystick control. The work also provides multiple perspectives on the problem: a formulation of the problem, theoretical analysis, simulation experiments, real robot experiments, and a user study. This makes the work have an original and holistic perspective on the problem, considering the mathematical side as well as the human-robot interaction side.

Clarity:
The paper was very well-written. The figures were nicely done and refined. The contributions were laid out clearly in the beginning. The hypotheses for the user study were clearly written. Overall, well-done!

Quality:
The quality of the paper is quite strong. The authors provided a nice theoretical analysis. They also varied several knobs in the simulation experiments, including human rationality, when the human changes goals in the task, fast vs. slow learner, etc. I particularly appreciated the user study, as many works stop at simulated experiments. It was encouraging to see that the time taken was reduced and that participants reported more positively for their condition.

Significance:
The work is significant and would be of great use for the community to think about how human input can be used in different ways to guide a robot towards high-level goals as well as low-level preferences.

Other comments:
- How do the demonstrations have the belief included? It seemed like the demonstrations would be provided before the robot starts interacting with the human.
- Why are the beta values for the real robot experiments different from the beta values for the simulation robot experiments? If this was a purposeful decision, it would be good to know why.
- In section V.D, there’s a small typo (a equivalent → an equivalent).
- The right figure of Figure 8 is missing the “LA+SA” label.
- The fit in Figure 5 doesn’t look linear. Could you please clarify?
- It was a little hard to understand the left side of Figure 6. There are two “scoop in icing” images in the first row. Also, why does “stab morsel” and “dip in rice” have lower preference alignment than “scoop in icing”? 

Overall, it’s a really interesting and well-polished paper!",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,https://youtu.be/7BouKojzVyk,,,11.0,7hHDXMGusfY
"Correspondence identification is a critical capability for multi-robot collaborative perception, which allows a group of robots to consistently refer to the same objects in their own fields of view. Correspondence identification is a challenging problem, especially due to the non-covisible objects that cannot be observed by all robots and the uncertainty in robot perception, which have not been well studied yet in collaborative perception. In this work, we propose a principled approach of regularized graph matching that addresses perception uncertainties and non-covisible objects in a unified mathematical framework to perform correspondence identification in collaborative perception. Our method formulates correspondence identification as a graph matching problem in the regularized constrained optimization framework. We introduce a regularization term to explicitly address perception uncertainties by penalizing the object correspondence with a high uncertainty. We also design a second  regularization term to explicitly address non-covisible objects by penalizing the correspondences built by the non-covisible objects. The formulated constrained optimization problem is difficulty to solve, because it is not convex and it contains regularization terms. Thus, we develop a new sampling-based algorithm to solve our formulated regularized constrained optimization problem. We evaluate our approach in the scenarios of connected autonomous driving and multi-robot coordination in simulations and using real robots. Experimental results show that our method is able to address correspondence identification under uncertainty and non-covisibility, and it outperforms the previous techniques and achieves the state-of-the-art performance.",RSS2020_Author_Agreement.pdf,Peng Gao (Colorado school of mines)*; Rui Guo (Toyota Motor North America); Hongsheng Lu (Toyota Motor North America); Hao Zhang (Colorado School of Mines),Peng Gao (Colorado school of mines)*; Rui Guo (Toyota Motor North America); Hongsheng Lu (Toyota Motor North America); Hao Zhang (Colorado School of Mines),RSS2020_Author_Agreement.pdf (82066 bytes); camera_ready.pdf (2046033 bytes),camera_ready.pdf,59.0,12.0,,1.0,Regularized Graph Matching for Correspondence Identification under Uncertainty in Collaborative Perception,[Not Answered],0.8174021382836391,"The work aims to solve correspondence identification problem in the case of collaborative
perception. To this end, it proposes to solve the problem as a high-order graph matching problem taking into count the uncertainty of detection and the existence of non-covisible object. This cost function is derived from three different features (visual appearance, distance between two objects and angles between three objects).

The uncertainty of the features are extract using the probabilistic attribute of Bayesian Neural Network. Using both uncertainty obtained (visual appearance and distance) from two graphs, the impact of the features in the cost function are weighted. In addition of the weights, 2 regulation terms are introduced. One aims to penalize a matching under poor certainty and uses once again the uncertainty obtain from BNN. The second one penalizes the addition of new correspondence in order for the algorithm to avoid matching every nodes.

To solve the optimisation problem formulated, an algorithm based on Markov Chain Monte Carlo sampling is proposed. 

The experiment has been conduct in three different scenario in both simulated and real
environment. For comparison, six other method based on visual appearance and/or spatial information are implemented. In addition, comparison has been made also with two hand crafted baseline that use the same method as the proposed method but with one of the regularization terms removed, showing the strength of using both of them. Finally, the robustness of the method regarding the uncertainty has been tested by manually increasing the uncertainty; Showing that the method still give good result up to 10% of increased uncertainty.

Positive:
The problem statement seems to be new. And the non-covisibility and are important and practical issues in the multi-robot collabolations. 

Concerns:
(1) The test sets used for evaluation is rather small (50 instances). And the two are based on simulations and one is based on markers.  

(2) It would be fair to compare the proposed method and others on some publicly available benchmark and dataset. It seems that the paper only conduct comparisons in the specific and limited scenarios. How did the re-id method trained on the car or robotic scenario?

(3) Ablation studies on the energy terms are lacking. How much are the attribute, pairwise and triplet energies are important? I would assume the results also depends on which attribute to be employed. By looking at Table 1, even without the non-covisibility and uncertainty, the method works relatively good. At this point, I could not see what is important to the overall performance: the optimization framework is important? or one of the energy terms or the regularization?

",,"The paper proposes a novel mathematical formulation of perception uncertainty and non-convisible objects in correspondence identification problem for collaborative perception. It is of great importance to take uncertainty into account as it is a common source of noise in real environments. The perception uncertainty and non-convisible objects are formulated as regularization terms in order to explicitly minimize the overall uncertainty and keep a moderate number of correspondences. The proposed MCMC-based algorithm is also novel and sound as it provides a detailed balance condition and a stationary distribution. Experiments are performed on three scenarios including two simulation experiments and one real world robot experiment. The insight of the proposed approach is novel and valuable. 

The literature survey provides a clear introduction to the three existing types of solutions to correspondence identification problem in collaborative perception. It precisely distinguishes the proposed approach and existing work with respect to the perception uncertainty with non-convisible objects. 

However the reviewer has some concerns and questions about the work and hope these can be addressed or clarified by the authors. 

1. The paper proposes a sampling-based algorithm to solve the non-convex problem with regularization terms and also analyzes the time complexity of the algorithm. However, it would be better for the author to indicate the average running time of each data instance in each scenario. As it is a non-deterministic algorithm, the running time and result for the same data is different each time and the result also depends on the initialization values. It would be better if these can be clarified in the result section to provide a thorough analysis of the proposed algorithm.

2. The lambda value for each regularization term is set to 0.1 and 0.4. It would be better if the intuition can be clarified how the lambdas are chosen and why the hyper-parameter for non-convisible objects is four times than the hyper-parameter of the perception uncertainty. 

3. It would also be better to see the performance of the baseline method with both hyper-parameters set to zero. It would be interesting to see the performance of the proposed algorithm without considering any regularization term. 

4. In the proposed MCMC algorithm, the probability of deleting a node follows a uniform distribution. It would be better to explain why it doesn’t follow the weight of the node as the same in the way the node is added. 

5. For computing the position of the object in the scene, the average of the depth information is computed. However, when there are partial or severe occlusions, the average of the depth information is not reliable as the missing part of depth information is not counted. Even the uncertainty of the position is computed as the average of the variance of the depth data. It would be better to explain how the noisy position from depth estimation can be used in the correspondence identification. 

Overall, the paper is easy to follow and very well written. It is presented with clear motivation towards solving the problem of correspondence identification with perception uncertainty and non-convisible objects. 
",,"The major strength of this paper is its theoretical soundness and the
generality of the approach. The formulation of the objective function
is reasonable, as it includes the three important similarity types by
which correspondences between views can be established. The
formulation of the MCMC based optimization algorithm novel and well
designed for the given problem. Also, the results on the three data
sets in terms of precision and recall are encouraging.

Despite these strengths, there are also a number of drawbacks of the
proposed approach. First and foremost, the applicability in real
situations seems very limited, given that the approach is
computationally very demanding. The dimensionality of the optimizaion
problem is huge, and it is not clear whether and to which extent the
proposed MCMC approach helps to mitigate that. Also, while the authors
provide a theoretical complexity as O(n^4), they do not provide actual
computation times in the experimental section. This is however crucial
to assess the capability of the approach for real applications.

Another drawback is that the weight matrices are derived from
uncertainties that come from the predictive uncertainties of
BNNs. While BNNs tend to give relatively good estimates of the
epistemic uncertainty, it is questionable whether this uncertatiny is
reliable for all kinds of applications. In particular, when using BNNs
for depth estimation, it is not clear whether the estimated
uncertainty does not lead to over- or underconfidence, given that the
depth estimation itself may fail unpredictably. To be a useful
estimate of uncertainty, the BNNs would have to be analysed (at least
empirically) regarding their actual ability to describe the model
uncertainty. For example, the intuition that far away objects are more
uncertain is not necessarily reflected in a BNN. Also, ideally it
would be good to have means to incorporate other sources of model
uncertainty, for example if a physical motion model is given and
uncertainty is described as a 6D covariance matrix. This could for
example be obtained from a SLAM system, and the resulting uncertainty
is likely to be more precise than the BNN estimate.

One question: Are the attribute and distance values normalized? Or how
is it guaranteed that they are comparable and can therefore be used in
the same equation?

In summary, the paper is very valuable from a theoretical point of
view, however it lacks a significant amount of considerations for the
actual application in real scenarios. The real experiment helps a bit
to mitigate that, but more large-scale experiments - along with a
detailed analysis on required ressources - are necessary for a more
convincing work.





",,,,Agreement accepted,07/14 15:00,07/14 17:00,[Not Answered],[Not Answered],,,12.0,VEmWswnETjc
"Sidewinder rattlesnakes generate movement through coordinated lateral and vertical traveling waves of body curvature. Previous biological and robotic studies have demonstrated that proper control and coordination of these two waves enables robust and versatile locomotion in complex environments. However, the propagation of the vertical wave, which sets the body-environment contact state, can affect static stability and cause undesirable locomotion behaviors, especially when for movement at low speeds. Here, we propose to stabilize gaits by modulations of the spatial frequency of the vertical wave, which can be used to tune the number of distinct body-environment contact patches (while maintaining a constant overall contact area). These modulations act to stabilize configurations that were previously statically unstable and therefore, by eliminating dynamic effects such as undesired turning, broaden the range of movements and behaviors accessible to limbless locomotors at a variety of speeds. Specifically, our approach identifies, for a given lateral wave, the spatial frequency of the vertical wave that statically stabilizes the locomotor and then uses geometric mechanics tools to identify the coordination (i.e., the phase shift) between the vertical and lateral waves that produces a desired motion. We demonstrate the effectiveness of our technique on the locomotion of both robotic and robophysical systems. ",RSS2020_Author_Agreement.pdf,Baxi Zhong (Goergia Tech)*; Tianyu Wang (Carnegie Mellon University); Jennifer Rieser (Georgia Institute of Technology); Abdul Kaba (Morehouse College); Howie Choset (Carnegie Melon University); Daniel Goldman (Georgia Institute of Technology),Baxi Zhong (Goergia Tech)*; Tianyu Wang (Carnegie Mellon University); Jennifer Rieser (Georgia Institute of Technology); Abdul Kaba (Morehouse College); Howie Choset (Carnegie Melon University); Daniel Goldman (Georgia Institute of Technology),RSS_2020_final.pdf (9213861 bytes); RSS2020_Author_Agreement.pdf (156244 bytes),RSS_2020_final.pdf,92.0,13.0,,1.0,Frequency Modulation of Body Waves to Improve Performance of Limbless Robots,[Not Answered],0.6601238707661241,"The authors describe and implement a gait for sidewinding snake robots to enhance directional stability by improving static stability with vertical wave modulation. Gaits are tested over a variety of spatial frequency ratios and temporal frequencies, in two different robots, and the results evaluated in terms of open-loop directional stability. The gaits are simulated, and the model is used to evaluate stability, with predictions compared to the experimental results.

Trial counts should be included in figures or their captions (figs 5-7). It would also be helpful to have a visual indication of which robot (or simulation) is being used plotted in each of the figures.

How certain are the authors that the static stability accounts for all of the locomotion discrepancies? It would be helpful to have a statistical measure of the correlation (e.g. between the two fig 4 panels), rather than a qualitative inference. It would also improve confidence to address/eliminate other possible experimental/simulation fidelity effects, for example:
- How well does the actual gait of the robot follow the prescribed gait in terms of joint angles at different speeds/wavelengths (especially important given the use of a series elastic actuator in one system)? 
- The model omits inertia but the authors acknowledge its importance, can a sense of the size of the inertial forces vs. temporal frequency be given?
- How well is friction predicted? The robophysical model looks to have a variety of contact surfaces. 
These sorts of things would be useful to have information on when evaluating the data.

The only performance measure the authors consider is directional stability, I think expanding the evaluation could be beneficial. Things such as joint work / cost of transport?

The authors could also look at robustness more in the context of external perturbations - what does the robot performance look like on uneven hard ground, or over a small obstacle? The definition of robustness as directional stability is a little narrow to me.

""...careful manipulation and protection of motor modules are required."" Some elaboration on this point would be good, i.e. what is the specific deficiency of the dynamixel robot? Why could the more sophisticated robot not be used for all trials? The authors should show a pair of equivalent trials with both robots side by side so how comparable they are is clear

This is a semantic point, but the distinction the authors are making between robotic and robophysical systems is not clear to me. To my mind the difference is mainly to due with the intended purpose (i.e. applied robotics vs. physical modelling). Here the robots are used in the same way and to the same purpose.

The paper is well written and easy to follow. This is very minor but the paper would read more comfortably if the figures were better positioned relative to their references in the text, particularly the results plots.
",Agreement accepted,"The paper is well structured and presented. An appropriate description of the analysis involved and context of the research is included. The authors clearly make a case for how they extend the understanding of the dependence of limbless locomotion on temporal and spatial frequencies of body waves, demonstrating modes that were not known to be stable. Simulated results are supported by a robotic and robophysical model, lending weight to their results. 

Some small comments in wording: 
""Good agreement between exp"" -> I would write out ""experiment"", if this is what was meant 
""showed that no significant turning were observed over a range of temporal frequencies."" were -> was
""The color represent gait periods""-> colors

I'd mention the definition of robophysical, (consisting of robotic components but not an autonomous independent system?) somewhere in the prose. ",,"This paper does a good job to present the problem being solved, the current state of work in the field, as well as the contributions made with this work. Limbless, snake-like robots currently seem to suffer from undesireable motions. The authors use geometric mechanics to stabilize the robot for controlled, desirable lateral and rotational movements, which they validate on 2 separate robotic systems. 

The paper is well-written and easy to follow. Section 2 presents a succinct description of sidewinder locomotion and the geometric mechanics that are used throughout the rest of the paper to analyze and control the system, which is important for the clarity of the paper and in particular for readers that may not be as familiar with the background work. Figures clearly convey the methods used and their effects on the system. However, while stated in the Results section and shown in the accompanying video, the final results showing the dependency of the frequency on the gait performance could be shown through a series of time-lapse images in the paper directly. There is plenty of space to do this and would be a good addition to make the paper more self contained and not rely on the video to demonstrate the success of the robot. 

A study into the frequency of contact sequences to take advantage of both static and dynamic stability is significant in that it can be extended to other systems that use similar principles for control. For example, the same principles are present in legged systems such as bipeds and quadrupeds where gaits are rarely statically stable and require a minimum contact switching frequency for underactuated control modes in order to prevent it from falling over. Work looking at the concept of ""dynamic stability"" is important and methods to analyze systems that can move in a controlled manner even during temporarily underactuated systems is necessary to create robots capable of being controlled while executing dynamic maneuvers. This work addresses both high speed movements, as well as stable low speed movements.",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,[Not Answered],[Not Answered],,,13.0,u4r32OxXnCA
"Modular robotic systems comprise groups of physically connected modules which can be reconfigured to create morphologies that suit an environment or task. One method of reconfiguration is via subtraction, where extraneous modules disconnect from an initial configuration, before being removed by external intervention. In this paper, we consider an approach to reconfiguration in two dimensions, here termed active subtraction, in which unwanted modules traverse a configuration in order to remove themselves safely, without the need for external intervention, making it a form of self-reconfiguration. We present a sequential solution that selects suitable extraneous modules that then remove themselves, one by one. We also present a parallel solution that, while being more computationally demanding, allows multiple modules to move simultaneously. Both solutions are proven to (i) be correct for any given non-hollow structure, and (ii) require, in the worst case, quadratic time proportionally to the number of modules. Simulation studies demonstrate that both solutions work effectively for specified and randomly generated desired configurations with hundreds of modules, and reveal a non-monotonic dependence between the performance and the percentage of modules to be removed. This work demonstrates active subtraction as a viable method of self-reconfiguration, without the need for heuristics or stochasticity, and suggests its potential for application in real-world systems.",RSS2020_Author_Agreement.pdf,"[Matthew D. Hall](http://naturalrobotics.group.shef.ac.uk/people-mhall.html), [Anil Ozdemir](http://aozdemir.net/), [Roderich Gross](https://www.sheffield.ac.uk/acse/department/people/academic/roderich-gross)",Matthew Hall (The University of Sheffield)*; Anil Ozdemir (The University of Sheffield); Roderich Gross (The University of Sheffield),RSS2020_Author_Agreement.pdf (45519 bytes); RSS_2020_Camera_Ready.pdf (3319311 bytes),RSS_2020_Camera_Ready.pdf,111.0,14.0,,1.0,Self-Reconfiguration in Two-Dimensions via Active Subtraction with Modular Robots,http://naturalrobotics.group.shef.ac.uk/supp/2020-002/,0.9027961695095579,"The overall paper is nicely written and very enjoyable to read. The technical rigor in each of its part (literature review, algorithm description with pseudocode, mathematical analysis, performance evaluation) is of very high level. The authors should be commended for this effort.
However, I have some concerns that I believe should be addressed prior to a potential publication.
-	My main concern is novelty and competitiveness of the work presented. The algorithm presented is relatively straightforward, starting to remove modules from a top row and working itself down whereas always the most-west of the top row module with free edges is allowed to leave. In a way, this resembles a standard CNC-machining technique with the exception of n-shaped forms. It would have been nice to have some comparison with current subtraction techniques in manufacturing to address this criticism. While perhaps the overall algorithmic novelty would have been reduced, I am sure the authors could have pointed out a number of other details that really differentiate their contribution from classical manufacturing techniques, simply because the “material” can self-locomote. Moreover, while the two proposed algorithms, SAS and PAS, are in any case centralized (there is a leader that coordinates the movement of the other modules, at least it triggers the movement sequence that is then executed autonomously and deterministically, in series or in parallel, by the other modules), it is not clear to me that the other algorithms proposed in the literature are as well centralized or to what extent they are (I am not an expert in modular robotics but I believe the RSS audience is wide, essentially the whole robotics community, so the paper should also be written for non-expert roboticists). In a nutshell, it would have been nice to get a sharper, possibly quantitative comparison between SAS and PAS and existing algorithms in the literature. 

-	Another concern is related to novelty and relevance to physical implementation. The authors claim that their work is the first addressing the problem of gravity (“The problem is particularly challenging in the presence of gravity, as the redundant modules may have to leave in a particular order to prevent the current structure from collapsing. This problem is addressed for the first time in this work.”, p.2). While not being an expert in modular robotics, I fully trust the novelty claim of the authors. However, I then expected in the rest of the paper that the investigation of non-collapsing properties and gravity influence would have been investigated in a more thorough way, taking into account such forces/constraints in simulation at least. This was unfortunately not the case.

-	The introduction of the parallel active subtraction is a nice idea, however as there are no real robot or more realistic simulation experiments, the relevance of it gets a bit diminished. The leader module basically has to simulate the movement of the other modules at each timestep, meaning that in the same simulation the PAS algorithm does not experience any discrepancies between a potential reality and its internal simulation. With real (or realistically simulated) robots, on the other hand, the situation gets more complex due to locomotion constraints (e.g., using the HyMod platform an arrival time difference of two might not always be possible). For instance, modules might have slight variability in terms of locomotion speed due to manufacturing differences, their actuator and sensor noise or even possible hardware failures might affect the currently assumed perfectly deterministic and reliable movement. In other words, how robust are the SAS and PAS algorithms? Can they be easily modified to increase their robustness in a physical implementation? Since both SAS and PAS assume proper communication connectivity among the modules, would be possible to relax centralization in order to increase robustness and possibly speed up the active subtractive process? A few lines discussion on the current limitations/future directions along these lines would help the paper to be more rounded.  

-	The simulation studies compare the two proposed algorithms SAS and PAS. However, a comparison to existing algorithms such as the one by Gilpin et al. would highly increase the relevance of such comparison. In particular, I found the section VA and VB a bit misleading on this front since the paper of Gauci et al. and Gilpin at al. are mentioned at the beginning followed immediately by a quantitative number (“The reconfiguration takes 39 time steps”). An inattentive reader might think that this number is the one obtained with the algorithm previously cited. Instead, the authors (I assume) refer to the performance of their SAS algorithm on the specific benchmark inspired by the literature and the improvement % mentioned afterwards always refer to the reduction of time to completion of PAS vs. SAS. Finally, although mentioned briefly in the discussion section, for a fairer comparison between SAS and PAS not only time steps to completion, but also the computational overhead should be compared due to the mentioned tradeoff between the two. Indeed, depending on the underlying physical modules, PAS as presented in this work might not be feasible on all hardware systems due to the high computational load.

Additional detailed comments:
-	Lemma 3 assumes that the sink is adjacent to the initial configuration, as otherwise excluded modules might block the sink. Furthermore, it also assumes that there is free room around the initial configuration.

-	The explanation for the performance of SAS over varying density is interesting. However, I would have liked to see some further insight taking into account shape complexity. 

-	It would be interesting to see an example of an automatically generated shape.

-	The logarithmic scale mentioned in the discussion is confusing as the figures do not use any logarithmic scales.

-	Theorem 7 mixes O, \Theta and \Omega for the time step bounding symbol.

-	The references are not quite consistent. For example, some references list the publisher while others do not. Some mention the publishing year twice.
",,"This is a paper describing self-reconfiguration by subtraction for sliding cube module style motion. Overall it is interesting and a good contribution, but the authors need to be more explicit about assumptions made. 

Stability of structure during reconfiguration? You say there is gravity and a floor, but there is nothing about the stability of the structure during reconfiguration.  

-need to add assumption of known coordinates/sensors.  You use a leader to generate a cooridnate system via message passing, but the election of that leader needs to know some position information (it is west most on ground)  How is west sensed?  How is direction (n,s,e,w) known?  Is it able to sense that it is contacting / communicating to the ground?

-be clear that you are looking at 2d case.

-how is synchronization implicit with the assumptions you made?  Message latency and delay could still cause asynchronization even if all robots have an internal clock. 

“ We hypothesize that the problem of determining an optimal order is NP hard” is not justified, please remove or justify

-how is timing enforced with parallel active subtraction?  This requires all modules to move according the the leaders clock, but they only have a communication channel to the leader with unknown latency, and no synchronization mechanism or assumption has been made. 

What are your assumptions about initial configuration?  I think you need to assume a fully packed rectangle, otherwise robots not connected to the ground could be disconnected who are not connected to the ground.  For example, in figure 6 left, if we added 2 modules to the top row of the initial configuration on the right side, forming a cantilever.  The leftmost of these 2 modules would move first, disconnecting the second module from the shape and not on the ground.  -- on a second read it looks like you do make the assumption, but it is not clearly stated, you say : “A modular robot of rectangular shape is situated (i.e., standing) on the ground, extending upwards”.. Please reword to be “a group of modules in a filled rectangular shape” - this is a strong assumption, and you need to make it clear.    
",Agreement accepted,"The paper is clear and well written; the problem is clearly formulated and the proposed solution is effective for the proposed scenario. However, the significance and the novelty of the proposed solution with respect to analogous solutions from the field of modular robotics is not clear to the Reviewer. Indeed, the solution is well presented and correct but it is not clear what is effectively new with respect to other approaches for self assembling, self reconfiguring modular robot. The Authors are respectfully suggested to better highlight the novel problem that the proposed solution can solve (while other can not), and, if possible, by performing a performance comparison with other approach from the literature. The results of a comparison among sequential and parallel solution were quite predictable. 
Moreover, the Authors should further discuss the application domain and the limits of the proposed solution; as the fact that the proposed approach works for planar cases and for specific geometries. Moreover, the Authors should also deal with physical limits of modular robots, that can be unable to move in some of the considered scenario (e.g. it is hard to think to a modular device that can easily move in long corridors as in fig.5), or should deal with the physical limit of the inter robot connections related to the formations that can be effectively achieved (e.g. the formation if Fig.8 requires a set of robot solidly anchored to the floor).
",,,,Agreement accepted,07/14 15:00,07/14 17:00,,http://doi.org/10.15131/shef.data.12420326,,,14.0,RgGi_mc4tOQ
"We present a numerical method to compute singularity sets in the configuration space of free-floating robots, comparing two different criteria based on formal methods. By exploiting specific properties of free-floating systems and an alternative formulation of the generalized Jacobian, the search space and computational complexity of the algorithm is reduced. It is shown that the resulting singularity maps can be applied in the context of trajectory planning to guarantee feasibility with respect to singularity avoidance. The proposed approach is validated on a space robot composed of a six degrees-of-freedom (DOF) arm mounted on a body with six DOF.",RSS2020_dc_Agreement.pdf,"[Davide Calzolari](https://www.in.tum.de/i23/people/davide-calzolari/), [Roberto Lampariello](https://rmc.dlr.de/rm/de/staff/roberto.lampariello/), [Alessandro M. Giordano](https://rmc.dlr.de/rm/de/staff/alessandro.giordano/)","Davide Calzolari (Technical University of Munich (TUM), German Aerospace Center (DLR))*; Roberto Lampariello (German Aerospace Center); Alessandro Massimo Giordano (Deutches Zentrum für Luft und Raumfahrt)",RSS2020_dc_Agreement.pdf (266324 bytes); root_RSS_2020_camera_ready.pdf (1189027 bytes),root_RSS_2020_camera_ready.pdf,1310.0,15.0,,1.0,Singularity Maps of Space Robots and their Application to Gradient-based Trajectory Planning,,0.058750471854357,"Introduction
This papers presents a two new approaches based on formal methods to identify the 
singularity configurations of a free floating manipulator.
The approach is validated through numerical simulations.

Contributions
The paper is well written and clear. The problem is well stated and justified.
The innovation is located in Section III and Section IV.
In the first one the ability to exploit the interval arithmetic and the second one is based 
on Taylor models.
In the second part, the ability to apply the configuration space constraints within a 
trajectory planning constrained approach is shown.
The contribution is clear and relevant. However, there are several aspects that need to be 
clarified in the paper I have just few comments to improve the readability 
First, it would be nice to compare the proposed solution in the validation part comparing it with at the 
determinant of the generalized Jacobian. I believe it should possible in all cases presented 
in the simulation part. A table showing this comparison can be added to identify the accuracy 
and precision of the multiple approaches that can be employed to solve the problem.
Second, it will be nice to show for a complex manipulator case, that the proposed solution is more 
efficient compared to the classic one involving the Jacobian computation.
Third, the approach needs to be tested on an experimental platform to confirm its validity.
Finally, I would like some clarifications on the heuristic pruning. Is the gradient descent 
only within the set of candidate locations within the given set close to ? The writing of that paragraph 
seems disconnected between the first part of that section and the second one.
Some comparisons will respect to the start of the arts methods in terms of computation efficiency 
and runtime are needed. In Section IV c, I believe the condition 23 should not be verified because 
that represent the singularity case in the map.
Section V b does not present the Taylor case, which nullifies the 
main purpose of the work. This cannot be neglected and postponed to future works 
since the Taylor approach is core algorithm in the paper.

Conclusion
The paper is well written, easy to follow, and the contribution is clear. However, there 
are several aspects that need clarifications and improvements to make the contribution 
stronger and clearly suitable for this type of conference.",Agreement accepted,"The authors present the singularity maps approach and a trajectory planners that takes advantage of it to plan the motions of free-floating robots. The proposed techniques are implemented and tested in simulations. The paper is well written and flows nicely. In particular, the qualitative discussions of the advantages and disadvantages of various techniques is welcome. The techniques are, to my knowledge, novel and sound and will be of interest to those working in space robotics.

The area where I see the potential for significant improvement is in the experimental evaluation. The techniques tested all utilize the singularity maps approach. Thus, it is not clear what quantitative benefit is gained (in terms of computation and/or path quality) versus a technique that determines singularity using an existing method. Furthermore, the example shown does not highlight a situation where the robot actively avoids a singularity. A set of examples where it is clear what the benefit is to modeling the singularities in an efficient way would strengthen the case for the proposed algorithm.

Minor comments
-in introduction, I assume subscript des means ""desired"" but not explicitly stated
-the complete search algorithm could benefit from being described in an algorithm environment stepping through the algorithm
-Many references have redundant information (multiple year entries) which should be removed
-very minor: quotation marks are the wrong way prior to the word. use `` in latex to fix this
",,"The paper „Singularity Maps of Space Robots and their Application to Gradient-based Trajectory Planning“ proposes a method for the calculation of singularity maps in configuration space of free-floating space robots. These robots consist of a carrier body (i.e. a spacecraft), which is not actuated, and a robot manipulator. The proposed method enables the computation of singularity maps for open-chain kinematic structures, which was previously not possible for the regarded setup.
The derivation of the proposed method for the computation of the singularity maps is well written and therefore easy to comprehend. The approximation of the true singularity locations by using interval arithmetic and Taylor models together with the proposed search method seems reasonable. However, the provided example for a 6-DoF free-floating robot indicates that this method is computationally very expensive.
The authors also show that the computed singularity maps can be used for trajectory planning under distance constraints. By this, it can be guaranteed that the trajectory in configuration space keeps a safe distance to the robot's singularities. The proposed distance function for utilization of the singularity maps for trajectory planning could not be reproduced (details at the end of this review). Section IV-B describing the segment intersection computation is very sparse in explaining the method. It did not become clear why the proposed method was chosen. Also, the vector ""n"" in the explanation for equation (21) is not explained.
At the end the paper provides examples for evaluation of the proposed methods. The evaluations include plots of computed singularity maps as well as runtime and memory analyses. For the 6-DoF example, the provided plots aren't very informative, because of the necessary dimensionality reduction (this is even noted by the authors in the caption of figure 3). The runtime analyses are also not very informative since, as the authors note, their implementation is not running efficiently. For the evaluation of the memory usage, details of the data structure would be helpful. Overall, the provided evaluations seem a little weak and cannot evaluate the correctness of the results of the proposed methods.
Instead of the used examples, it would have been meaningful if a setup with known singularities would have been used. It would have been possible to evaluate the correctness and the conservativeness introduced by the proposed method. Also, this would have made it possible to evaluate the planned trajectory and the performance differences of the criteria used to compute the singularity maps.
Following the details of the attempt to reproduce the proposed distance function (equations are provided in LaTeX code):
The squared Euclidean distance between the configuration q and the center of the singularity sphere q_sm is
$\begin{align*}
d^2_{sm} = \Vert \boldsymbol{q}_{sm} - \boldsymbol{q} \Vert^2_2 &= \left( \sqrt{\sum_i \left( q_{sm,i} - q_i \right)^2} \right)^2 \\ &= \sum_i \left( q_{sm,i} - q_i \right)^2 \\ &= \sum_i \left( q_{sm,i}^2 + q_i^2 - 2 q_{sm,i} q_i \right)\text{,}
\end{align*}$
which corresponds to the first two sums of equation (18).
The squared Euclidean distance between the configuration q and the surface of the singularity sphere (i.e., considering its radius r_sm) can be calculated by squaring the L^2 norm d_sm of the straight line between q and q_sm minus the radius r_sm
$\begin{align*}
d^2 &= \left( d_{sm} - r_{sm} \right)^2 \\ &=d^2_{sm} + r^2_{sm} - 2 d_{sm} r_{sm} \text{,}
\end{align*}$
which is clearly different from equation (18).
If the proposed distance function was chosen to get a signed value indicating if the configuration q is inside the singularity sphere, why not use the euclidean distance d = d_sm - r_sm without the squares, which is positive if d_sm > r_sm?
Minor issues:
- p. 4, 2nd paragraph of III-B: 3th -> 3rd (two times)
- p. 5, 2nd paragraph of III-D_: 'if the size of considered set [...]' -> 'if the size of the considered set [...]'
- p. 5, definition of r_sm: Should be r_sm in R^N (LaTeX: $\boldsymbol{r}_{sm} \in \R^N$) since r_sm is said to be the N-dimensional vector of norms of the interval radius vectors (main diagonals) of the N boxes in the search set and a norm is a scalar.
- p. 6, V-A: ""explicitly function"" probably means ""explicit functions""!?
- from p. 6 onwards: Why use square brackets around the units? To me, this notation is only used in dimensional analysis!?
- p. 7, last paragraph before V-C: Closing brackets behind equation references are duplicated.
- p. 8, shortly before VI: ""[...], and that does to not [...]"" -> ""[...], and that does not [...]""
",,,,Agreement accepted,07/14 15:00,07/14 17:00,,,,,15.0,N3VgwYp9htc
"Natural language instructions often exhibit sequential constraints rather than being simply goal-oriented, for example ``go around the lake and then travel north until the intersection''. Existing approaches map these kinds of natural language expressions to Linear Temporal Logic expressions but require an expensive dataset of LTL expressions paired with English sentences. We introduce an approach that can learn to map from English to LTL expressions given only pairs of English sentences and trajectories, enabling a robot to understand commands with sequential constraints.  We use formal methods of LTL progression to reward the produced logical forms by progressing each LTL logical form against the ground-truth trajectory, represented as a sequence of states, so that no LTL expressions are needed during training. We evaluate in two ways: on the SAIL dataset, a benchmark artificial environment of 3,266 trajectories and language commands as well as on 10 newly-collected real-world environments of roughly the same size. We show that our model correctly interprets natural language commands with 76.9% accuracy on average. We demonstrate the end-to-end process in real-time in simulation, starting with only a natural language instruction and an initial robot state, producing a logical form from the model trained with trajectories, and finding a trajectory that satisfies sequential constraints with an LTL planner in the environment.",RSS2020_Author_Agreement.pdf,Roma Patel (Brown University)*; Ellie Pavlick (Brown University); Stefanie Tellex (Brown University),Roma Patel (Brown University)*; Ellie Pavlick (Brown University); Stefanie Tellex (Brown University),RSS2020_Author_Agreement.pdf (219175 bytes); RSS_LTL.pdf (8510542 bytes),RSS_LTL.pdf,1166.0,16.0,,1.0,Grounding Language to Non-Markovian Tasks with No Supervision of Task Specifications,[Not Answered],0.9739945519206139,"This paper presents a solution to learning to understand natural language instructions that describe navigation steps.  Thus, the problem is to map natural language expressions onto trajectories in space.  It takes the approach of using a logical form encoded in LTL as an intermediate representation of the task.  Given an LTL expression, it is relatively straightforward to generate a trajectory.  However, it is nontrivial for humans to generate LTL expressions for training purposes.  Therefore, the method is to provide (language, trajectory) pairs for training and automatically extract plausible LTL expressions from the training examples.  Thus, the primary contribution of the paper is to learn the intermediate logical form in LTL without ever seeing training examples that contain LTL.

The authors observe that the introduction of an intermediate representation aids learning, makes explicit any temporal ordering encoded in the natural language expression, and permits the use of formal methods to follow the navigation instructions.  They cite several works that use intermediate logical forms, especially Artzi and Zettlemoyer.  However, this paper's use of logical forms is novel because it's the first to use a temporal logic as an intermediate representation for semantic parsing.

The authors raise good points about the value of an intermediate representation.  However, they somewhat neglect recent work that has moved away from such representations toward the sensorimotor learning paradigm of mapping pixels to controls.  For example, Artzi himself has taken a hard swing in this direction ([1] and other recent works).  I take no position on this debate here, but I think it important that works in this area fully acknowledge both sides of the debate so that readers can fully appreciate the contribution.

The paper shows that a model can be trained to predict LTL expressions that can in turn produce the original trajectory or one very similar with high probability.  This is the largest strength of the paper.

The evaluation, by comparison, does not hold up.  This problem is admittedly difficult to benchmark effectively.  The authors acknowledge that the bulk of instructions in the SAIL corpus do not have temporal dependencies and so the paper's methodology is not exploited.  Thus, although the SAIL corpus has long been used to benchmark semantic parsing tasks, those results are not particularly interesting here.  The paper invents a second benchmark comprising a set of OSM maps of the areas surrounding a number of American university campuses.  These street maps can easily be used to generate sequential instructions that highlight the strength of the method.  Unfortunately though, the baseline is somewhat of a straw man.  It predicts the goal only, and then it uses a shortest-path planner to generate a path to the goal.  The authors include non-shortest paths in the corpus, so it is a fait accompli that their method will perform better than the baseline.

A baseline and corpus better suited to the authors' purpose is coincidentally also found in Artzi's more recent work. The LANI corpus [2] gives sequential navigation instructions and trajectories provided from workers on Amazon Mechanical Turk.

The paper presents an interesting concept and methodological contribution.  The evaluation is somewhat convincing, but it would benefit from using a newer standard corpus for which the results of competing methods are available.
I think the paper would also significantly benefit from a more open discussion of limitations.  For example, the use of LTL requires a continuum space to have discrete states.  This is appropriate in the two environments explored in the evaluation, but is not an inherent property of the real world.  To take the example of the MIT campus shown in Figure 1, people typically say it is located at Kendall Square, although parts of it would more appropriately be described as being in Central Square.  Cambridge's squares/neighborhoods are inherently ambiguous in a manner that makes it hard to ascribe states to locations at that level.  The LANI dataset is similar in that you can be ""at the pumpkin"" or ""at the lighthouse"" or in some vaguely defined in between position.  Rather than taking this limitation as evidence of a flaw in the method, I would be inclined to think of it as a way to better focus the method towards where it is best suited.  After all, even on the OSM dataset, a robot that was actually navigating these streets is going to experience the world by a very different set of landmarks (e.g. the tall building, the curve in the road, the new age sculpture) than a user who is viewing an overhead map.  Perhaps the method would be most effectively be employed as part of a larger system.  If so, then there is an opportunity to sketch out what components this contribution complements.

[1] Valts Blukis, Yannick Terme, Eyvind Niklasson, Ross A. Knepper, and Yoav Artzi. ""Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight"". In: Proceedings of the Conference on Robot Learning (CoRL). Osaka, Japan, October 2019.

[2] D. Misra, A. Bennett, V. Blukis, E. Niklasson, M. Shatkin, and Y. Artzi. ""Mapping instructions to actions in 3D environments with visual goal prediction"". In Conference on Empirical Methods in Natural Language Processing, 2018.
",Agreement accepted,"This paper has a bunch of really nice ideas – I appreciate the weakly supervised mapping of natural language (NL) to LTL since, yes, annotating utterances with LTL is not an easy or quick process. I also really appreciate the new complex dataset which is going to be a unique resource for the community.

That being said, this paper, in its current form, has a few issues that need to be addressed:
Evaluation metrics: I found the evaluation metrics to be weird. The paper is focused on learning LTL formulas that describes trajectories. An LTL formula, unless it is really detailed and contains a lot of safety constraints which I do not think is the case here, captures a family of trajectories, not just one. The evaluation done in this paper is to compare either the end state (when comparing to other methods) or the generated path to the ground truth path. Both of these evaluations do not make sense to me since they do not evaluate how well the LTL formula captures the original trajectory. To me, a more meaningful evaluation would be to learn the model and then check, on the testing data, whether the ground truth trajectory satisfies the LTL formula that the model predicted. Granted, this does not compare to other techniques, but the other techniques solve a different problem so I am not sure what insight I am supposed to gain from the current comparison. 

Planning: Why is the planning done over an MDP? The map has no probabilities, LTL has no probabilities, why not do the usual LTL planning by creating the cross product between the Buchi automaton and the environment graph (see my comments about related work and relevant citations below)? 

Related work: the paper is missing a lot of relevant work in the context of mapping NL to LTL, mapping trajectories to temporal logics (although not from language but it is nonetheless relevant), and synthesis (planning) with LTL. See a list of citations at the end. The ideas in this paper are novel, but they are not well situated in the relevant work that is not in the deep learning flavor. 

Other comments:
It would be really great to add to the video the data collection process – what is the interface that the turkers saw? Example trajectories and associated language would be very cool to see.

Why use a Voronoi decomposition and not use the road network? That way the work would be relevant to a ground robot and not just an aerial robot. 

Progressing Vs semantics: The semantics of LTL are not really defined in the paper and instead the notion of “progression” is defined. It would be good to explain what the difference is between them especially since some of the lines in Table 1 are equivalent to the semantic and some are not. Furthermore, shouldn’t prog(sigma_i,next phi) be prog(sigma_i+1, phi)? And the same for the eventually operator. Writing just phi is related to the next position of the sequence but the definition of prog(sigma_i,p) relates to the current position so it seems to be an inconsistent definition. 

Fig 4 has an FSA – where is it coming from? For the progression of the formula, how do we know ‘p’ became true? Should the graph on the left be annotated with ‘p’ and ‘q’?  Figure 4 is not referred to in the text – it would be useful to add a description for the figure.  

Minor comments: 
The paper uses the term “trajectories” but really the mapping is to sequences of propositions. It is worth clarifying. Trajectories implies continuous (x,y) locations. 

There are three broken citations. Search for ‘[]’

Some relevant citations:
NL to LTL for robot control:
V. Raman, C. Lignos, C. Finucane, M. Marcus, H. Kress-Gazit, Sorry Dave, I'm Afraid I Can't Do That: Explaining Unachievable Robot Tasks Using Natural Language, Robotics: Science and Systems 2013

A. Boteanu, J. Arkin, T.M. Howard, and H. Kress-Gazit, ""A Model for Verifiable Grounding and Execution of Complex Language Instructions,"" In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, Oct. 2016, pp. 2649-2654

Trajectories to temporal logic:
Bayesian Inference of Temporal Task Specifications from Demonstrations
A Shah, P Kamath, S Li, J Shah, Conference on Neural Information Processing Systems 2018

Giuseppe Bombara, Calin Belta, Online Learning of Temporal Logic Formulae for Signal Classification, European Control Conference (ECC), Limassol, Cyprus, 2018

Chanyeol Yoo and Calin Belta, Rich Time Series Classification Using Temporal Logic, Robotics: Science and Systems (RSS), Boston, MA, 2017

LTL synthesis/planning : There is a lot of work in the “formal methods for robotics” community. Specifically, you should look at the work of Hadas Kress-Gazit, Calin Belta, Richard Murray, Lydia Kavraki, Jana Tumova, Necmiye Ozay, Dimos Dimarogonas, etc. Here is a fairly recent review on the topic:
Synthesis for robots: Guarantees and feedback for robot behavior, H Kress-Gazit, M Lahijanian, V Raman, Annual Review of Control, Robotics, and Autonomous Systems vol. 1, 2018
",Agreement accepted,"The paper is clearly written and demonstrates improved performance and trainability relative to past methods,  The formulation of the problem is straightforward and effective.  I hope the authors release the OSM dataset created for this work to facilitate future comparison studies.

There are two weaknesses with the paper.  First, the navigation instructions consist primarily of fairly short phrases.  It would be helpful to see how performance changes based on the length (number of steps) of the instructions.

Second, the Skydio evaluation is somewhat confusing and different from the others.  The quadrotor does not appear to follow streets (which is fine, but should be clarified) and follows straight-line motion instead.  It's more difficult to evaluate this case because it's less clear what should be happening.  For example, in the video of the Atlanta Georgia Tech campus scenario, the robot adds an additional edge to the path near the end of its destination.  Why is this?  A more in-depth discussion and analysis of this benchmark is needed.  Currently it feels like this was just an add-on there to satisfy the requirements of submitting to a robotics conference.  I believe there are many promising applications for this work in the robotics field, such as mobile manipulation tasks that incorporate physical interactions beyond navigation.  It would be helpful to see a more detailed analysis or discussion of the robotics aspects of the approach.

Minor comment:  In IV.B., missing text in brackets: ""...LTL-based planner [ ] that...""",,,,Agreement accepted,07/14 15:00,07/14 17:00,[Not Answered],[Not Answered],,,16.0,hOlRx0ll-6k
"We consider the problem of completely covering an unknown discrete environment with a swarm of asynchronous, frequently-crashing autonomous mobile robots. We represent the environment by a discrete graph, and task the robots with occupying every vertex and with constructing an implicit distributed spanning tree of the graph. The robotic agents activate independently at random exponential waiting times of mean $1$ and enter the graph environment over time from a source location. They grow the environment's  coverage by `settling' at empty locations and aiding other robots' navigation from these locations. The robots are identical and make decisions driven by the same simple and local rule of behaviour. The local rule is based only on the presence of neighbouring robots, and on whether a settled robot points to the current location. Whenever a robot moves, it may crash and disappear from the environment. Each vertex in the environment has limited physical space, so robots frequently obstruct each other. Our goal is to show that even under conditions of asynchronicity, frequent crashing, and limited physical space, the simple mobile robots complete their mission almost surely in linear time, and time to completion degrades gracefully with the frequency of the crashes. Our model and analysis are based on the well-studied ``totally asymmetric simple exclusion process'' in statistical mechanics. ",RSS2020_Author_Agreement (1).pdf,"[Michael Amir](http://),  [Alfred M. Bruckstein](https://freddy.cs.technion.ac.il/)",Michael Amir (Technion - Israel Institute of Technology)*; Freddy Bruckstein (Technion),RSS2020_Author_Agreement (1).pdf (37974 bytes); RSS_submission_crashing_swarms (12).pdf (343890 bytes),RSS_submission_crashing_swarms (12).pdf,116.0,17.0,,1.0,Fast Uniform Dispersion of a Crash-prone Swarm,,0.226737265738773,"The paper presents an interesting algorithm with a substantial analysis.  However, I do have some suggestions for improvement.

1.  The simulation results could be improved. The results illustrate the claims of the algorithm's performance, but they are as abstracted as the underlying problem formulation, thus not adding appreciably to the strength of the paper.  The simulation results would be more effective if they were made to reflect real robots in real environments, attempting to bridge the apparent abstraction-reality gap of the problem formulation.  This could be done with open source robot simulators such as WeBots or ROS-Gazebo.

2.  The simulation results do not compare against other algorithms. The authors admit this fact, and excuse themselves given that no other algorithm seems to operate under the same assumptions as theirs. However, it would still be valuable to see the performance of other algorithms for qualitative reasons, even if the underlying assumptions are different.

3.  The substance of the paper is the performance analysis of the algorithm.  The authors do a good job of trying to make the analysis digestible and giving intuition for non-experts in this area.  However, it was not clear to this reviewer why a simpler analysis would not suffice.  E.g., if the timing randomness and robot failures were removed, and robots were instead ordered to act one at a time in a discrete time deterministic sense according to the proposed algorithm, it would seem that similar results would be straightforward to obtain.  Then introducing random robot failures on edge traversals would also be relatively straightforward to analyze.  Why then does the random timing present such a challenge?  Some higher level intuition about why this analysis is challenging, and how it becomes simpler under different assumption, would be helpful.",Agreement accepted,"This paper presents a method for covering an unknown graph with crash-prone robots and no centralized control. This is an interesting problem that has not been treated in the literature. The authors claim that it works on arbitrary graphs, and that it requires basic, indirect (implicit) communication between robots. I do believe this paper has a decent contribution but the authors would do well to rethink how the work is presented. 

While I believe this problem and solution have some significance, I was left with many questions about the applicability of this approach, perhaps due to the omission of a list of assumptions from the authors. Furthermore, I have some doubts about the claim that ""there are no restrictions on G as long as its connected"". 

Regarding the graph G, It seems from reading the work that G must have a specific structure, perhaps evenly spaced nodes such as on a grid, and that robots must be aware of this. Otherwise it is impossible for robots that do not know the graph to find a ""location"" to fill. I have my doubts as to whether this works on a generic graph, where edges vary in length. Perhaps if the authors instead claim that there are no restrictions on the environment other than connectivity, this might make more sense. But it would clearly work best in rectilinear environments. It's actually not till the end of the paper (in the conclusion!) where the authors finally state that the environment is discrete. Stating in the abstract that the environment is represented by a discrete graph and saying that the environment is discrete are two VERY different things. Furthermore, leaving that information out of the body of the paper until the conclusion makes for very difficult reading. Regardless, a discrete environment doesn't technically mean much, as it does not imply that the environment is divided perfectly into evenly-sized square cells. 

The robots also must have some type of sensing onboard to sense the environment and the distance between themselves and other robots. Unless, of course, they are moving blindly and may crash into obstacles, but it seems that the crashing the authors intended had more to do with instability or technical issues instead of crashing into obstacles. The authors should clarify what they mean by ""destructive mechanical faults"" on pg 1. 

Other questions also arise regarding ""marking"" or ""pointing to"" other robots. What technology is this modeling? How would robots mark each other, and how do robots make decisions about which direction to move in once they have found a settled robot. How do they know there is an adjacent node that is available? This question kept arising throughout the paper, even in Algorithm 1, where it states that ""if a neighbor u of v contains no robots then attempt to move to u""

It's also unclear to me how the robots are activated at random exponential waiting times of mean 1. I first took this to mean that one robot is activated randomly at waiting times of mean 1, and that once its activated it does not go out of activation unless it crashes. Then I thought that the robots can be activated on and off throughout the execution. I'm not exactly sure which it is. 

Unfortunately all of these doubts lingered as I read through the paper. The simulation and evaluation left some helpful information out as well. First, there is no discussion of the ""robot simulator."" This is not so important as this is more theoretical work, but a little info would be helpful. The authors also state that the results were averaged ""over a number of simulations"". What is this number? 2, 20, 200? This makes a very big difference in the credibility of your comparisons in Table II.

I do believe this paper has a good contribution, but it needs to be rewritten. The authors would do well to consider what assumptions have been made and how best to present them, and what technology one could actually use to ""mark"" locations in a real robot system. ",Agreement accepted,"The paper is generally easy to read and its technical content is illustrated clearly.
The theoretical analysis appears to be sound. 
The achieved results are significant and contributes to advance the state of the art, according to what stated by authors in Section I.

Personally, I am not convinced about the practical relevance of the proposed approach. Authors quickly mention that their “model in fact captures many of the relevant traffic phenomena that will occur in real life implementations”, but no other information is provided to assess how and why the proposed model could be useful in practice. Do authors expect to implement robots behaving according to Algorithm 1 or do authors intend to apply their results in order to improve current coverage approaches (in this case, which approaches? how could they be improved?). 

The presentation is not linear and suffers of several repetitions between Sections I and II. Section I is a sort of extended abstract that discusses not only context and motivation for the work, but also several details about assumptions and results (one example among many: constant c is introduced in Section I and never mentioned in Section II). It would be better for authors to significantly shorten Section I, moving most of the content to Section II (and merging it with what is already there).

It would be useful to show a representation of G(t) for the first and second stage of the example reported in Fig. 1.

Fig. 2 is not referenced in text.

The discussion on multiple source vertices (Section III) seem to imply that the different trees are build independently from different sources. However, such trees will touch at some vertices. How are interactions between agents building different trees managed in these situations? 

Extra space in caption of Table II.",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,,,,17.0,USCXMhQqUec
"In this paper, we introduce and tackle the simultaneous enhancement and super-resolution (SESR) problem for underwater robot vision and provide an efficient solution for near real-time applications. We present Deep SESR, a residual-in-residual network-based generative model that can learn to restore perceptual image qualities at 2x, 3x, or 4x higher spatial resolution. We supervise its training by formulating a multi-modal objective function that addresses the chrominance-specific underwater color degradation, lack of image sharpness, and loss in high-level feature representation. It is also supervised to learn salient foreground regions in the image, which in turn guides the network to learn global contrast enhancement. We design an end-to-end training pipeline to jointly learn the saliency prediction and SESR on a shared hierarchical feature space for fast inference. Moreover, we present UFO-120, the first dataset to facilitate large-scale SESR learning; it contains over 1500 training samples and a benchmark test set of 120 samples. By thorough experimental evaluation on UFO-120 and several other standard datasets, we demonstrate that Deep SESR outperforms the existing solutions for underwater image enhancement and super-resolution. We also validate its generalization performance on several test cases that include underwater images with diverse spectral and spatial degradation levels, and also terrestrial images with unseen natural objects. Lastly, we analyze its computational feasibility for single-board deployments and demonstrate its operational benefits for visually-guided underwater robots.",Author_Agreement_Form.pdf,Md Jahidul Islam (University of Minnesota Twin Cities)*; Peigen Luo (University of Minnesota-Twin Cities); Junaed Sattar (University of Minnesota),Md Jahidul Islam (University of Minnesota Twin Cities)*; Peigen Luo (University of Minnesota-Twin Cities); Junaed Sattar (University of Minnesota),Deep_SESR_RSS_20.pdf (3952796 bytes); Author_Agreement_Form.pdf (210315 bytes),Deep_SESR_RSS_20.pdf,121.0,18.0,,1.0,Simultaneous Enhancement and Super-Resolution of Underwater Imagery for Improved Visual Perception,http://irvlab.cs.umn.edu/image-enhancement-and-super-resolution/deep-sesr,0.046182859227403,"The problem of simultaneous super-resolution and image enhancement is relevant because it avoids the possible amplification by one step of the artifacts introduced by the other one, especially for underwater images. The paper is well written and the authors provide a thorough comparison with the SOTA for both underwater and terrestrial images. Also, the integration of additional saliency constraints is a good idea. The qualitative results are appreciated although the image quality prevents the reader from inspecting the image’s details, but this is expected.

Some experimental aspects lack details that would make the reading smoother.

* The data generation is not explicit enough. The authors should add that the collected images from oceanic explorations are the ‘undistorted and
high-resolution’ ground-truth. The distorted images are synthetically generated using the CycleGAN network. This generator of CycleGAN is
trained to transform these images into distorted underwater images so that the discriminator can not tell whether it is an actual distorted image
or a synthetic one.

* The low-resolution image generation is explicited in the paper. However, I don’t understand the relevance of the F split as it is made of images from
both U and O.

* The network takes a low-resolution distorted image and has three outputs: a saliency map S, a low-resolution enhanced image X, and a high- resolution (HR) enhance image Y. To compare their model against existing approaches, the authors evaluate the enhancement in Y (or X, I am not sure) with previous enhancing methods, and they evaluate the high-resolution of Y against previous HR methods.

The main motivation of the paper is to address the resolution and the enhancement problems simultaneously. So I would have liked to see a
comparison of the enhanced HR image Y compared with an image that would be first enhanced then upsampled, and vice-versa, with the top
1SOTA method. A comparison in terms of both image quality and computational time would have been relevant. Also, the enhancement in the high-resolution output (i.e. Y) is not evaluated.

* The authors rely on various metrics that assess the image quality. Additional information on their interpretation and their relevance would help
the reader to better appreciate their results.

* In addition to the main contributions listed above, the authors announce that they will release the underwater data used for training and testing: it is made of 150 testing images and 1500 training images. The images are deemed undistorted and high resolution and they are transformed into distorted ones using an existing style transfer network. They are then transformed into low-resolution manually. This dataset release is well appreciated.
",Agreement accepted,"Overall the paper is well written and clearly presents the structure of the proposed learning network and the rationale for the network structure.  Figures serve well to illustrate the results and the results suggest that the system may be suitable for running in realtime on future robotic deployments.

The question of how the ground truth was actually generated was not addressed in the paper.  Comparisons against ground truth imagery were presented but there is no discussion of whether these ground truth images were hand labelled or derived from some other source.  A discussion of the process for generating the ground truth is likely to be warranted given that finding ground truth imagery for underwater datasets is a challenging task in itself.",Agreement accepted,"The authors provide a deep learning framework for doing simultaneous enhancement and super-resolution of images. Loss functions are presented to train the network. The network is implemented and compared to a number of state of the art techniques and generally shown to outperform them.

The domain addressed is topical for RSS, and a solution in the underwater domain is sorely needed. The primary interest from readers will be in the network architecture and in the design of the loss functions. This paper will likely be of interest to deep learning perception researchers working on robotics applications.

The paper well written and flows well. The description of the network architecture and loss functions is comprehensive. The literature search is, to my knowledge, complete. The results are impressive in terms of the number of algorithms compared to. The proposed algorithm generally outperforms the state of the art on most datasets.

The claim is made that this framework is generalizable to terrestrial images, but this claim is somewhat weak. The results show competitive performance, but the authors state this performance could be improved. More work would need to be done to determine the best way to tune the network for these types of images. This goes towards a broader problem of what level of expert tuning is required to move an algorithm to another domain. While this is not something to be solved in this paper, it's not clear how much the terrestrial results add at this point. I would suggest not stressing this in the introduction because it's really more of an afterthought in the results.",,,,Agreement accepted,07/14 15:00,07/14 17:00,,https://youtu.be/wEkTu2CPW-g,,,18.0,8zBdFxaK4Os
"Demand for high-performance, robust, and safe autonomous systems has grown substantially in recent years. These objectives motivate the desire for efficient risk estimation that can be embedded in core decision-making tasks such as motion planning. On one hand, Monte-Carlo (MC) and other sampling-based techniques provide accurate solutions for a wide variety of motion models but are cumbersome in the context of continuous optimization. On the other hand, “direct” approximations aim to compute (or upper-bound) the failure probability as a smooth function of the decision variables, and thus are convenient for optimization. However, existing direct approaches fundamentally assume discrete-time dynamics and can perform unpredictably when applied to continuous-time systems ubiquitous in the real world, often manifesting as severe conservatism. State-of-the-art attempts to address this within a conventional discrete-time framework require additional Gaussianity approximations that ultimately produce inconsistency of their own. In this paper we take a fundamentally different approach, deriving a risk approximation framework directly in continuous time and producing a lightweight estimate that actually converges as the underlying discretization is refined. Our approximation is shown to significantly outperform state-of-the-art techniques in replicating the MC estimate while maintaining the functional and computational benefits of a direct method. This enables robust, risk-aware, continuous motion-planning for a broad class of nonlinear, partially-observable systems.",rss_agreement.pdf,"[Kris Frey](http://acl.mit.edu/people/kfrey), [Ted Steiner](https://scholar.google.com/citations?user=M6AOoBcAAAAJ), [Jonathan P. How](https://www.mit.edu/~jhow)","Kristoffer Frey (MIT)*; Ted Steiner (Charles Stark Draper Laboratory, Inc.); Jonathan How (MIT)",rss_agreement.pdf (104335 bytes); root.pdf (800363 bytes),root.pdf,1290.0,19.0,,1.0,Collision Probabilities for Continuous-Time Systems Without Sampling,,0.5345925212579979,"The paper is not an easy read. Many equations without much intuition are provided nor a clear definition is provided. For instance, even the very first equation 1 does not provide full descriptions of x_s, y_s, f, c. Due to this, despite the author's attempt to provide a rigorous treatment of the subject, I could not fully appreciate the effort. Another reason is that the authors did not do a good job of clearly explaining the main contribution of the work or how novel the work is, which I was expecting in the very beginning of this manuscript. Finally, I was not sure how significant the result is even though the authors provide a single table in Table 1 to show comparisons, not to mention that the tested experiment is rather limited (four Dubin car examples with a similar setting).",,"This paper presents a method to compute less conservative approximate of chance-constrained control for continuous-time problems. In particular, it uses the Lagrangian formulation that convert risk-constrained into risk-minimization, and propose an approximation of this formulation based on the concept of first passage time, rather than time discretisation. Convergence guaranteed is provided very thoroughly and simulation results for controlling 2nd order Dubins car in environment populated by obstacles is provided.

I think the problem of chance-constrained control in continuous-time is interesting and the paper's attempt to solve the problem without discretising the time domain is interesting and could be very useful. 

Further, the derivation on how to evolve distribution for the relax Lagrange formulation of chance-constrained control via the concept of first passage time could be useful and thorough. 

Several feedback

1. I think avoiding fixed time discretisation is interesting and useful. However, the proposed approximation (sec. IV) seems to go back on using fixed time discretisation. I think some elaboration on how this affects the exact difficulty that the paper is trying to avoid would be needed.

2. It would be interesting if the provided example include a scenario where the proposed method is superior than Monte Carlo approach. I do understand that the contribution of the paper is more on the theoretical side, and this does not effect my score. However, considering Monte Carlo is used quite a lot to estimate distribution, I think it would have much more impact if such an example is provided. 

3. I think eq. (19), right hand side, P(... | z_t = z) should be P(... | z_s \in dz) and P(z \in dz) should be P(z_s \in dz).  ",Agreement accepted,"Originality: Good
Quality: Good
 Clarity: Good
Significance: Good

Summary: The authors address the problem of estimating and minimizing failure probabilities in the context of continuous motion planning by proposing a light-weight approximation method. The proposed method avoids estimating the anthropic belief and produces a conservative risk estimate with minimal computation.  Further the method efficiently discretizes the time and reduces the planner brittleness. The paper is written well and the authors provide good theoretical analysis to support their claims. 

Comments:
1.	In this work, only the accuracy of the proposed method is provided. No result, which describes the computation overhead of the method is given. The trade-off between speed vs accuracy is an interesting aspect, and it is difficult to measure the overall efficiency without the computational overhead.
2.	It may be better for the readers if the authors specify all the contributions in a separate paragraph in the Introduction section.
3.	No Future work is given.
4.	It is not clear what are the limitation of the Eq.(6) in comparison of Eq. (4).
5.	It may be better for the readers if the authors briefly describe the intuitive ideas at the beginning of each section.
",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,,,,19.0,7nZgNxWf92Q
"This work contributes an event-driven visual-tactile perception system, comprising a novel biologically-inspired tactile sensor and multi-modal spike-based learning. Our neuromorphic fingertip tactile sensor, NeuTouch, scales well with the number of taxels thanks to its event-based nature. Likewise, our Visual-Tactile Spiking Neural Network (VT-SNN) enables fast perception when coupled with event sensors. We evaluate our visual-tactile system (using the NeuTouch and Prophesee event camera) on two robot tasks: container classification and rotational slip detection. On both tasks, we observe good accuracies relative to standard deep learning methods. We have made our visual-tactile datasets freely-available to encourage research on multi-modal event-driven robot perception, which we believe is a promising approach towards intelligent power-efficient robot systems.",RSS2020_Author_Agreement.pdf,"Tasbolat Taunyazov, Weicong Sng, Brian Lim, Hian Hian, Jethro Kuan, Abdul Fatir, Benjamin Tee, [Harold Soh](https://haroldsoh.com)",Tasbolat Taunyazov (National University of Singapore); Weicong Sng (National University of Singapore); Brian Lim (National University of Singapore); Hian Hian See (National University of Singapore); Jethro Kuan (National University of Singapore); Abdul Fatir Ansari (National University of Singapore); Benjamin Tee (National University of Singapore); Harold Soh (National University Singapore)*,RSS2020_Author_Agreement.pdf (123317 bytes); Event_based_Visual_Tactile_Representation_Learning-19.pdf (6375864 bytes),Event_based_Visual_Tactile_Representation_Learning-19.pdf,117.0,20.0,,1.0,Event-Driven Visual-Tactile Sensing and Learning for Robots,https://clear-nus.github.io/visuotactile/,0.029184769788233,"The idea of using event-based tactile sensing is very interesting, and the combination with event-based vision is also new. However, the paper should improve on the following aspects to better justify the contribution:

- The authors consider the design of the event-based tactile sensor is a major contribution of the paper, so that they should provide more details of the sensor, regarding the design and evaluation. There is no evaluation of the sensor in the paper. In addition, it is unclear why the authors consider the sensor 'event-based'. Does it mean the sensor can hardly measure the static pressure? Or does it mean the sensor can hardly measure the magnitude of the contact force/pressure value or derivative?

- It is unclear why the authors build this event-based sensory system. What're the advantages over the RGB(D) camera and tactile sensors that measure force/pressure constantly? Especially in the classification task, the RGB(D) vision is expected to make a good performance. 

- The authors need to report more about their experimental data, especially when they are training a deep neural network model. How many datapoints are there in the dataset? How did the authors divide the training set, validation set, and test set? What did the authors do to make sure their dataset has enough variance? Particularly, for the rotational slip detection task, the authors experimented with only one object. The authors need to justify the model could detect rotational slip in more common cases. 

- In the classification task, there are two variables: types of objects, and weight of the objects. It will be helpful if the authors could report how well the model  could differentiate the types of objects, and how well it can differentiate the weight of objects. ",Agreement accepted,"This paper proposed an event-driven visual-tactile perception system, comprising a novel biologically-inspired tactile sensor and multi-modal spike-based learning.  Visual-Tactile Spiking Neural Network (VT-SNN) enables fast perception when coupled with event sensors. I agree the the topic is important and the idea of visual-tactile fusion is good. However, I have the following comments:

(1)The figure shows the visual-tactile fusion SNN. It is new from the the viewpoint of SNN, but is naive since the some intrinsic relation between visual and tactile modalities is not considered. For example, the alignment problem.
(2)The neuro model is nearly borrowed from exsting literature of SRM
(3)The exists a minor error in equation (1)
(4)The adopted objects for verification is simple

  In totally, I think this work is good since it uses the SNN for visual-tactile fusion, and a new sensor is proposed. However, I feel the content is too diversified and I suggest the author to focus on one aspect. 
",,"There are inconsistent descriptions about the number of object classes in the paper.
“a third data-collection experiment that expands the number of grasped items to 36 different objects”
“Visual-tactile event sensor datasets comprising more than 50 different object classes”
“36 object classes with various visual and tactile profiles”

With 36 (or 50?) classes available, the first experiment is conducted using only 4 kinds of containers. How does your VTSNN model perform on all object classes?

The second experiment (binary classification) seems to easy for visual sensory (100% successful rate). It does not show any improvement because of the tactile sensor.

For Equation 2, instead of using hand-crafted regression targets, why not use a classification loss (e.g. cross-entropy) instead?

The visual model is an SNN operating on pixel differences. Today’s state-of-the-art models are mostly convolutional networks. What’s would the performance be if using a light-weight CNN?

How does the SNN model scale with the increase of taxels? A denser taxel arrangement would have a stronger local correlation between nearby taxel signals, how does that affect your model?
",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,,,,20.0,zPlrqtjEcUY
"In this paper, we study the resilient diffusion problem in a network of robots aiming to perform a task by optimizing a global cost function in a cooperative manner. In distributed diffusion, robots combine the information collected from their local neighbors and incorporate this aggregated information to update their states. If some robots are adversarial, this cooperation can disrupt the convergence of robots to the desired state. We propose a resilient aggregation rule based on the notion of \emph{centerpoint}, which is a generalization of the median in the higher dimensional Euclidean space. Robots exchange their $d$-dimensional state vectors with neighbors. We show that if a normal robot implements the centerpoint-based aggregation rule and has $n$ neighbors, of which at most $\lceil\frac{n}{d+1}\rceil - 1$ are adversarial, then the aggregated state always lies in the convex hull of the states of the normal neighbors of the robot. Consequently, all normal robots implementing the distributed diffusion algorithm converge resiliently to the true target state. We also show that commonly used aggregation rules based on the coordinate-wise median and geometric median are, in fact, not resilient to certain attacks. We numerically evaluate our results on mobile multi-robot networks and demonstrate the cases where diffusion with the weighted average, coordinate-wise median, and geometric median-based aggregation rules fail to converge to the true target state, whereas diffusion with the centerpoint-based rule is resilient in the same scenario.",RSS2020_Author_Agreement.pdf,"[Jiani Li](https://jianili.github.io/jianili/), [Waseem Abbas](http://www.wabbas.com/), [Muddasir Shabbir](https://itu.edu.pk/faculty-itu/muddasir-shabbir/), [Xenofon Koutsoukos](http://www.vuse.vanderbilt.edu/~koutsoxd/)",JIANI LI (Vanderbilt University)*; Waseem Abbas (Vanderbilt University); Mudassir Shabbir (Information Technology University); Xenofon Koutsoukos (Vanderbilt University),RSS2020_Author_Agreement.pdf (146195 bytes); CenterpointDiffusion_FINAL.pdf (887859 bytes),CenterpointDiffusion_FINAL.pdf,1252.0,21.0,,1.0,Resilient Distributed Diffusion for Multi-Robot Systems Using Centerpoint,,0.25800922711548,"* Main comments
Unfortunately, the theoretical analysis of the method is not as rigorous as the rest of the paper:
- In the inline equation before eq. (8): It is not clear why $\frac{2\mu_l}{L}-\mu_l^2$ becomes $\mu_l^2$. This step is crucial for the proof of the main theoretical result of the paper, and it does not seem to be correct.
- Section V.D: the authors seem to suggest that the proposed method is a particular case of DLMS, and so it inherits the same benefits in terms of MSD. However, the authors need to prove or cite a result showing that the relation between DLMS and non-cooperative LMS results holds also for time-varying weights (which is how the proposed algorithm falls into the DLMS category).
- Lemma 1: in order to make the proof valid, the authors need to provide (either prove or refer to) a theorem stating that the center point is always in the convex hull of the points (this is rather simple, but it needs to be done for completeness). Also, Lemma 1 should be stated in its more generic (and powerful) form: for any subset of $\ceil{\frac{n}{d+1}}-1$ points, the centerpoint cannot be in their convex hull. This is because the centerpoint does not know which points are the Byzantine agents, so the result should be invariant to their choice.
Finally, it would be nice if the authors could test the performance of the algorithm where the squared 2-norm in the cost in Section VI.A is substituted with an 1-norm; in this case the cost separates across coordinates, so the median-based algorithm should return to work robustly.

Overall, the paper is based on an innovative idea, but it is not publishable in its current form due to the theoretical analysis.


* Additional comments
- Abstract: ""optimizing global cost"" should be ""optimizing a global cost""
- Page 3, column 2: ""In one dimension, median"" should be ""In one dimension, the median""
- Lemma 1: It seems that there should be some assumption on the minimum n.
- Section V.C:
  - Property 1: the customary assumption of Lischitz continuity uses the distance between two arbitrary points (not just a single point and the origin), and links B and D. As it is, this definition is not clear (can B depend on w?), and it does not imply that the function is actually continuous (e.g., take a J which is a ""staircase"")
  - Property 2: the authors invoke co-coercivity of the function, which requires simple convexity (mu=0)
- Page 5, column 1, first equation: \mu_l should be \mu_k
- Eq (4), second line: the term containing \eta and \rho should have a plus sign instead of a minus sign.
- Page 8, column 1: ""sates"" should be ""states""
",Agreement accepted,"Overall, the paper is well written and the ideas are communicated clearly. Given the increasing interest in resilient strategies, the paper makes an important contribution. The analysis in the paper follows logically. A minor comment, it would be better to move the related work section to after the introduction for better readability.",,"The paper has proposed an aggregation rule based on center points for distributed fusion for multi-robot systems in the presence of adversarial robots. Such center points can  be achieved based only on local  information available to each robot. The proposed method can guarantee the aggregated state always lie in the convex hull of the states of normal neighbors. Convergence analysis and numerical simulations are provided to validate the proposed method. 

While the whole paper looks very nice in general, especially in quality and clarity,  the key idea to achieve a state in the convex hull of normal states is not new. There has recently been several other existing methods to achieve such a convex combination without knowing which state is malicious.  To name a few, please refer to the followings:
[1]. H. Mendes, M. Herlihy, N. Vaidya and V. Garg, Multidimensional agreement in byzantine systems, Distributed Computing, 28 (2015), 423–441
[2] H. Park and S. A. Hutchinson, Fault-tolerant rendezvous of multirobot systems, IEEE Transactions on Robotics, 33 (2017), 565–582.
[3] L. Tseng and N. H. Vaidya, Asynchronous convex hull consensus in the presence of crash faults, in Proceedings of the 2014 ACM symposium on Principles of distributed computing, ACM, 2014, 396–405
[4]. X. Wang, S. Mou, S. Sundaram. A resilient convex combination for consensus-based distributed algorithms. Numerical Algebra, Control and Optimization. 269-281, 9(3), 2019. 

The authors could compare their methods with the above papers and further justify their contribution. 
",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,https://github.com/JianiLi/Centerpoint_resilient_diffusion,https://youtu.be/Y9sdOKLKs24,,,21.0,lImjPWiANRA
"Uncooled microbolometers can enable robots to see in the absence of visible illumination by imaging the “heat” radiated from the scene. Despite this ability to see in the dark, these sensors suffer from significant motion blur. This has limitedtheir application on robotic systems. As described in this paper, this motion blur arises due to the thermal inertia of each pixel. This has meant that traditional motion deblurring techniques, which rely on identifying an appropriate spatial blur kernel to perform spatial deconvolution, are unable to reliably performmotion deblurring on thermal camera images. To address this problem, this paper formulates reversing the effect of thermal inertia at a single pixel as a Least Absolute Shrinkage and Selection Operator (LASSO) problem which we can solve rapidly using a quadratic programming solver. By leveraging sparsity anda high frame rate, this pixel-wise LASSO formulation is able to recover motion deblurred frames of thermal videos without using any spatial information. To compare its quality against state-of-the-art visible camera based deblurring methods, this paper evaluated the performance of a family of pre-trained object detectors on a set of images restored by different deblurring algorithms.All evaluated object detectors performed systematically better on images restored by the proposed algorithm rather than any other tested, state-of-the-art methods.",rss2020_author_agreement_signed_srmani.pdf,Manikandasriram Srinivasan Ramanagopal (University of Michigan)*; Zixu Zhang (University of Michigan); Ram Vasudevan (University of Michigan); Matthew Johnson Roberson (University of Michigan),Manikandasriram Srinivasan Ramanagopal (University of Michigan)*; Zixu Zhang (University of Michigan); Ram Vasudevan (University of Michigan); Matthew Johnson Roberson (University of Michigan),rss2020_srmani_hysteresis_of_microbolometers.pdf (4222672 bytes); rss2020_author_agreement_signed_srmani.pdf (708860 bytes),rss2020_srmani_hysteresis_of_microbolometers.pdf,1238.0,22.0,,1.0,Pixel-Wise Motion Deblurring of Thermal Videos,https://fcav.engin.umich.edu/papers/pixelwise-deblurring,0.17639411721240197,"The paper makes a significant novel contribution in my opinion; it is well-written and well-presented, and the maths is consistent. There are some minor comments below to improve the latter. I don’t have major criticism, but I wanted to point out the use of the term hysteresis: I am not convinced that hysteresis is the right term to describe the main phenomenon underlying the blur in thermal images... How about ""thermal inertia"" or the like? See for instance https://en.wikipedia.org/wiki/Hysteresis . To quote: “…where there are different values of one variable depending on the direction of change of another variable…” and: “Systems with hysteresis are nonlinear, and can be mathematically challenging to model”. All of these characteristics of hysteresis are decisively not the case here. I don’t want to appear as a nit-picker, but I think this will be a core reference for the thermal deblurring papers to come, which makes correct use of terminology all the more essential.

Here are some smaller points for improvement:
- Please introduce the LASSO acronym in the abstract.
- substrate -> substrate
- Please define the indicator function in (12).
- Eqn. (12): it would be good to say that this is simply a piece-wise constant signal with K_n equal-length intervals already here to give the reader easier access to the maths employed and the intuition behind it.",Agreement accepted,"The paper is well written, original and with a clear theoretical contribution that translates into significant results.
Experiments compare the proposed method against five deblurring methods in the state of the art, including learning-based methods, and the proposed method outperforms them in the metrics utilized: visual quality and the output of an object detector.

Other comments:
- Some symbols are not always explained. Example: function II(x) in Eq (12) or (14b) is not introduced
- The absence of ground truth is not desirable. It would be beneficial to try to devise and use image quality metrics for thermal cameras, similar in spirit to the ones developed for natural images (SSIM, etc.). Relying on the results of a pre-trained object detector to justify that the method is better is not the most satisfying approach.
- There is a gap in the content of the text at the end of Section 1, when the organization of the paper is introduced.
- ""...every A satisfying (11) ..."" -> strictly speaking A does not appear in (11). Maybe the authors refer to another equation, such as (16)-(17). This reference to ""A satisfying (11)"" appears also in  Section IV.
- It is difficult to see the 11 ms in Fig. 3.
- In the conclusion, the last sentence about ""the blur in thermal cameras can be explained by a fixed kernel that can be estimated..."" needs to be revised, since it is not illustrated in the paper: Sections 3 and 4 do not talk about the kernel, and Section 5 does not plot any kernel. So, what kernel are the authors referring to?
- References need to be revised: the acronyms are not capitalized and sometimes the publication venue is missing [32].

Suggestions:
- Section 1 could be split into introduction (& contributions) and the related work.
- I think it would be nice to mention in the introduction that the linear system of equations arises from discretizing at high rate a differential equation that models the thermal image formation process.
- The introduction of the method in Eq. (12) could benefit from using an easier to follow description, such as ""we consider the class of signals given by piecewise constant functions..."". This appears much later in the text (Assumption 1). I also suggest to include the clarification in Folland's book: that the chosen class of functions is dense in L1 in the L1 metric. I think it makes it easier to follow.
- Why not use (continuous) piecewise linear functions for a better approximation?
- Please include units whenever possible, e.g., standard deviation 0.5 (Celsius?).
- It would be nice to show some sensitivity analysis, to show how the algorithm behaves as its main parameter are changed.
",Agreement accepted,"The paper is well reasoned and presents an interesting approach to forming and solving the linear set of equations to deblur microbolometer images using temporal filtering.  I have a few points that would greatly improve the readability and connection to time series analysis.

1) Hysteresis is used incorrectly throughout the paper, and should be removed.  The authors are not modeling hysteresis in any way, but instead simple transient response of the pixel sensor. This is highly confusing and unnecessary, just replace hysteresis with transient response everywhere.  

2) It should be noted that the dataset is using static cameras with moving objects, and that the background, a large proportion of the image in all scenes presented, is at steady state such that no deblurring is required.  This is why the sparsity arguments work, and makes the method extremely time-consuming and problematic for the stated application of robotics.  If every pixel in the image is affected by transients, does the problem become overly computationally complex?

3.  It is clear that both temporal and spatial effects should be corrupting each pixel, as the underlying signal from a motion blurred pixel is driven by the source at each timestep, and that source moves to an adjacent pixel at the next time step.  The authors conclusions should be that the temporal effect outweighs the spatial effect, not that their model is correct.  As such, it seems the authors overstate their conclusions in the intro and results, and more performance improvement may be possible if a spatial regularization was also included in solving the resulting source intensity problem over the image.

4) Finally, the authors over-complicate the presentation of multiple standard components in their formulation, choosing to derive or define without connecting to well established methods in signal processing and linear systems.  For example, Equation 2 is simply a particular form of numerical integration (ZOH), which could easily be done any number of ways. ",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,,,,22.0,w6Z-9ykMDBA
"In this paper, we present an integrated, model-based system for state estimation and control in dynamic manipulation tasks with partial observability. We track a belief over the system state using a particle filter from which we extract a Gaussian Mixture Model (GMM). This compressed representation of the belief is used to automatically create a discrete set of goal-directed motion controllers. A reinforcement learning agent then switches between these motion controllers in real-time to accomplish the manipulation task. The proposed system closes the loop from joint sensor feedback to high-frequency, acceleration-limited position commands, thus eliminating the need for pre- and post-processing.  We evaluate our approach with respect to five distinct manipulation tasks from the domains of active localization, grasping under uncertainty, assembly, and non-prehensile object manipulation. Extensive simulations demonstrate that the hierarchical policy actively exploits the uncertainty information encoded in the compressed belief. Finally, we validate the proposed method on a real-world robot.",form.pdf,Florian Wirnshofer (Siemens AG)*; Philipp Sebastian Schmitt (Siemens AG); Georg von Wichert (Siemens AG); Wolfram Burgard (University of Freiburg),Florian Wirnshofer (Siemens AG)*; Philipp Sebastian Schmitt (Siemens AG); Georg von Wichert (Siemens AG); Wolfram Burgard (University of Freiburg),form.pdf (1794031 bytes); rss.pdf (942288 bytes),rss.pdf,123.0,23.0,,1.0,Controlling Contact-Rich Manipulation Under Partial Observability,[Not Answered],0.7077464550796129,"In this paper, the authors propose an integrated system for state estimation and  control in dynamic manipulation tasks with partial observability. The system is provided only with an initial belief of the state at the starting time and, then, only joint states are received as feedback.  The pipeline they designed consisted of the following steps: 1) they track the belief over the system state using a particle filter; 2) they compressed the belief in a GMM model; 3) they use the GMM model to automatically create a discrete set of goal-directed motion controllers; 3) they finally use RL to learn a high-level switcher which selects the low-level motion controllers.

Even though the results are interesting and promising, the paper is not clear and lacks of explanation in several points.
For this reason, I'm not convinced about the usefulness and relevance of the proposed method, in that it sounds over complicated and not so clear. I think a better shape of the paper would help in evaluating the contributions better.

Here after, I'll give more detailed feedback on each section.

Abstract:
In my opinion, saying immediately why the environment is partially observable would value the paper much more.
In general, the authors never say explicitly why the system is PO, making the reading not to clear at a first glance.

Introduction:
Again, also here the PO is not properly explained. It seems the authors attribute the PO to the fact that the system has high-dimensional state and action space, nonlinear dynamics, multimodal distributions and real-time constraints, while it is instead because the only feedback is given by the joint states. Even though this is written, it's not explained properly.
Also, what does this sentence mean: ""Our system considers the full state space of objects""?

Related work:
This section includes several works although they are presented in a confusing way in my opinion. A better guiding thread of the speech would improve this section a lot.

Problem formulation and background:
I would not highlight the richness in contact of the considered manipulation tasks. All manipulation tasks require contact with the object and, in particular, the ones addressed in this paper do not require so much contact, in that they mostly consist of pushing tasks.

Controlling manipulation under uncertainty:
In section C., the authors do not explain how the low-level Cartesian controllers are generated, although this seems to be the central contribution. 
In my opinion, equations (8) and (9) make the reader loose the focus. I would rather explain more why acceleration-resolved controllers are advantageous since it is not clear enough from the authors' explanation.
Also the reinforcement learning setting in section D. is not very well explained.

Implementation and results:
The authors should justify more why they use that RL particular training, instead of relying on more standard ways of training policies.
The benchmarks problems chosen for the evaluation are very nice and challenging.
Results are good and pretty clear, even though Fig. 8 could be either explained better or replaced with another kind of plot, since it is not very intuitive. 
Regarding the real world evaluation, more details would be required to better evaluate the quality of the results. Also, is the evaluation executed as zero-shot transfer? Mentioning that would be clearer.

Discussion:
It's good to read that the authors are aware of some of the limitations of their work.",Agreement accepted,"The paper proposes an interesting approach to the challenging problem of hierarchical learning in manipulation. The approach is well demonstrated in the experimental section and the belief-space formulation is interesting and sets the contribution apart from other techniques. I also enjoyed the introduction section and thought that the related work section was comprehensive.

I have three comments that I believe may improve the paper. First, in section III.B. the authors mention that z_t is the only feedback observation, denoting the noisy measurements of the states of the robot and does not include the object. However, in the following subsections the particle filtering and belief propagation algorithms operate on the uncertainty of the object. There is some disconnect here and the wording/explanation could be clarified here. Following on this point, the approach depends on an a priori specific number of Gaussian Mixtures to collapse the particles used to propagate belief, but there is no explicit mention/ablation of the effect of variation of their count. There is also an implicit assumption that objects can be detected, counted, and tracked which would could be made explicit early on in the paper. This is a sharp distinction w.r.t. to other approaches that operate on the raw sensory data.

Second, particle depletion seems to be a significant challenge w.r.t. the approach. Particle depletion is commonly encountered in state-estimation, and the paper takes one approach to injecting particles in the physics representations to attempt to fix this issue. This is a solid idea. In the experimental section, the authors attribute some loss in performance to this issue but it appears that this was not the case in the simulation studies. If this is the case, why does particle depletion play a bigger role in real experiments? Any guesses would be extremely helpful to others deploying this or similar approaches.

Third, there is no clear explanation on how the t_i targets for each component policy are chosen w.r.t. to the GMM beliefs of the objects in the scene. There seems to be some hand-engineering here such as making sure contact-free motion is executed before the robot pushes the object. If this is the case, there should be a clarification in the text alongside a more explicit explanation of determining t_i. It would be interesting to see the effect of ablation for these choices on the performance of the approach given a fixed task.",Agreement accepted,"While the interesting proposed frame work, the evaluation (especially experimental) is not enough. Rather, while the authors performed some elemental experiment, it seems that it is difficult to implement the proposed method to the real experiment. The contact condition will be very different between simulation and experiment. Moreover, the effect of sensor noise is included in the real experiment (although the simulation is nicely performed). This is especially be crucial under nonprehensile manipulation. 

Justification of the compressed expression of the particle filter is not enough. 



 ",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,[Not Answered],https://www.youtube.com/watch?v=o4zMcKzbY-M,,,23.0,wsLEAaMJ3s8
"Robotic reinforcement learning (RL) holds the promise of enabling robots to learn complex behaviors through experience. However, realizing this promise for long-horizon tasks in the real world requires mechanisms to reduce human burden in terms of defining the task and scaffolding the learning process. In this paper, we study how these challenges can be alleviated with an automated robotic learning framework, in which multi-stage tasks are defined simply by providing videos of a human demonstrator and then learned autonomously by the robot from raw image observations. A central challenge in imitating human videos is the difference in appearance between the human and robot, which typically requires manual correspondence. We instead take an automated approach and perform pixel-level image translation via CycleGAN to convert the human demonstration into a video of a robot, which can then be used to construct a reward function for a model-based RL algorithm. The robot then learns the task one stage at a time, automatically learning how to reset each stage to retry it multiple times without human-provided resets. This makes the learning process largely automatic, from intuitive task specification via a video to automated training with minimal human intervention. We demonstrate that our approach is capable of learning complex tasks, such as operating a coffee machine, directly from raw image observations, requiring only 20 minutes to provide human demonstrations and about 180 minutes of robot interaction.",RSS2020_Author_Agreement.pdf,"[Laura Smith](https://lauramsmith.github.io), [Nikita Dhawan](https://www.linkedin.com/in/nikita-dhawan-7a4a29149/), [Marvin Zhang](http://marvinzhang.com),  [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/), [Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/)",Laura Smith (UC Berkeley)*; Nikita Dhawan (UC Berkeley); Marvin Zhang (UC Berkeley); Pieter Abbeel (UC Berkeley); Sergey Levine (UC Berkeley),RSS2020_Author_Agreement.pdf (116992 bytes); AVID__RSS2020_final.pdf (2793611 bytes),AVID__RSS2020_final.pdf,1258.0,24.0,,1.0,AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos,https://sites.google.com/view/rss20avid,0.357741278071461,"(Originality)

The main originality in the work is the full system to go from videos of human demonstrations to a robot policy that can solve a multi-stage task. The authors break this problem up into several pieces. They propose to train an image translation model to translate from human demonstration videos to robot videos to account for differences in morphology. Next, they manually segment the human videos at stage boundaries into instruction images that are used for training a model-based RL agent to reach these instruction images and solve each part of a task in sequence. 

The authors leverage a latent-space model-based RL algorithm along with a learned classifier that provides rewards for reaching instruction images. They use the classifier confidence to dictate whether additional human labels of success and failure should be solicited, and whether to try and autonomously reset to the start of a stage to try again, in order to limit human intervention in the system.

There have already been several works that tackle each of the aforementioned pieces - including using CycleGAN for image translation, latent-space MPC for model-based RL, doing RL with autonomous resets, and using a learned classifier as a reward function in real robot RL along with human queries. The main novelty of the paper is a system that combines all of these approaches together, and demonstrating the efficacy of the system over alternative choices.

(Quality)

The authors compare against a large set of baselines and ablations, and the empirical evaluations are good. They showcase the value of instruction images, using latent-space planning (compared to planning directly in image space) and the value of stage-wise training and resetting through the classifier (when compared to a method like BC). 

However, although the tasks shown are multi-stage, the action space seems to have been severely restricted, making the tasks significantly easier. The robot appears to purely move in a 2D vertical plane at a very low rate. While the authors mention that end effector velocity control is used, the robot appears to have just 3 dimensions of control - a delta position in 2 dimensions, and a grasping signal.

The quality of the final policies is also pretty poor (this is perhaps due to the frequency of control being low - the policy is very choppy in execution and seems to fail often).

(Clarity)

The clarity of the paper is sufficient. A few parts of the method could use some more detail. For example, the MPC-CEM subroutine could use some more explanation.

(Significance)

While the basic motivation is clear - to ease the burden of humans while allowing the system to learn as much as it can on its own from videos of human demonstrations and a modest amount of human labels - there are significant concerns about why the proposed methodology should be used in practice over alternatives.

Only a subset of translated CycleGAN translated images are used - the instruction images. Consequently there is no need for the CycleGAN to generate accurate translations - they only need to be accurate at the task segment boundaries. 

Furthermore, the ease of providing supervision for the CycleGAN is questionable when compared to an alternative such as kinesthetic teaching. The authors mention that only a modest number of human videos are used for traning. In that case, it seems as though kinesthetic teaching could be used to just collect a variety of robot images at stage boundaries. The CycleGan supervision already requires placing items into the robot's hand - it is a small step from there, to moving the robot to the appropriate locations in the environment. It seems to me that not much technical human ability would be required to capture a modest number of frames of the robot in each of the stages, compared to ensuring that diverse and varied robot data is captured for training the full CycleGAN. That being said, the generated robot translations seem to be of surprisingly high quality. ",Agreement accepted,"The paper is well written and addresses an interesting and relevant problem.

The proposed method shows originality in combining state-of-the-art results in a novel way, even if the ideas (e.g. using videos of human demonstrations) are not completely new. 

Experimental research questions are specified in a nice way, and proper procedure is followed, including ablation experiments for studying effects of particular choices.

Details of the models (network structure etc.) are not specified. For this reason, it seems that it is not possible to reproduce the method and experiments. The information would be valuable for other researchers and could be provided in a supplementary document.

The practical applicability of the proposed method remains somewhat unclear, and the authors do not discuss the applicability beyond the two experimental tasks. Especially the need for constructing task specific CycleGANs seems to be possibly a major limitation.

Minor comments:
According to V.B the action space is joint velocities of the arm. How is grasping (gripper opening/closing) then handled?

How many stages were there in the coffee making demonstrations? 900 (images) / 30 (demos) = 30 ? Or three stages as shown in Fig. 4? Or why are there 900 images in 30 demos if there are only three stages and one goal frame per stage per demo?

It would be good to specify the wall-clock time required for the data collection of the unsupervised pretraining, to put in context the pretraining effort vs demo and online learning effort. If I understood correctly, the pretraining images were also task-relevant and not general (e.g. with different/wrong objects etc.)?

",,"This work proposes a new framework for robots to learn from human demonstrations. It consists of several stages: A Cycle-GAN is trained to convert human demonstrations to robot demonstrations frame by frame. From these robot demonstrations, key frames (stages) are selected to be used as rewards. A probabilistic latent variable model is learned which encodes the hidden state transition and the observation model. For each stage, a classifier is trained to give high probability to true encoded frames of the robot demonstration, and give low probability to other frames. Model-predictive control (MPC) is used for controlling the robot. Here, classifiers serve as reward functions and the latent graphical model encodes state transitions. Human intervention is used for resetting the algorithm. This intervention is kept minimum by using the planning in reverse to go back to the previous stage if the algorithm is failed to get positive feedback from the human.

The work combines several ideas and implements a way for robots to learn from human demonstrations. Although the each stage of the algorithm is not new, the overall framework can be considered as novel. It is nice that the feasibility of the algorithm are discussed for each stage of the algorithm. Related works are up to date and the differences are discussed. Methods that are compared with are fairly reasonable.

One major concern is related to learning the visual mapping between human and robot movements. Although no paired samples are used in training translation from human to robot movement visualizations, separate datasets are used for each different task. Furthermore, the dataset on robot's side includes task-related movements of the robot. These two are significant assumptions: For each different task, the robot is required to make different motions and these motions should be composed of parts related to the task that will be learned at the end. If the task is a complex task that includes cup grasping, then the dataset should include action segments of cup grasping; and if the task is a complex task that includes other manipulation actions, then the corresponding dataset should include those action segments. First of all, in order to collect such a dataset, this approach assumes that the robot already knows to exhibit the required segments. Second, somebody should pick up the required segments for a particular task from the available ones and combine them into a dataset in order to learn the corresponding task. These are already huge assumptions. What happens when the robot is required to learn new tasks? These are all big assumptions that were not discussed or addressed, and heavily limits learning without human intervention claim and scalability.The authors might address this issue by combining both datasets and learning a common network for transition in both tasks.

Another concern related to learning the transition is on the perspective and similarity of the demonstrations. While they have different forms, human and robot body and arm configurations are very similar during movements. How does the system behave if the morphology or perspective is different is an open question. So it is important to show that CycleGAN and the state representation learning have wide capabilities to translate the frames and extract relevant information from them even in different conditions. The authors might address this issue using human and robot demonstrations that are at least captured from different perspectives.

Related to above questions, the authors might provide the exact instructions to the human who generates ""random movements"" for training the system.

Since the motivation of the paper was about using imitation learning and reducing the human interaction on a learning system, are the interaction numbers (number of key presses) reasonable compared to a system where an instructor gives a couple of demonstrations directly and leave the robot to learn the behavior using combination of LfD and RL.

Although the general pipeline is sound and interesting, I question whether the experimentation setup is sufficient to prove the usefulness of the idea. The paper emphasizes its low use of human demonstrations. Will this be scalable to other tasks as well? Two tasks in the paper are quite the similar. It would have been better to include one very easy task (maybe consists of 2 stages), and one very hard task (consists of more than 7 stages). I would be interested in the required human demonstration time and the robot interaction time when the task difficulty increases. If this time increases dramatically, then instead of using this method, training from raw videos might be a better alternative. So I think this is a very nice demonstration, but I am not sure whether this is scalable. There is no evidence in the paper regarding this issue. Other than this issue, the paper is well written.

Algorithm 1 can be clarified. As far as I understand, in line 13, there exists human intervention but this is not specified. If the algorithm concludes that it's in the goal state but gets a negative feedback from the human (after line 13 and 14), `attempts` is set to 0. Why is it set to 0? After line 13, does the method reverses back to C_{stage-1}? On line 18, is it possible for model to fail to go back to previous stage?

From a developmental perspective, humans and other animals already have their own behavior repertoire that was developed before and that enables learning from imitation. Some of their actions are not fully developed to directly imitate the complex actions, so parents step-in and scaffold learning. See parental-scaffolding, motionese and related robotics literature for this. Furthermore, imitating humans/infants probably not only imagine other/self movements but also extract task-related goals and try to achieve those. See goal-emulation approaches in robotic studies.

- Table captions must be above tables.
- Table 2 is misleading. For stage 2 and stage 3 in coffee making, the best performing method is behavioral cloning.
- Authors might consider using  Temporal Cycle-Consistency Learning [1] for automatically creating multiple stages.
- Please clarify why 'pressing the button' did not work.
- Please clarify how exactly behavioral cloning is implemented and why it sometimes successfully achieves and sometimes fails in the same task. What changes in between different executions, especially in the second experiment?


[1] Temporal Cycle-Consistency Learning. Dwibedi et al. https://arxiv.org/abs/1904.07846.


",,,,Agreement accepted,07/14 15:00,07/14 17:00,,,,,24.0,V0Hm9sQ5Q9A
"In warehousing and manufacturing environments, manipulation platforms are frequently deployed at conveyor belts to perform pick and place tasks. Because objects on the conveyor belts are moving, robots have limited time to pick them up. This brings the requirement for fast and reliable motion planners that could provide provable real-time planning guarantees, which the existing algorithms do not provide. Besides the planning efficiency, the success of manipulation tasks relies heavily on the accuracy of the perception system which often is noisy, especially if the target objects are perceived from a distance. For fast moving conveyor belts, the robot cannot wait for a perfect estimate before it starts execution. In order to be able to reach the object in time it must start moving early on (relying on the initial noisy estimates) and adjust its motion on-the-fly in response to the pose updates from perception. We propose an approach that meets these requirements by providing provable constant-time planning and replanning guarantees. We present it, give its analytical properties and show experimental analysis in simulation and on a real robot.",RSS2020_Author_Agreement (1).pdf,Fahad Islam (Carnegie Mellon University)*; Oren Salzman (Technion); Aditya Agarwal (CMU); Likhachev Maxim (Carnegie Mellon University),Fahad Islam (Carnegie Mellon University)*; Oren Salzman (Technion); Aditya Agarwal (CMU); Likhachev Maxim (Carnegie Mellon University),RSS2020_Author_Agreement (1).pdf (44337 bytes); fahad_rss.pdf (4148295 bytes),fahad_rss.pdf,1329.0,25.0,,1.0,Provably Constant-time Planning and Re-planning for Real-time Grasping Objects off a Conveyor Belt,,0.6414810288798679,"This paper deals with reactive motion planning for a manipulator robot that is required to grasp objects on a conveyor belt. The system is composed of
  - a conveyor belt moving at constant speed,
  - a 7 degree-of-freedom manipulator robot
  - an object put on the conveyor belt, and
  - an RGBD sensor that detects and localizes the object on the conveyor belt.

Upon detection of an object, the system plans a motion for the robot in order to grasp the object and starts executing the motion. During execution, the sensor provides a more accurate perception of the object position. The robot then needs to replan a new path starting downstream along the original path and going to the new goal state.

In order to be able to plan motions in bounded time, the authors propose to precompute a data-structure containing a lot of paths, and to search this data-structure at execution-time.

The main contributions of the paper are
  1. the construction of the above mentioned data-structure,
  2. a proof of bounded search time once the data-structure is computed.
Finally, the approach is validated in simulation and on a real PR2 robot.

The main idea of the paper can be summarized as follows. Given a number $n_{goal}$ of discretized goal states, and $n$ a number of possible starting states, precomputing a data-structure that provides for each pair (initial state, goal state) a path requires to store in memory $n.n_{goal}$ paths. The paper proposes to compute a set of paths that share the same starting part for many goal states, thus reducing the value of $n$.

The idea is interesting and seems to be efficient, according to the experimental results and according to comparisons with other approaches. The implementation on a real robot makes the work clearly more valuable.

However, the paper suffers a few shortcomings that are described below.

1. The authors do not provide any expression of the size of the data structure built with their method with respect to the number of goal states and number of discretized times along the motions. They provide such a expression for the brute force method: $O(n_{goal}^{l})$. They only give the value of 20MB in the evaluation section. Note moreover that the simple fact to store a set of paths in a roadmap (graph) makes the complexity decrease from $n^2$ where $n$ is the number of states, to $n.k$ where $k$ is the average number of neighbors of the nodes. For big roadmaps, $k$ is much smaller than $n$.
2. The proof provided in IV.E is short and simple. Basically searching a path in a precomputed roadmap is always bounded in time and therefore the title is somewhat misleading.
3. The continuity of the velocity is not guaranteed. This might be a problem for real industrial applications where the conveyor belt is much faster.

Below are few minor comments.

The motion primitives used are not clearly defined in the paper. Why not using linear interpolations ?
Algorithm 1: there is a confusion between $G^{cov}$, $G^{uncov}$, $G^{'cov}$, and $G^{'uncov}$. Line 5 for instance, $G^{cov}$ should be replaced by $G^{uncov}$.

",Agreement accepted,"The paper addresses an interesting problem. However, the discussion of previous work is not complete and fair, as I'm pretty sure one can modify existing work on replanning using PRM/RRT to achieve the objectives stated in this paper, see also my point 1 below.

1) The main concern I have with this paper is that there is no discussion on the completeness of the algorithm. The proof of Lemma 1 (completeness) is omitted ""due to lack of space""!!! Yet, it seems pretty clear that the algorithm is not complete. For example, in algorithm 4, if the iteration of line 20 (t <- t -\delta_t) is performed until t < t_curr, then the algorithm fails. 

This point is critical since there's no use having a constant-time planning algorithm if one has no guarantee of success rate. For example: another algorithm could return ""Nil"" all the time (thereby provably constant time) for a success rate of 0%. So it's always a matter of trade-off between planning time and success rate.

Here, one would expect, at least, a proof that the proposed algorithm is complete given some bounds on t_curr, T_bound.

2) Since the task at hand is time-critical, the motions are expected to be very fast, saturating the velocity and acceleration bounds of the robot to minimize execution time. However, it seems that this paper does not take into account such bounds. For example, the state space considered does not include the current velocity. This is particularly important during ""latching"", as switching between different trajectories may easily violate velocity/acceleration bounds.

3) In general, I found that the paper was not very well written. The notations are not consistent. For example, there are \pi, \Pi, \Pi(s,t,g), \Pi_s, i->j... coexisting, which is very confusing. Key notions (e.g. how ""latching"" is done, proof of completeness,...) are skipped.",Agreement accepted,"Detailed comments and questions for clarification are provided below. The work may require a significant revision of the manuscript to address these.

- The introduction focuses exclusively on the very problem presented in the paper and does not include a broader motivation (and as thus is fairly long for the problem considered). Related work in search-based planning (omitting a body of work on “time-configuration space planning” which is relevant to the formulation) follows in a separate Section.
- Some sections are verbose and repetitive (particularly when not deviating from state of the art which could be referenced to reduce verbosity without losing the self-containedness of the paper, e.g., w.r.t. perception), while other key sections that could highlight the contributions of the work more clearly are omitted for lack of space.
- Given the simplicity of the conveyor belt scenario in terms of collision avoidance (with the correct starting configuration and no external obstacles as shown, point-to-point planning without collision avoidance may readily succeed), in addition to replanning, real-time tracking and local adaptation methods [even a simple IK servoing] could solve the same task – it would be good to mention some of the relevant literature or this possible approach (local adaptation) in related work.
- The perception is simplistic and does not appear to make use of prior estimates/information (tracking). This is fine for the setup at hand as the main contribution is in the planning method, however, this allows to reduce the verbosity of the corresponding section.
- Related work: Recent graph-based planners that use preprocessing, that while being kinematic planners, have shown to be highly efficient. E.g., [1] using FPGAs and methods to compress the roadmap and use live sensing, and [2] using CPU and workspace-to-configuration-space mapping for collision information in a Dynamic Roadmap and live sensing. Both approaches show replanning in *static* scenes in 1-10ms. Since the time dimension in the presented work is effectively discarded (by forward predicting and considering a static scene for every time instant), both approaches would be applicable, are related work (as also search-based and with a provably upper bound on search time) and comparable in times. In fact, both methods could be explicitly extended with the time-dimension (speculatively:) without much penalty to their scalability – and then enhanced using your proposed constant-time heuristic for the search.

Notes:
- A2 appears to be a strong assumption - in a time-configuration space setting, this would be accounted for.
- “prohibitive amount of memory” => can you provide an intuition for how much, e.g., “for a 6-DoF system, this would be XXX vertices/edges and xx GB memory” ?
- The straw man approach: path retrieval with O(1) look-up assumes that no subpaths can be used and full paths are stored. It also does not make use of the well-established (and mentioned in Related Work) concept of roadmaps, or sub-samples, or search on graphs. Given that the work is closer related to these (search methods on graphs / roadmaps), it would be worth basing the work closer to/on top of roadmaps or graphs than relating it to a straw man approach.
- The dimension or setup of the goals “g” is only given much later in the paper (first mentioned as a full 6d pose for the gripper, then the goal is discretized as a 3d x,y,yaw position). It would be good to have an intuition early on to what it could be when “g” and the set ”G” are introduced.
- How is the initial sampling done? Uniform? Random? How many samples? How do you achieve coverage? E.g. Fig. 3 and 4 and the manuscript much later on suggest that the sampling is done in a directed/discretized fashion (e.g. a grid)
- Jacobian pseudo-inverse with the end-effector to minimise distance (for the dynamic primitives) is a simple optimisation scheme w/o collision avoidance. How does your method include collision avoidance?
- Lemma 1: Completeness is w.r.t. the graph \mathcal{G}. Particularly since the goal state set is finely discretized, there is a particular set of literature for completeness w.r.t. discretely sampled spaces that would be suitable to be cited or put in context here.
- Complexity: The time complexity depends on the heuristic [which is constant time] – it would be good to mention this.
- V.A.1.: The preprocessing is fairly standard, yet, the initial estimate seems simplistic: This is unlikely to handle multiple objects on the conveyor belt (as the mean would be between several different clusters). A better approach is to use Euclidean clustering and use the mean of each cluster rather than the mean of the remaining points.
- V.B.: 3.5h preprocessing on what system, with what configuration / implementation? How many paths are stored in the 20MB? How many states / edges?
- Table I: % out of how many samples? 
- Comparison: RRT is weak (uni-directional, heavily depends on the setting for epsilon and the goal biasing) – and a single-query planner. Why not compare with multi-query planners that much closer resemble your method (e.g. vanilla/traditional PRM) or a bidirectional, single-query sampling-based planner (RRT-Connect)?  There are time-configuration space variants, e.g. [3]. Particularly as you already compute a goal configuration from IK for goal biasing, you can as well use them for a bidirectional search. My (speculative) presumption is that RRT-Connect with that goal pose *including* the time taken for IK (~1-3ms for IK with state of the art optimisation-based methods) is likely to rival your proposed method (probabilistically – without time upper bound guarantees). Particularly, since your graph appears to only contain collision-free samples and does not do online collision checks during QUERY (please correct me if I missed this) – even RRT-Connect with online collision checking + IK should be able to rival your method in planning time (~50ms), and likely outperform in success rate. Recent works in literature for instance have shown planning on 7-10 DoF systems on the order magnitude of 10-20ms including collision checking. 
- Recent optimisation-based planning with sparse or continuous-time representations (GPMP2, TrajOpt) have shown fast solutions (ms scale) for comparable systems in more complex environments. It would be good to mention or discuss them in related work.
- With a cut-off time of 0.2s, there are number of time-configuration space planning methods (albeit not guaranteed to be constant or bounded time like yours!) that could also solve this problem in the areas of (a) sampling-based [bidirectional time-configuration space planners], (b) optimization-based [discrete as well as continuous trajectory optimization methods with dynamic obstacles], (c) search-based [like this work]. 
- Definition of “Pickup success”: I am trying to wrap my head around defining “pickup success” as “exactly one replanning after passing a mark” (where the 1m reference frame is unclear, e.g., where on Fig. 1 is that mark?). Would that mark a continuous replanning/adapting with a “super-fast planner” (or local adaptation method) a failure? E.g. continuous servoing.
- There is no discussion of limitations or relation to related work.

Readability: I personally feel it’s unnecessary to replace “robot” and “perception” with calligraphic R/P, respectively. It hurts the readability. At other places descriptions are verbose – this is in contrast to multiple places where omissions are made due to lack of space (e.g. Proof for completeness – a specification of what kind of completeness and a reference to a work doing something similar would be nice, particularly if the proof itself is omitted).

Video attachment: It would be nice to include a sequence illustrating the system in continuous operation (picking and placing multiple randomly placed objects without stopping the video to show when replanning happens).

Language/Minor:
- [22] is broken [Ira Pohl: First results on the effect of error in heuristic search ]
- IV.A. double full stop at the end of the second sentence
- IV.B. full stop missing before final sentence
- IV.C. “heuristic” (typo)
- IV.D. full stop missing after T_bound; final sentence superfluous full stop after Fig. 4
- p.5 second last paragraph: line numbers in algorithm are identical (11-11).
- p.6 second line, full stop after e missing (i.e.)
- p.6 paragraph two “can be used as” (missing word)
- Alg. 4, l. 4: superfluous equal sign
- IV. D. end, “see Alg. 4” (Alg. Missing)
- IV. E.: “coarse” instead of “course”
- V.B.: full stop missing after final sentence before (1).
- V.B.2.: Strike image – it’s a point cloud so no clarification needed in the brackets.

References:
[1] S. Murray, W. Floyd-Jones, Y. Qi, D. Sorin, and G. Konidaris, “Robot motion planning on a chip,”, R:SS, 2016. ← constant time replanning in complex environments (using a FPGA)
[2] Y. Yang, W. Merkt, V. Ivan, Z. Li, and S. Vijayakumar, “HDRM: A Resolution Complete Dynamic Roadmap for Real-Time Motion Planning in Complex Environments,”, IEEE RA-L, 2018. ← proves the resolution completeness for discretely sampled roadmaps/graphs w.r.t. workspace resolution. This is closely related to the discretely sampled goal set G in this paper
[3] Y. Yang, W. Merkt, V. Ivan, and S. Vijayakumar, “Planning in Time-Configuration Space for Efficient Pick-and-Place in Non-Static Environments with Temporal Constraints,” IEEE Humanoids, 2018.",,,,Agreement accepted,07/14 15:00,07/14 17:00,,https://youtu.be/iLVPBWxa5b8,,,25.0,FH4hLJSHii0
"This paper addresses the problem of visual-inertial self-calibration while focusing on the necessity of online IMU intrinsic calibration. To this end, we perform observability analysis for visual-inertial navigation systems (VINS) with four different inertial  model variants containing intrinsic parameters that encompass one commonly used IMU model for low-cost inertial sensors. The analysis theoretically confirms what is intuitively believed in the literature, that is, the IMU intrinsics are observable given fully-excited 6-axis motion. Moreover, we, for the first time, identify 6 primitive degenerate motions for IMU intrinsic calibration. Each degenerate motion profile will cause a set of intrinsic parameters to be unobservable and any combinations of these degenerate motions are still degenerate. This result holds for all four inertial model variants and has significant implications on the necessity to perform online IMU intrinsic calibration in many robotic applications. Extensive  simulations and real-world experiments are performed to validate both our observability analysis and degenerate motion analysis.",RSS2020_Author_Agreement.pdf,"[Yulin Yang](http://udel.edu/~yuyang/), [Patrick Geneva](http://udel.edu/~pgeneva/), [Xingxing Zuo](http://), [Guoquan Huang](http://udel.edu/~ghuang/)",Yulin Yang (University of Delaware)*; Patrick Geneva (University of Delaware); Xingxing Zuo (Zhejiang University); Guoquan  Huang (University of Delaware),RSS2020_Author_Agreement.pdf (550171 bytes); 2020_RSS_intrinsic.pdf (1352440 bytes),2020_RSS_intrinsic.pdf,1283.0,26.0,,1.0,Online IMU Intrinsic Calibration:  Is It Necessary?,,0.6705739958568909,"This manuscript proposes four IMU intrinsic models and incorporates them into a Visual-Inertial system for online estimation. Then the observability analysis of the intrinsic models are given and the corresponding degenerate motions are shown. Extensive experiments are conducted, including simulation and real world environment, and results show the correctness of their theory and analysis. This is an interesting research work that addresses an important problem in the area of X-inertial fusion.

The reviewer have a number of concerns/comments regarding this manuscript:

1. In section VII.B, the authors treat the motion of MAV in EuRoc dataste as 1-axis rotation, which is not convincing. Although the pitch and roll angle are nearly zero when MAV moves slowly, it's not the case for Euroc dataset. Thus the performance on Euroc dataset should be further investigated.

2. The major contribution of this paper is the observability analysis and degenerate w.r.t the IMU intrinsic parameters. In section V, several motions with constant axis component are analysed. Although those situations are common, is it possible that other motions will cause some unobservable direction, like some special combination of w1 and w2? Or is it possible to do reverse analysis, like given d_a11 is unobservable, then study the caused motions?

3. There is a typo in the third term of equation (25).

4. Some figures are quite blurred, like figure 3, 4, 5. It will be better to make such small sub-figure clear in order to zoom-in. ",,"This paper studies the problem of online IMU intrinsic calibration. On the theoretical side, different IMU intrinsics models are considered, and observability analysis is provided. The unobservable directions turn out to be the constant acceleration/angular velocity in each axis of the accelerometer and the gyroscope. These findings are validated in simulation. From a practical perspective, the accuracy of VIO with and without IMU intrinsics calibration is tested with different motion profiles. The results are consistent with the theoretical analysis. Finally, the authors make the suggestion that IMU online calibration is dangerous for many robotic platforms with limited motion DoFs and should be performed with caution.

The paper studies an important problem in VIO and is very relevant to practical applications. The paper is well written and easy to follow. The theoretical results are presented (within the page limit of the manuscript), and the experimental results are extensive and convincing.  The only suggestion is about the presentation of the theoretical results. While it is understandable that not all theoretical derivations can be included due to the page limit, the presentation of observability analysis should be made clearer. For example, which state is each block in Equation (36) corresponding to? Also in Section V, how are (37) - (41) obtained from the observability matrix? What are the rows and columns in these matrices corresponding to? Some explanation about the intuition/physical meaning of these equations will greatly help the reader understand the theoretical results. At the current state, it is a bit overwhelming, especially for someone who is not familiar with the topic. In addition, providing these details in a supplementary document should also be nice (currently there are only the state transition matrix and Jacobian).

Some detailed/minor comments are:
1. Eq. 37 - 41: N here is already used in (1) and (2), but for a different meaning.
2. In Section V, the authors provide 6 unobservable directions. How are they determined? Is there any other degenerate motion?
3. Section VI-C: what is the reason of choosing the specific axis for constant rotation and acceleration? Are the results holding for the other degenerate motion patterns?
4. Section VII-B: what is the reason of choosing the Vicon room sequences from EuRoC but not the Machine Hall ones?

",,"The observability analysis for the model in [21] is valuable, since the identification of degenerate motions is important to avoid filter inconsistency and other problems. The authors describe four different IMU models in Section II.B, but the reviewer is uncertain why models 'imu3' and 'imu4' are valuable? The use of matrices containing nine parameters does not seem to be physically motivated? The model in [21] already captures known axis scale factor and axis misalignment effects (that are physical) - adding more degrees of freedom would seem only to make the problem much harder / more prone to degeneracies. Can the authors comment on this? The observability analysis is also based on linearization - a fully nonlinear analysis might provide further insight.

It it not surprising that the results in Table III show that an IMU with fixed, 'bad' calibration leads to estimator inconsistency (as seen in the NEES scores). What is surprising, to the reviewer, is that inconsistency does not appear to manifest in the simulation results shown in Figures 4 and 5; although the three-sigma bounds do not decrease much for several of the parameters, all of the estimates appear to stay within their respective bounds. Typically, for unobservable states/parameters, the values wander outside of the bounds - perhaps the motion was not sufficiently 'degenerate'?

Section IV.D can be removed - it is already very clear from the models presented in Section II.B (and is otherwise known) that an over-parameterization with an additional rotation makes the system unobservable/unidentifiable (because the kinematic chain then has three redundant DOFs that are arbitrary). It is not necessary to provide simulation results that show this - the space would likely be better dedicated to other aspects of the problem 

Minor notes: there are a few grammatical errors in the paper that should be fixed, e.g., in the first sentence of the Introduction, ""visual-inertial navigation system (VINS) [10] has gained great popularity"" is not correct. Also, in Section II.C, the authors use the phrase ""denotes JPL quaternion"" - I believe they mean that the JPL quaternion convention is uses but this should be clarified (and the sentence is not grammatically correct).",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,,,,26.0,k2JPxXnE78Q
"We present a hybrid rigid-soft arm and manipulator for performing tasks requiring dexterity and reach in cluttered environments. Our system combines the benefit of the dexterity of a variable length soft manipulator and the rigid support capability of a hard arm. The hard arm positions the extendable soft manipulator close to the target, and the soft arm manipulator navigates the last few centimeters to reach and grab the target. A novel magnetic sensor and reinforcement learning based control is developed for end effector position control of the robot. A compliant gripper with an IR reflectance sensing system is designed, and a k-nearest neighbor classifier is used to detect target engagement. The system is evaluated in several challenging berry picking scenarios.",RSS2020_Author_Agreement_signed.pdf,Naveen Kumar Uppalapati (University of Illinois at Urbana Champaign)*; Benjamin Walt ( University of Illinois at Urbana Champaign); Aaron Havens (University of Illinois Urbana Champaign); Armeen Mahdian ( University of Illinois at Urbana Champaign); Girish Chowdhary (University of Illinois at Urbana Champaign); Girish Krishnan (University of Illinois at Urbana Champaign),Naveen Kumar Uppalapati (University of Illinois at Urbana Champaign)*; Benjamin Walt ( University of Illinois at Urbana Champaign); Aaron Havens (University of Illinois Urbana Champaign); Armeen Mahdian ( University of Illinois at Urbana Champaign); Girish Chowdhary (University of Illinois at Urbana Champaign); Girish Krishnan (University of Illinois at Urbana Champaign),rss_final.pdf (10285554 bytes); RSS2020_Author_Agreement_signed.pdf (125961 bytes),rss_final.pdf,1281.0,27.0,,1.0,A Berry Picking Robot With A Hybrid Soft-Rigid Arm: Design and Task Space Control,,0.785792509665129,"## 1. General Feel and Major Comments:
The authors present a robot arm with a soft gripper, continuum arm, and rigid links to support and reposition the continuum arm's base. The whole setup is placed upon two mobile rover bases.

Overall, the paper is very clear with one exception (more later).

The introduction could use more citations for some of your claims, especially the discussion about the human labor shortage. Such claims should be backed by data.

Your gripper system should be useful in other applications, and on other objects. I recommend showing the gripper grasping and manipulating some other objects. This wouldn't take long, and I'm not suggesting a rigorous study on a complete benchmark object set - just a few samples to make it clear that the approach does indeed generalize to non-ellipsoid objects (it should... just show it!).

What simulator did you use? This is critical to understanding your work, and it was mostly omitted. If it was presented in another publication, cite that and present a concise summary of the procedure and modeling assumptions. If it's buried in this manuscript, make it clearer and explicitly stated. For example, on page 5 you say ""Rather than performing some system identification for specific arm settings and loading, we use a Kirchhoff rod model of the soft arm [2] to train a control policy directly from experience. Virtually any arm configuration and simulated loading can be trained using an existing reinforcement learning (RL) strategy called Deep Deterministic Policy Gradient (DDPG) introduced by Lillicrap et al. [12]."" References [2 of submission 1281]  (a classic textbook on elasticity) and [12 of submission 1281]  (one of the original, possibly the original, paper on DDPG's)are both very general, and it is unclear what you specifically did.

## 2. Additional comments
""Furthermore, the control method is based on reinforcement learning, and as such provides a strong validation point for the use of such learning based, model-free control methods for challenging reach problems in robotics."" - Suggest deleting. RL and related methods are well-developed. Just a quick search in my Zotero library pulled up several related research on RL in (soft) robotics [1-4]. (In many ways, the distinction between soft and hard is arbitrary, and I expect the fields to become more integrated, as submission 1281 is suggesting. Hence, I consider both hard/soft robotics as relevant and included in the below references.)

The notation is confusing, for example the use of B for bending and R^2 for rotation. These are both common terms in fields adjacent to that of submission 1281 (B = magnetic field, R^2 coefficient of determination).

P3 ""the bush"" what bush?

P4 be clear when you mean accuracy vs. precision


[1]M. Zhang et al., “Deep reinforcement learning for tensegrity robot locomotion,” in 2017 IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 634–641, doi: 10.1109/ICRA.2017.7989079.
[2]V. Mnih et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, p. 529, Feb. 2015, doi: 10.1038/nature14236.
[3]F. Agostinelli, S. McAleer, A. Shmakov, and P. Baldi, “Solving the Rubik’s cube with deep reinforcement learning and search,” Nat Mach Intell, vol. 1, no. 8, pp. 356–363, Aug. 2019, doi: 10.1038/s42256-019-0070-z.
[4]H. Zhang, R. Cao, S. Zilberstein, F. Wu, and X. Chen, “Toward Effective Soft Robot Control via Reinforcement Learning,” in Intelligent Robotics and Applications, 2017, pp. 173–184, doi: 10.1007/978-3-319-65289-4_17.


Figures, overall: 
- They have the necessary content, suggest polishing. High-level: be clear what you want readers to learn from each figure. Prioritize that, and remove unnecessary clutter (without removing inconvenient but true data. Keep all data, but see specific comments below on how to improve your presentation)
Figure 1: 
- caption ""mounted on a TerraSentia *mobile rover*"" (also clear this up in the text on page 1)
- Consider cropping ~10% tighter to make it clearer what is going on in b and c. This figure is currently a pretty good summary of your paper, thanks for including it.
Figure 2 (and somewhat 4):
- Way too messy. Additionally, some of this is redundant with Figure 4 and Figure 1. Why was this figure even included? I'd reduce it to just the central image, with labels, and shrunk down. Then, consider placing it side-by-side with a simplified Fig. 4.
Figure 3 is tough to read. It will strain your readers' eyes. Consider changing the colors of the background, or at least putting backdrops on the text annotation and adjusting the text colors.
Figure 9: Odd (and somewhat confusing) choice of ordinate (y-axis) label.
Figure 10: unclear subfigure labels. Consider bolding them, or adding a backdrop. Jarring to look at the different subfigures, some whitespace (0.05"" maybe) between the images would be helpful.

Scale bars are useful for the less isometric views (Fig. 5), or at least a note about lengthscale in the caption (Fig. 7. 10).

References seem to be biased toward the work of Girish Krishnan et al. (Refs 16, 18, 19, 20, 21 = 5 out of 23 total). Furthermore, when removing general (soft) robotics references such as Laschi [1], Antman [2], etc.,  the authors seem to only be citing Krishnan from within the agricultural robotics community, despite the existance of a wide range of other authors in this area. For instance, the work of Lie Tang (ISU), Yin Bao (Auburn), Jian Jin (Purdue), Y. Shibano (Okayama Japan), Giulio Reina (University of Lecce)... etc. are all relevant yet seem to be willfully omitted, despite the fact that presumably the authors of 1281 read other authors besides Krishnan.


## 3. Comments on Multimedia (Videos, etc.)
- In final submission, title slide could be more informative (author names, school, etc.) - I understand why you did not include such information here, thanks for adhering to the double-blind policy.
- The text annotations are difficult to read (put opaque backdrop for instance black or white, or change color), and don't make complete sense (""First Link"" - ""Rotate first link"")
- Shaky camera. Please use a tripod. At a minimum, use video-stabilization software as post-processing (this would be unacceptable if the video was used in experiments and not reported as part of the algorithm... but clearly these videos were just for presentation to humans, and not core to the research results).
- Are these videos using a human to teleoperate? They don't look as smooth as I expect the DDPG to generate. Especially the interior video",Agreement accepted,"This manuscript presented a mobile berry picking robot with hybrid soft-rigid arm and developed a reinforcement learning based controller for the hybrid arm. 
Originality:
The presented work focuses on system development of a mobile manipulator for dexterous manipulation. The capability of this robot system is demonstrated in the berry picking tasks. Though the VaLeNS arm has been reported, the hybrid robotic arm presented in this paper is interesting.

Quality:
This manuscript well presented a piece of solid work on development of a hybrid robotic arm and an integrated mobile robotic manipulator. Both the hardware and control system are well reported. Further, the performance of the robot has been evaluated in field tests. 
 
Clarity:
This manuscript is well structured and written. 

Significance:
The scientific contribution is convincing while the advantages of such a system comparing to other existing large machines could be examined. 

Some detailed comments here:
1. The modular gripper need to be changed to adapt to various type of berries. More details could be added to explain the interface between the soft arm and the gripper both mechanically and electronically.  
2. As illustrated in the Fig. 2, the compressor for the soft arm actuation is listed but key specs of the actuation system should be added. In the current form, it is uncertain if the compressor is able to power the soft arm.
3. Fig. 8: The three sensor should be better named to avoid confusion.
4. The advantage of using the gripper in comparison with other systems using different end-effectors should be clearly stated. 
5. If possible, the authors are suggested to compare the performance of controller using kinematics of soft arm and the proposed controller based on reinforced learning. ",Agreement accepted,"
## Review Details

* thank you for a system block diagram.  Please make arrows bigger.
* I believe festo developed a soft gripper of the same architecture prior to the fin ray work.  Please cite festo in addition.
* how do you plan to incorporate the full system into an automated platform for berry picking?
* reporting a success rate when the user is in the loop is difficult.  Please report the success rate of the non-human subsystems separately and numerically
* interpreting table I is difficult because you do not explain the meaning of check, check*, check+ or x.  Please put some numbers to the success rates.  what percentage of the time was each trial successful?
* My interpretation is similar to those in the paper, that at 20cm, the sca becomes difficult to control due to lowered system stiffness and divergence in the model used to control vs reality.  Though you listed a strategy to mitigate the problem, doesn't this indicate a fundamental limitation of your approach, in that we should expect your model to change throughout a number of use cases?  Which learning strategies would be able to compensate for this better?


## video
* The video uses a jump-cut, removing critical frames in the middle of the experiment.  Please take a complete video without cutting to assure the reader that what you say is what you demonstrate.
* use a tripod, or at least don't zoom in with a smartphone, it gets shaky.
* use landscape orientation rather than portrait orientation when capturing video
* provide stabilized, close-up shots of critical tasks by using a second camera from a different angle and closer vantage point.  Inset this video into the main video.
* unfortunately due to the shakiness, the far distance and grainy video due to zoom and shake, the video poorly represents the work done by the authors.  It reduces my confidence in the method and the results.


",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,https://youtu.be/0hFTP0UUaIE,,,27.0,NB1l5e7eY9w
"Creating natural and autonomous interactions with social robots requires rich, multi-modal sensory input from the user. Writing interactive robot programs that make use of this input can demand tedious and error-prone tuning of program parameters, such as tuning thresholds on noisy sensory streams for detecting whether the robot's user is engaged or not. This tuning process dealing with low-level streams and parameters makes programming of social robots time-consuming and inaccessible for people who could benefit the most from unique use cases of social robots. To address this challenge, we propose the use of iterative program repair, where programmers create an initial program sketch in our new Social Robot Program Transition Sketch Language (SoRTSketch), a domain-specific language that supports expressing uncertainties related to thresholds in transition functions. The program is then iteratively repaired using Bayesian inference based on corrections of interaction traces that are either provided by the programmer or derived from implicit feedback given by the user during the interaction. Based on experiments with a human simulator and with 10 human users, we demonstrate the ease and effectiveness of this approach in improving social robot programming and program outputs that represent three common human-robot interaction patterns. We also show how our approach helps programs adapt to environment changes over time.",RSS2020_Author_Agreement_signed.pdf,"[Michael Jae-Yoon Chung](https://homes.cs.washington.edu/~mjyc/),  [Maya Cakmak](http://www.mayacakmak.com)",Michael Jae-Yoon Chung (University of Washington)*; Maya Cakmak (University of Washington),RSS2020_Author_Agreement_signed.pdf (107213 bytes); root.pdf (1733403 bytes),root.pdf,97.0,28.0,,1.0,Iterative Repair of Social Robot Programs from Implicit User Feedback via Bayesian Inference,,0.193620342355531,"Enabling end-users to fine-tune behavior is important for democratizing robot programming. It allows users to personalize behavior without robot programming expertise. The motivation for this work is good, and the paper is relevant to RSS. I appreciate that the authors conducted both simulation and real-world experiments with human subjects. Application of the repair scheme appears to consistently improve the overlap score.  

== Areas for Improvement ==
I have concerns about the technical aspects of the paper. Specifically, the Bayesian update Eqn (2) appears incorrect. In the normalizer:
- what is the index j associated with?
- the sum appears to be over the correct data traces; shouldn't the marginalization be over hole variables? Potentially, this is a typographic error and can be easily corrected. However, if the update is inherently incorrect, the subsequent results would be invalid.  

The posterior update doesn't appear to be closed-form. If I understand correctly, the authors perform numerical integration over discretized parameter sets. However, this implies exponential complexity wrt the number of hole variables. This point should be clarified in the paper since it limits the applicability of the approach to a small number of hole variables.

The input trace doesn't appear to be corrected like the output traces; won't this lead to a ""mismatch"" between the input and output traces? If so, this could lead to incorrect inference. 

I appreciate that experiments were performed with human users. A potential improvement is to perform a comparison to a control group (e.g., an alternative baseline method), with a proper statistical tests or Bayesian analysis. 

Finally, the paper requires a thorough proof-read to correct typographic errors, e.g., 
- ""XXX describe how variables are sampled for execution???"" is an unfinished sentence.
- ""corrections that are user by"" -> ""corrections that are used by""
-  \mu_open^iter4 and \mu_open^iter1 appear to be swapped? 
- Table 1 and in-text: ""quite"" -> ""quiet""
- ""tunned for"" -> ""tuned for"" 

Overall, I find the key idea of iterative Bayesian program repair interesting, but the presented work appears preliminary. I hope that the authors can address the technical and presentation issues above. 
",Agreement accepted,"The authors provide a solution to a problem that not only appears in non-expert programming but also when experts intend to program new applications involving social robot behavior. The work is original in the way it attempts to find the unknown variables, sometimes also referred to as magic numbers.
The quality of the paper is promising given the number of experiments and examples that were incorporated within the paper but the paper in the current status needs some more clarification to get a good idea about the impact of the work. 

I had difficulties understanding some parts of the work and its implications properly. Therefore, I believe it will be important to improve the quality and clarity of the paper to ensure that the paper is understandable in all points and its implications are clear to every reader (see details below).
From my point of view, one aspect that could be improved is the motivation and introduction. There is a missing link between the motivation and the approach which gives me the impression the approach is not the right solution to the problem given in the motivation. For example, I have difficulties imagining how a non-expert could choose the right distribution for a hole variable when already to experts the range of values might sometimes be unclear (motivation: how non-experts could find new applications).
I really think the problem stated is very valuable to the community to be solved but following up on the previous point, the contribution could be more complete if it provided a way to deal with completely unknown hole variables. The need to give the probability distribution gives limits to the applicability of the proposed method. On the other hand, the method provides an excellent way for experts to find the unknown variables faster and in a more pleasant way.

Even though the description of the simulation experiment seems detailed enough, I have difficulties to understand how the simulations were done. For example, I did not understand how the sensor data, e.g. head angle, is simulated from intentions. Further, I am unsure about the noisy state traces. Was the noise added to the time of transitions or the length?
For me to evaluate the quality of the user study I would need more information on the users, e.g. did they have previous experience with programming. Further, the lack of statistical tests does leave a doubt on the perceived change in fluency or number of interventions. I can see that the authors might have decided to not apply statistical tests due to the small number of participants and large diversity among them. Still, the reasons for the lack of statistical tests should be stated explicitly. Especially, since the results seem not to support the author's hypothesis completely.

As 3 out of 10 participants did not manage to create a more fluent interaction, I expect a more critical discussion on this limitation. I think it would be more valuable to the community to find that non-experts have difficulties understanding the ""Back"" button as their knowledge on state machines might be limited or how robot sensors work. Therefore, I do not agree with the conclusion that the experiments prove the feasibility of the approach. I think a thorough rework on the discussion and result section would improve the quality of the work immensely without needing to rerun experiments. To make the results even more convincing, I encourage the authors to collect more data about their participants and to rerun the experiments with at least 20 participants to get significant and more reliable results.",Agreement accepted,"===Detailed review===
Overall, the paper is well-written and clear. The paper makes a timely and relevant contribution to the field of robotics and human-robot interaction, where an active field of research is the effective and efficient creation and repair of robot programs, lowering the load on experts and shifting more towards non-experts to provide input and feedback. The contributions are clear and the motivation well explained. Overall, the methodology seems sound and the evaluation is thorough, including simulated and human experiments, as well as including changes in environment and noisy input. Please find my detailed comments below.

===Major===

-	The paper describes three different ‘social robot tasks’ that consist of storytelling, a neck exercise, and open Q&A. Where the three tasks essentially differ, they all require the same type of user feedback (‘go back’, or ‘next’). It would be great if the paper can discuss this approach for more complex situations and how this affects the effectiveness of this approach. For example, what if there is a larger number of potential actions to choose from than three (stop, wait, read) and the 'go back' does not necessarily reflect that instead of action X the alternative is Y, maybe there are many alternatives, and it may even become unfeasible for a human to keep providing feedback until the robot has learned the correct transition, just because it becomes too timely and the user may be reluctant to providing feedback and/or using the technology. Since the paper focuses on social robot actions specifically, I think this is crucial to at least touch upon in a discussion in the paper, maybe if the paper would include a Discussion section to discuss any drawbacks or potential avenues for future work would improve the paper even more. 
-	Similarly, it would be interesting to discuss the limitation of incorrect user feedback, whether on intentional or not. For example, one may turn the head to the left, while the robot said right but not notice that it was the incorrect side (left and right are often mixed by people). In this task, maybe the consequences are negligible, however, in a situation where the error may propagate further through the interaction, it may be worth discussing how to best handle this. Humans who did not fix incorrect transition errors is briefly described in IV.C.4 but not discussed in depth. 
-	Section IV.C 4) Results: the before-repair percentage overlaps as reported in text seem incorrect, I think the value for iter4_open and iter4_neck are accidentally switched, given the numbers shown in Fig. 6 (left). Correct this and maybe (this is minor) the paper could instead report that mean_iter1_neck increased from 0.33 (SD = 0.08) to 0.36 (SD = 0.08), and mean_iter1_open decreased from 0.80 (SD = 0.30) to 0.57 (SD = 0.21), for clarity of reading. Currently, it’s a bit tricky to read in the way they are presented now.
-	One concern I have is that the sample of people used in the experiment (N=10) is small and therefore, it is reasonable not to perform stats, however, the results are reported in a strong manner, talking about strict increases and decreases. I think it is fair to say that this trend is observed, however, maybe tone down the results a bit to align it with the actual evidence that is presented to prevent the results to be overly confidently interpreted by readers. 
-	The conclusion introduces sudden new directions for future work that are not mentioned before, which is not the goal of a conclusion. A conclusion section should summarize what has been discussed in the paper. Maybe it could be called ``conclusion and future work’’ section, but it should talk about avenues for future work in more detail. 

===Minor===

-	In Section III.A. Program Execution, there is an unfinished sentence that needs to be changed: ``XXX describe how variables are samples for execution???’’ Probably, due to the unfinished paragraph the goal of this subsubsection is not clear, maybe it can be included in another section rather than be separate. 
-	In Fig. 4: It could increase clarity to mention ‘dotted line’ in the figure’s caption when talking about `missing transition’ errors.
-	In Section III.B, the paper mentions that speech commands can be used for ‘next’ and ‘go back’ button alternative, however, one must then consider that this is more error prone (e.g. speech recognition errors) in itself as input mechanism. 
-	Very minor (typo): Section III.B, last paragraph: ``To that end the use of interaction repair mechanisms in the state trace need to be converted to state corrections that are user by the repair algorithm’’ – are user/are used. 
-	Very minor: sort the reference numbers, for example [35, 10, 2] – [2, 10, 35]. 
-	In section III.C the paper describes some details about implementation, I would like to ask for clarification if the code will be made available as to promote reuse and continuation of this work also by other researchers?  And if chosen not to release the code, a motivation for this decision would be appreciated.
-	IV.B 1) procedure: ``IterativeBayesRepair (Alg. 2)’’ should be (Alg. 3)
-	Same paragraph IV.B 1) procedure: ``The noisy state trace was computed by adding uniform noisy of [-2,2] to every state changes in the ground truth …’’ should be ‘noisy of [-2,2]’ should be ‘noise’ and ‘every state changes’ should be ‘every state change’
-	Same paragraph IV.B 1) procedure: ``(…) on observing the two first time users’’ – observing two first time users. The paper does not mention them before, so should not say ‘the users’.
-	Paragraph IV.B 2) measures: ``… being sensitive to the length of the interaction of which we control...’’ – ``which we control’’, and also ``we measured the speed of (…) without break in-between iterations’’ – `breaking in-between iterations’, or ‘breaks in between iterations’ or `a break in between iterations’? 
-	IV.B 3) results: ``algorithbms’’ typo.
-	Why not the storytelling task in the evaluation?
-	Why was it set to four repair iterations, was it clear beforehand that this number was enough? 
-	Q&A capitalized in some places not capitalized in others.
-	Section IV.C: ``Over the four iterations, The transition parameters (…)’’ – the should not be capitalized. 
-	Section IV.C.5 ```(...) we conducted an experiment involving one human user who as a participant (…)’’ – who was a participant (typo)
-	Section IV.C.5 ``(…) i.e., the quite room (…) – quiet room (typo) Also in TABLE I. change ‘quite’ to ‘quiet’.
-	Section V. ``the goal of our research is motivating by…’’ -- ``motivated’’ (typo).
-	Section V. ``(..) exploring the interactive system for keep the programmer (…)’’ – ``for keeping the programmer’’ (typo)
 ",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,,,,28.0,LbcMxC_3x1U
"Cables are complex, high dimensional, and dynamic objects. Standard approaches to manipulate them often rely on conservative strategies that involve long series of very slow and incremental deformations, or various mechanical fixtures such as clamps, pins or rings.We are interested in manipulating freely moving cables, in real time, with a pair of robotic grippers, and with no added mechanical constraints. The main contribution of this paper is a perception and control framework that moves in that direction, and uses real-time tactile feedback to accomplish the task of following a dangling cable. The approach relies on a vision-based tactile sensor, GelSight, that estimates the pose of the cable in the grip, and the friction forces during cable sliding.We achieve the behavior by combining two tactile-based controllers: 1) Cable grip controller, where a PD controller combined with a leaky integrator regulates the gripping force to maintain the frictional sliding forces close to a suitable value; and 2) Cable pose controller, where an LQR controller based on a learned linear model of the cable sliding dynamics keeps the cable centered and aligned on the fingertips to prevent the cable from falling from the grip. This behavior is possible by a reactive gripper fitted with GelSight-based high-resolution tactile sensors. The robot can follow one meter of cable in random configurations within 2-3 hand regrasps, adapting to cables of different materials and thicknesses. We demonstrate a robot grasping a headphone cable, sliding the fingers to the jack connector, and inserting it. To the best of our knowledge, this is the first implementation of real-time cable following without the aid of mechanical fixtures.",RSS2020_Author_Agreement_Signed.pdf,"[Yu She](https://sites.google.com/view/yu-she), [Siyuan Dong](https://sites.google.com/site/siyuandong000/), [Shaoxiong Wang](http://wx405557858.github.io/), [Neha Sunil](http://nehasunil.com/), [Alberto Rodriguez](https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU), [Edward Adelson](http://persci.mit.edu/people/adelson)","Siyuan Dong (MIT); Shaoxiong Wang (MIT); Yu She (MIT)*; Neha Sunil (Massachusetts Institute of Technology); Alberto Rodriguez (MIT); Edward Adelson (MIT, USA)",RSS2020_Cable_Final.pdf (3942246 bytes); RSS2020_Author_Agreement_Signed.pdf (207124 bytes),RSS2020_Cable_Final.pdf,1193.0,29.0,,1.0,Cable Manipulation with a Tactile-Reactive Gripper,http://gelsight.csail.mit.edu/cable/,0.17123358873652303,"I liked very much this paper, a solid overall contribution to manipulation of deformable linear objects via important contributions in hardware (gripper with compliant joints equipped with computer-vision tactile sensor), perception (tracking of pose and force in real-time), and control (smooth sliding and cable alignment).

It would be good to have experiments results with cables of larger diameter. It seems that the results of the proposed approach will deteriorate as the diameter approaches the width of the fingertips. I think this is worth to be discussed in the paper.

The insertion experiment is quite interesting. It would be good to report the results compared to an open-loop approach.",,"
The paper is original in the sense that this is a solution for an unsolved application. The task could be relevant because it is related to other cloth manipulation tasks, for instance, edge tracing. However, their approach is not generic enough and it is not motivated enough. Also, the bibliography research seemed very short, considering the large amount of rope manipulation literature that exists.

The paper is clear and well explained, and the video is also very clarifying. However, the overall work doesn't have a lot of significance for the community, unless you need to solve the exact same application and are willing to buy that same sensor. I think the controllers could be applied to other sensors as long as you can estimate the pose of the cable, but the authors have not made any effort to make the contribution a bit more general. 

Authors should also clearly state the limitations of their approach, as it seems they can solve it pretty well, but in the final paragraph they state that better learning of the dynamic model could increase accuracy, but it didn't seem necessary for the shown results. 
Also, the presented controllers perform just slightly better than the baseline solutions that are based on very simple approaches. 

In conclusion, I think this is a nice work that solves very well the task, but the authors have to better convince me of the novelty of their solution and why it could be significant for the community of deformable object manipulation.",Agreement accepted,"The most impressive aspect of this paper is how little precedent there is for such a task. Cable manipulation in general is rarely attempted; to the best of my knowledge, a task of the complexity of the one shown here has not been previously shown. The experimental performance is also remarkable. While none of the individual building blocks introduced here is particularly complex or novel, their combination is, and there is significant novelty in the complete system being able to accomplish such a task.

Further showing how different this work is from previous literature, there is really no baseline in the literature for the authors to compare their results against. They thus do a very thorough comparison against ablated versions of their own system, as well as a naive open loop execution. While open loop execution fails utterly, some of these ablations do hold their own very well against the full system, and have the benefit of being much simpler. Still, the complete system, including the cable pose controller, does perform best, especially in terms of requiring fewest re-grasps and going furthest on one grasp (within error bounds, but still). The authors should be commended for these complete results.

A few clarifications would be helpful. The authors state in the introduction that the gripper has force control capabilities, but that does not seem to be the case. If I understand correctly, there is a spring in series with the motor, which allows conversion between position displacements and output force, but the actual value of the output force is neither measured nor regulated. (The grip controller operates directly on desired size of tactile imprint.) It can be hard to figure out what exactly is the output of the cable pose controller, and also how these two controllers operate together (if I understand correctly, they are orthogonal, regulating completely different outputs). Finally, it would be great to see how well the linear dynamic model fit to cable pose data works. A better characterization of this model could include training error, generalization error, an analysis of how well it works over a wide range of situations (cable poses, distance from the fixed point), etc. Overall however, the paper is clearly written and easy to follow. 

In conclusion, the paper introduces a complete system-and-method approach to a task not previously attempted, and obtains remarkable experimental results. It is also a valuable piece of work towards the introduction of manipulation algorithms that use tactile sensing, as opposed to tactile sensors in search of applications. It would be useful and interesting to the community and conference audience.
",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,https://drive.google.com/file/d/16vfl6B55Ae5_NclOlcy9v3UwIVcr4oS4/view?usp=sharing,,,29.0,KDdKcQ9NRXc
"In this work, we describe an end-to-end system for automatically synthesizing correct-by-construction structure and controls for modular manipulators from high-level task specifications. We define specifications that include both continuous trajectories the end-effector must follow and constraints on the physical space (obstacles and possible locations of the base of the manipulator). In our formulation, trajectories are composed of basic shape primitives (lines, arcs, and circles) and we avoid discretizing the desired trajectory, as other approaches in the literature do. We encode the task as a set of constraints on the manipulator’s kinematics and return the manipulator’s structure and associated control to the user, if a solution is found. By solving for the continuous trajectory, as opposed to a discretized one, we ensure that the original task is satisfied. We demonstrate our approach on three different specifications, and present the resulting physical robots tracing complex trajectories in the presence of obstacles.",RSS2020_Author_Agreement_signed.pdf,Thais Campos de Almeida (Cornell University)*; Samhita Marri (Cornell University); Hadas Kress-Gazit (Cornell),Thais Campos de Almeida (Cornell University)*; Samhita Marri (Cornell University); Hadas Kress-Gazit (Cornell),RSS2020_Author_Agreement_signed.pdf (147457 bytes); ThaisRSS2020.pdf (2243743 bytes),ThaisRSS2020.pdf,65.0,30.0,,1.0,Automated Synthesis of Modular Manipulators’ Structure and Control for Continuous Tasks around Obstacles,,0.38268170830159703,"The authors formulate their planning system using constrained optimization to solve for both the position of the base of their RRR robot and movement of the RR chain that achieves the desired task.  I cannot help but wonder why they did not plan the trajectory of the end point in polar coordinates relative to the base joint, and use the extra degree of freedom to separately position the links of the manipulator in ways that achieve their obstacle avoidance constraints.  I am certain that I am missing something. 
This is an interesting paper complete with the analysis necessary to understand it implementation.  I recommend acceptance for publication.
",Agreement accepted,"The main interest of the proposed method is to avoid discretization of the path which may, as illustrated in the examples given in the paper, lead to feasibility issues. The video provided with the paper nicely shows the possibilities offered by this work. On overall, the paper is well organized and presented.

However, in my opinion, the following concerns must be dealt with.

1. The main issue with the method is its applicability limited to planar paths, obstacles in form of circles and planar 2 and 3-DOF manipulators. These important limitations are not explained, not even mentioned, in the abstract, introduction and conclusion of the paper. The literature review, comparing this work with previous ones, should also account for these limitations by providing fair comparisons with previous methods which can be applied to spatial cases.

2. The main tool allowing the method to avoid discretization is the use of swept volume (SV) calculations. Regarding these calculations, I have the following concerns:

2a. There is not literature review on this topic in the paper which makes it difficult to assess the detailed technical contribution of the paper. Unless I miss something, he SV calculations mainly amounts to simple planar convex hull calculations which tends to show that the technical contribution is limited.

2b. The method is mainly limited to planar 2-DOF manipulators since the SV for 3-DOF manipulators is calculated assuming that only the last two links are moving during the execution of a shape primitive which, essentially, makes the 3-DOF manipulator having 2 DOFs.

2c. The extension of the method to spatial paths and manipulators with 3 or more degrees of freedom may be very challenging. This extension is mentioned in the future works but there is no clue of the difficulties linked to SV calculations in 3D and for n-DOF manipulators with n>3 (notably on the issue of multiple inverse kinematics solutions).

3. The choice of the optimization problem cost function as the sum of link lengths is not motivated. Several other criteria, as found in the literature, could have been used so why this one? Why not a multi-objective optimization problem formulation?

4. What about possible issues with kinematics singularities and what about the timing and synchronization of the manipulator joints along the path?",Agreement accepted,"In this work the authors present a constrained optimization approach to computationally determining a design and trajectory for a multi-link (2 or 3) planar robot in order to enable it to follow a continuous, user specified end effector trajectory on a 2D plane in the presence of obstacles.

The authors demonstrate the efficacy of their method on a fully implemented physical system that includes a user interface for detailing circular obstacles in the robot’s environment, constraints on the robot’s base position, and the continuous trajectory that must be followed by the end effector. The authors show that the method is successfully able to generate a design for 3 trajectories, and then show the robot performing the task in the physical world after fabricating the design produced by the method. Additionally, the authors favorably compare their method to one that does not consider continuous trajectory following but rather discretized trajectories based on a sampling-based motion planning method.

The strengths of the paper include the impressive end-to-end demonstration of the method’s use from task definition to physical execution on a robot constructed according to the designs produced by the method. This step, physical construction of multiple designs, is one that is often missing in design optimization work and is appreciated here. A related strength is that the paper demonstrates a complete system, including the user interface for defining the task and constraints. Consideration of the interface through which users define tasks and constraints is an important aspect of building useful robotic systems that is frequently overlooked. Its inclusion here is appreciated. Additionally, for the most part the method is thoroughly described and the paper is generally well written. Further, the video attachment is valuable and demonstrates the results nicely.

There are some concerns that would strengthen the paper were they addressed.
In no particular order:

1) In some places in the paper, the descriptions of aspects of the method for the 2-link case are thorough, but the description of how that is extended to 3 links is not sufficiently detailed to be clear. For instance, in the last two paragraphs of Section IIIa, the SV simplification for the 3DOF case is done by not allowing the proximal link to move during the execution of a shape primitive. The implications of this simplification need to be described. Does this simplification have a meaningful effect on the method’s ability to find valid solutions? This needs described in more detail and the implications should be discussed. Similarly the following paragraph which describes the “changing origin” simplification should be expanded upon and the implications discussed.

2) In some cases the terminology is not conducive to clarity and may be inconsistent or utilized prior to being defined. For instance j, u, I, f, closest, and farthest in the constraints on equation 1. These can be inferred with some difficulty but should be explicitly defined prior to their use.

3) It is appreciated that the authors note that the method’s success is subject to its initialization. As the method is constructed as a very highly constrained optimization problem and implemented using an optimization algorithm that doesn’t provide global guarantees, this is potentially a major issue. This is partially addressed by the “Initial guess” paragraph in the implementation paragraph, however this issue is worthy of more thorough evaluation and discussion. How frequently does the optimization fail to find a solution? The ramifications of this issue would become more clear if quantitative evaluation of this issue was included in the work, perhaps via some notion of randomized trajectories and randomized environments. Such an evaluation would then in turn would help the reader understand the likelihood of success of the method with respect to desired trajectories outside of the three currently evaluated.

4) Building on the last point, inclusion of the computation time analysis in the paper is appreciated, but could be more thorough. Computational time required by constrained optimization is generally heavily dependent on the specific problem instance and as such the computational time required by the method isn’t sufficiently evaluated by only 3 problem instances. If randomized trajectories or environments were included as described in the previous point, the timing results would provide more insight into the time required by the method across more problems. Further, the RRT-based comparison method, as a sampling-based method, is going to be dependent on the random seed it is initialized with. It would be valuable to run the RRT-based method multiple times and report statistics on the timing results for this to be more meaningful.

5) The method is specifically tailored to end effector trajectories in R^2 to be followed by a planar manipulator of 2 or 3 links. However most of your motivating examples in the introduction would require R^3 or SE(3) end effector trajectories to be executed on more complex, non-planar manipulators. The paper would be strengthened if the case solved by the method (Planar trajectory with 2 or 3 link planar manipulator) is given more motivation. I do appreciate that an extension to 3D trajectories with more DOFs in the manipulator is mentioned in the Future Work paragraph, however it would be great if the authors could also include a very short description of potential ways they envision modifying or building on the method in order to expand to higher dimensional trajectories.",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,https://github.coecis.cornell.edu/tcd58/Shape-Primitives,https://youtu.be/9Uvyu2FJtVM,,,30.0,3O3OLHn1uW4
"Controlling a non-statically stable biped is a difficult problem largely due to the complex hybrid dynamics involved.  Recent work has demonstrated the effectiveness of reinforcement learning (RL) for simulation-based training of neural network controllers that successfully transfer to real bipeds.  The existing work, however, has primarily used simple memoryless network architectures, even though more sophisticated architectures, such as those including memory, often yield superior performance in other RL domains. In this work, we consider recurrent neural networks (RNNs) for sim-to-real biped locomotion, allowing for policies that learn to use internal memory to model important physical properties. We show that while RNNs are able to significantly outperform memoryless policies in simulation, they do not exhibit superior behavior on the real biped due to overfitting to the simulation physics unless trained using dynamics randomization to prevent overfitting; this leads to consistently better sim-to-real transfer. We also show that RNNs could use their learned memory states to perform online system identification by encoding parameters of the dynamics into memory.",AUTHOR_AGREEMENT.pdf,Jonah Siekmann (Oregon State University)*; Srikar Valluri (Oregon State University); Jeremy Dao (Oregon State University); Francis Bermillo (Oregon State University); Helei Duan (Oregon State University); Alan Fern (Oregon State University); Jonathan Hurst (Oregon State University),Jonah Siekmann (Oregon State University)*; Srikar Valluri (Oregon State University); Jeremy Dao (Oregon State University); Francis Bermillo (Oregon State University); Helei Duan (Oregon State University); Alan Fern (Oregon State University); Jonathan Hurst (Oregon State University),RSS_2020__Learning_Memory_Based_Control_for_Human_Scale_Bipedal_Locomotion.pdf (5817738 bytes); AUTHOR_AGREEMENT.pdf (57588 bytes),RSS_2020__Learning_Memory_Based_Control_for_Human_Scale_Bipedal_Locomotion.pdf,1256.0,31.0,,1.0,Learning Memory-Based Control for Human-Scale Bipedal Locomotion,,0.960753579758091,"I think that the role of memory + domain randomization to effectively perform online-system-identification is important and understudied. By themselves they are not new ideas, but the fine points often matter, and it is never full clear whether proposed algorithmic ideas are as agnostic to the task and hardware as we might like.

The framing of the paper could be improved, I think.

Defining-and-distinguishing between online-system-identification and disturbance-observation would be helpful. It was not clear to this reader as to whether these were the same thing or not. It would also be nice to frame the work in the same space as work that learns a more explicit model of the dynamics parameters, i.e., ref 22. And splitting the discussion into the matrix of combinations defined by (FF,LSTM) x (noDR,DR) would be useful, because that is the core issue of the paper.  One could then hypothesize that:
FF, noDR -- hypothesis: will overfit to the simulation dynamics
FF, DR  -- hypothesis:  will produce a motion that is robust to some param variation
LSTM, noDR -- not clear what the hypothesis is here;  it's not clear why
    this should learn something too much different than for FF, noDR
LSTM, DR  -- hypothesis:  will use the policy memory to do online system identification

One subtle issue worthwhile thinking about:  if we draw an analogy between online adaptation and Kalman filtering, then what defines the Kalman ""gain""that must be implicit in the memory-based controller?

The abstract could jump more directly to the point, and instead devote more space to the conclusions.

The task could be clearly defined, i.e., walk at a range of speeds [a--b], as modeled implicitly by 
a reference trajectory that is parameterized by speed (in some way).

The structure of section IV A could be improved.
The first paragraph merges technical details with some results discussion.

Fig 4  the title (first part of caption) could be: ""Learning curve without dynamics randomization""

Adding a 4th column to Table III, i.e., FF DR, would help clarify the structure,
even if all the entries are a dash, indicating a failure or poor policy.

Why not give the disturbance/randomization information to the critic?  It will be discarded at run time
anyhow. This is a ""asymmetric actor critic"" structure (see a paper that has this title). Thus it is
not clear that the critic requires the memory.

The randomization interval for the COM seems excessively large, i.e., [-25,6] cm.
Why is it difficult to know the pelvis cm to within 5cm?

It would have been interesting to a see a dynamic alteration made to the COM of the robot and to see
that the recovered COM estimate had adapted accordingly.  Or some other parameter that might be easier to 
change in an online setting. 

""Our learning process makes use of a reference trajectory"" 
Perhaps better to say that it learns to imitate a given reference trajectory.

Fig 3 could be condensed, or simply summarized in the text.

The recurrent PPO policy learning is unique, which is both good and bad. 
How does the learning structure compare to other similar work?

Table IV and V are reference out-of-order in the text.",Agreement accepted,"The paper investigated the problem of transferring a simulation trained policy to a real physical robot (cassie). The core idea is to train a recurrent neural network policy (represented as LSTM) and randomize the dynamics of simulation during training. The result seems solid, the paper is well written and easy to follow, and it’s great to see that the method works on a real physical robot.

My main concern about  the paper is that the approach it’s taking is essentially the same as the one in Peng et al 2018, in which they also trained an LSTM policy and used dynamics randomization for sim-to-real transfer. The major differences are that the training algorithm and the robot are different. 

In addition to the method, the analysis on which dynamics to randomize is potentially interesting as it shows that the baseline methods, trained with certain dynamics parameters, can work as well as the proposed method. Though the focus of this paper is on the LSTM policy with dynamics randomization, it is nevertheless interesting to see more details about the selected dynamics sets as it might provide insights on which parameters are more important for the tasks. The analysis for predicting the dynamics parameters from LSTM latent variable is also interesting. However, they don’t really lead to significant change from the previous methods.

In general, I think the paper has developed an interesting learning system that demonstrates good results on real robots, while the technical contribution is limited due to the similarity to prior works.
",Agreement accepted,"This paper proposes to use deep reinforcement learning (PPO) and domain randomization to learn a recurrent policy (LSTM) for the Cassie robot. The paper is clearly written, thoroughly evaluated and the results are compelling. I really appreciate the real robot results.

Although none of the individual components (PPO, reward based on imitation, LSTM policy, domain randomization) of this paper is novel, as a researcher in the field of locomotion and learning, I admit that I have learned a lot from this paper, which is summarized below:
1) The clock (phase) input is essential for learning a successful policy.
2) A significant sim-to-real gap does exist for the Cassie robot (It appeared otherwise in the prior work of [Xie et al.]).
3) Combining RNN and domain randomization gives the best sim-to-real performance.
4) The memory may encode the dynamics. Although I have some doubts about this conclusion, given that the Mean Percent Error is still high (~31%) in Table IV, this observation is inspiring and worth further investigation (maybe as future work) because this finding could be paradigm shifting. If the memory learned to encode dynamics, learning with memory could replace the painful manual system identification process.

I believe that other researchers in this field would also be benefitted by reading this paper. It is clearly an important step towards automatic design of  locomotion controller for legged robots. For this reason, I would recommend accepting the paper.",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,https://github.com/osudrl/RSS-2020-learning-memory-based-control,https://youtu.be/V8_JVvdJt_I,,,31.0,xNyKpku8CNU
"We consider the problem of generating a time-optimal quadrotor trajectory that attains a set of prescribed waypoints. This problem is challenging since the optimal trajectory is located on the boundary of the set of dynamically feasible trajectories. This boundary is hard to model as it involves limitations of the entire system, including hardware and software, in agile high-speed flight. In this work, we propose a multi-fidelity Bayesian optimization framework that models the feasibility constraints based on analytical approximation, numerical simulation, and real-world flight experiments. By combining evaluations at different fidelities, trajectory time is optimized while keeping the number of required costly flight experiments to a minimum. The algorithm is thoroughly evaluated in both simulation and real-world flight experiments at speeds up to 11 m/s. Resulting trajectories were found to be significantly faster than those obtained through minimum-snap trajectory planning.",Ryou.Tal.ea.RSS20.Author_Agreement.pdf,"Gilhyun Ryou, [Ezra Tal](http://www.ezratal.net) [Sertac Karaman](http://karaman.mit.edu)",Gilhyun Ryou (Massachusetts Institute of Technology)*; Ezra Tal (Massachusetts Institute of Technology); Sertac Karaman (Massachusetts Institute of Technology),Ryou.Tal.ea.RSS20.final.pdf (1042746 bytes); Ryou.Tal.ea.RSS20.Author_Agreement.pdf (141062 bytes),Ryou.Tal.ea.RSS20.final.pdf,1342.0,32.0,,1.0,Multi-Fidelity Black-Box Optimization for Time-Optimal Quadrotor Maneuvers,,0.006022480597571,"This paper addresses  the problem of planning dynamically feasible fast trajectories, using given waypoints as reference, for autonomous drones that accounts for the limitations of vehicle dynamics and that fully exploits the capabilities of the vehicle. The main challenge is that feasibility of trajectories needs to be considered as a whole since it is hard to express it based on constraints on an admissible set of control inputs and states. Instead, they are modeled from real experiments, which may be risky considering that the faster trajectories are close to the limits for the robot.

The proposed approach consists on a multi-fidelity optimization that uses a gaussian process as a black-box that classifies candidate trajectories as feasible or infeasible. It uses a Bayesian optimization to explore and exploit the feasibility frontier.

The paper is very well written. It has a solid analytical development of the minimization algorithm. The complexity of the algorithm is nicely kept under control. The exploration/exploitation of the feasibility frontier is insightful, and he experiments are impressive. I recommend the authors to strengthen their evaluation with either a discussion or experiments against other approaches that aim to generate fast trajectories.

Minor comments:
page 1: “the resulting a set of… ""
the right column on the last example in the video should be replaced by one taken from a point of view that can see the whole trajectory. ",,"Here are a few comments below that may help further improve the paper.

I - Approach and presentation
- The statement 'optimal trajectory is found at boundary of the set of feasible trajectories' doesn't have a reference. I am not sure this is true in general, but even in some specific context an intuitive example or a reference is necessary.
- The first time \alpha appears it is refereed to as the acquisition function. Some more explanation as to what is an acquisition function would benefit readability for a wider audience.
- There is some ambiguity in the overall approach as to what happens online vs offline. Is the approach augmenting the dataset to do incremental learning in an online setting or is the model learned batch style and then deployed to find trajectories, for example, for the real robot. If there is incremental learning is the variational inference performed every time a new data point is added?
- There are a lot of variables and I found myself often going back and forth trying to find their definition. Sometimes the notation maybe inconsistent, for example, l has superscripts and subscripts and have been used interchangeably or their distinction wasn't made clear.
- Trajectory plots are hard to interpret. For instance, Fig 3 is quite small and the distinction in color is hard to see to understand the altitude. Maybe a perspective 3D view would be more helpful or the trajectory with the quadrotor on it at waypoints in various orientations would help ground it.

II - On experiments and practical details
- Going back to the online vs offline ambiguity is it unclear if the learned model is deployed in settings similar to the training data i.e. the boundaries between training and testing is unclear. Given this, how well does the method generalize to problems it has not seen before?
- Result in IVA referring to Fig. 2 is 2%. Maybe this is a typo and it is closer to 20%?
- In Fig. 4 the lower bound on improvement is about 5%. Why was the improvement significantly less here? Any way to know how improvement relates to the problem defined by the desired set of waypoints?
- What were the computational times of the approach?
- It wasn't clear how someone would choose the fidelity levels or even define how many are needed for an application. Also how does the approach scale with increasing number of fidelity classes.

III - Limitations
The authors should consider adding a limitations section where the following could be discussed:
- Is the approach real-time or is the trajectory computed offline and then followed by tracking?
- How is state uncertainty handled?
- How would obstacles be handled?
- The real world flights show the quadrotor moving through free space, how is the tolerance to the desired waypoint verified? With respect to the drone racing challenges mentioned in the introduction typically the quadrotors are required to pass through a set of floating gates.",Agreement accepted,"Overall, the paper exposes clearly its objective. It is well written and the contribution is relevant. 
My first point is that it could make a better job a motivating the need to reach exactly the optimum (there are plenty of good reasons for this,
but they are not mentioned here).

My second point is that the paper lacks an important discussion about the cases where the feasibility constraints are wrongly approximated.
In such cases the consequences would of course be rather dramatic, and it seems important to obtain some estimation of how likely this is to occur
with respect to formulation (6).
This brings back to the question of ""do we really want the optimum"" for uncertain hardware control ? 
I am not saying that identifying the feasibility constraint is not relevant, but the study should somehow discuss how confident the information is.
",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,https://youtu.be/igwULi_H1Kg,,,32.0,xI-g9dR_TPU
"In robot manipulation, planning the motion of a robot manipulator to grasp an object is a fundamental problem. A manipulation planner needs to generate a trajectory of the manipulator to avoid obstacles in the environment and plan an end-effector pose for grasping. While trajectory planning and grasp planning are often tackled separately, how to efficiently integrate the two planning problems remains a challenge. In this work, we present a novel method for joint motion and grasp planning. Our method integrates manipulation trajectory optimization with online grasp synthesis and selection, where we apply online learning techniques to select goal configurations for grasping, and introduce a new grasp synthesis algorithm to generate grasps online. We evaluate our planning approach and demonstrate that our method generates robust and efficient motion plans for grasping in cluttered scenes.",signature.pdf,"[Lirui Wang](https://liruiw.github.io/), [Yu Xiang](https://yuxng.github.io/), [Dieter Fox](https://homes.cs.washington.edu/~fox/)",Lirui Wang (University of Washington)*; Yu Xiang (NVIDIA); Dieter Fox (NVIDIA Research / University of Washington),signature.pdf (372153 bytes); Manipulation_Trajectory_Optimization_with_Online_Grasp_Synthesis_and_Selection_RSS.pdf (2418066 bytes),Manipulation_Trajectory_Optimization_with_Online_Grasp_Synthesis_and_Selection_RSS.pdf,66.0,33.0,,1.0,Manipulation Trajectory Optimization with Online Grasp Synthesis and Selection,https://liruiw.github.io/planning.html,0.271936687203348,"Grasping an object requires grasp planning (selecting a grasp) and motion planning (planning the arm motion to that grasp). Rather than treating these as separate problems, this paper propose an integrated Optimization-based Motion and Grasp (OMG) planning system. An iterative algorithm, at each integration, given a grasp, the planner uses CHOMP trajectory optimization with a goal set constraint to refine the trajectory. Next, the planner updates the probability distribution over the grasp set, which is composed of grasps from existing grasp sets and from newly synthesized grasps, to select the new best grasp. The process then repeats. The grasp synthesis process uses a novel algorithm, Configuration Space Iterative Surface Fitting (C-Space ISF), to iteratively refine grasps. The experimental section conducts detailed evaluation of the method using a Franka Emika Panda manipulator and the YCB object set. Specifically, the experiments compare various online learning strategies, the grasp refinement algorithm with various initial grasp sets, planner performance compared other existing planners and an ablation study on different parameters. 

The paper is clearly written and the explanation of the algorithm is straightforward. Sec. III introduces each algorithmic piece with fairly clear explanations. The related work (Sec. II) is quite detailed well organizes the many different research sub-areas that connect to this work. At various points, the paper offers intuitive explanations for the algorithmic choices made. Fig. 3 is particularly insightful in expressing the behavior of the algorithm. The experimental section is detailed, both in providing the parameters and settings and in exploring various aspects of the algorithm. The video animates several of the figures and shows examples of the planner's abilities. 

My primary questions relate to the online learning algorithm. As someone who is admittedly less familiar with this piece of the related work, I found the description of the four strategies for updating the grasp distribution in Sec III.D  (FC, FTL, EXP, MD) to be harder to follow and I would appreciate more details. Algorithm 2 nicely summarized the OMG planner, bringing together each aspect, however the subroutine ONLINE_LEARNING was not detailed. Additionally, its not immediate clear what is being learned. It seems that new grasps are being synthesized and probability distributions are being updated, but what is being learned? 

Also with respect to the grasp synthesis, Algorithm 1 shows how the grasp is refined via the objective function. Is only one grasp (the mode of the distribution) refined? Algorithm 1 does not make it clear if this grasp is added to the distribution. How well would this method work on a previously unseen object, where we do not have a predefined grasp set? 

Below are a few other minor questions/suggestions/comments: 
- In defining the objective function for grasp synthesis, the collision objective, f_collision(g) balances two notions of collision checking, one that checks the grasp and one that comes from CHOMP. It was not clear the difference between the two functions in practice. 
- Sec. III covers methodology. It is my understanding that Sect III.A-II.C are a recap of prior work (i.e. CHOMP). Is this correct? It may be useful to more clearly delineate what is prior work versus what is the contribution. 
- The related work section mentions that sampling-based methods suffer from computational challenges and the need for post-processing. This point is slightly undercut by the fact that the supporting cite is twenty-two years old and significant progress has been made in planning since then. 
- Sec. III.E mentions that a fixed grasp set can have low adaptability to an environment. However, a fixed grasp set could be generated independent of the environment and then grasps invalidated by the environment could be pruned online. 
- There is no discussion on how to compute the gradient of f_grasp. 
- In Sec. III.E it is not clear what transform T is referring to. 
- In Fig. 4 and particularly Fig.5 it is difficult to see the vectors when the paper is printed out in black and white. 
- \bar{\Xi} and \hat{\Xi} look fairly similar, which made reading part of Sec III.D a little difficult. ",,"Overall I think the paper is on its way to being a good contribution but there are still a few things to be addressed:

I - Presentation and approach
- Related work: Some super relevant work seems to be missing including [a]. In the intro two categories of grasping are mentioned but the seconds one does not cite anything. On the other hand there is a discussion in related work on TAMP that I don't think is particularly relevant to problem here.

- The primary difficulty I have with reviewing this paper is the confusion in the overloaded use of 'online'. Throughout the paper it is often ambiguous as to what aspect of the algorithm and its evaluation happen online vs offline. For the rest of this review I am going to assume the following based on what I could gather from multiple reads of the paper: the entire approach of optimizing the trajectory and selecting the grasp happens offline given a new scene and a problem, and once a plan is found it is executed open loop without any replanning. Given this 'online' from the title should also be dropped.

- This ambiguity could be resolved by stepping through the approach using an example problem. For instance in Fig. 2. at various steps of the algorithm the robot appears to be in different places. If an offline trajectory is being solved for discussing in the context of the whole trajectory would make sense. Is the loop in Fig. 2 an iteration of the offline algorithm or is it making online execution steps by giving feedback.

- In Eq. (8) g is refereed to as a goal configuration at ith iteration. If only one goal is used isn't this just now vanilla CHOMP and not goal-set CHOMP?

- Figures: Fig. 1 is not very clear to see what is happening. Maybe just show a few waypoint with the robot and add the trajectory taken by the end effector. In Fig. 6 is none of green or red stuff mentioned is clearly visible from those viewpoints.

II - Evaluation
- Overall the evaluation is unable to highlight the strengths of the approach. Comparing Table I and III it does not seem like the grasp synthesis part adds much improvement. Then, the critical part of the approach is just goal-set CHOMP plus grasp selection from the goal set with something like mirror descent. This makes the contribution seem incremental and weak at best.

- Parameter tuning and effect on the Algorithm: The ablation for \lambda while appreciated is less useful since this parameter has been explored in CHOMP. Discussion on \gamma, \alpha, \beta would be more helpful.

- How are body points uniformly sampled on the robot surface?

- Is the 'Execution' success a percentage out of the ones that were first successful in 'Planning'?

- Report std on the results would be useful.

III - Limitations
A limitations section could be included to possibly discuss the following:
- The trajectory tail approximated with linear interpolation: how accurate is this? what if the trajectory is in collision or violates other constraints?
- For a novel application how many grasps in G are necessary and how does the performance scale with the size of G?
- How is fighting between competing objective resolved? For example, reaching object vs obstacle avoidance to another object close to it.
- How would the simulator performance translate to the real world?


[a] Berenson, D., Srinivasa, S., & Kuffner, J. (2011). Task space regions: A framework for pose-constrained manipulation planning. The International Journal of Robotics Research, 30(12), 1435-1460.",Agreement accepted,"The summary is above.   The paper is well written and clear.  Several variations of the algorithms are presented and evaluated.  I liked that it acknowledges the presence of local minima in grasp refinement (ISF) and therefore the requirement to choose discretely among targets.  

I have a few small suggestions.
- In a number of cases, the paper and video uses ""optimal"" when ""locally optimal"" or ""best among our options"" should be used instead.
- There is no discussion about run-times (except at one point suggesting that MD works better but it's slower).
- Somewhere it should be acknowledged that these beautiful motions probably won't work in the presence of sensing uncertainty.
- If one imagines a depth-sensor on the end-effector, one could use the on-line learning to refine the grasp in the presence of uncertainty.  But, in that case, one would want to consider information gathering as part of the process.  Possible future work...",Agreement accepted,,,Agreement accepted,07/14 15:00,07/14 17:00,,https://liruiw.github.io/planning.html,,,33.0,0rZu2cYHzy8
"Robotic fabric manipulation has applications in home robotics, textiles, senior care and surgery. Existing fabric manipulation techniques, however, are designed for specific tasks, making it difficult to generalize across different but related tasks. We extend the Visual Foresight framework to learn fabric dynamics that can be efficiently reused to accomplish different fabric manipulation tasks with a single goal-conditioned policy. We introduce VisuoSpatial Foresight (VSF), which builds on prior work by learning visual dynamics on domain randomized RGB images and depth maps simultaneously and completely in simulation. We experimentally evaluate VSF on multi-step fabric smoothing and folding tasks against 5 baseline methods in simulation and on the da Vinci Research Kit (dVRK) surgical robot without any demonstrations at train or test time. Furthermore, we find that leveraging depth significantly improves performance. RGBD data yields an 80% improvement in fabric folding success rate over pure RGB data. Code, data, videos, and supplementary material are available at https://sites.google.com/view/fabric-vsf/.",author_agreement.pdf,"[Ryan Hoque](https://ryanhoque.github.io/), [Daniel Seita](https://people.eecs.berkeley.edu/~seita/), [Ashwin Balakrishna](https://abalakrishna123.github.io/), [Adi Ganapathi](https://www.linkedin.com/in/aditya-ganapathi), [Ajay Tanwani](http://ajaytanwani.com/), [Nawid Jamali](https://www.linkedin.com/in/nawidj), [Katsu Yamane](http://www.katsuyamane.com/), [Soshi Iba](https://www.linkedin.com/in/soshi-iba-7090467), [Ken Goldberg](https://goldberg.berkeley.edu/)","Ryan Hoque (UC Berkeley)*; Daniel Seita (University of California, Berkeley); Ashwin Balakrishna (UC Berkeley); Aditya Ganapathi (University of California, Berkeley); Ajay Tanwani (UC Berkeley); Nawid Jamali (Honda Research Institute); Katsu Yamane (Honda Research Institute); Soshi Iba (Honda Research Institute); Ken Goldberg (UC Berkeley)",vsf_draft_v24_jun3.pdf (2662287 bytes); author_agreement.pdf (457155 bytes),vsf_draft_v24_jun3.pdf,1214.0,34.0,,1.0,"VisuoSpatial Foresight for Multi-Step, Multi-Task Fabric Manipulation",https://sites.google.com/view/fabric-vsf,0.302678670833521,"The paper is well written and topical, and addresses an area of interest to RSS audience.

The contributions are primarily on system level, combining and adapting recent methods from computer vision and machine learning such as domain randomization, video prediction and visual planning with the aim to solve a currently open robotics problem of cloth manipulation planning. The contribution is moderately significant.

The experiments are nice and the analysis is based on data. Statistical significance tests would be useful in order to analyze which findings (e.g. # of actions) are not random. The results seem quite impressive, considering the fact that the dynamics are learned in simulation. However, a link to a video showing the results would have been nice to better understand the complexity of the policies as well as the dynamics. 

The paper would benefit from a more thorough discussion, especially related to the limitations and open issues. For example, the physical fabric folding experiment concludes that there is a mismatch between simulated and real dynamics models. What are the implications of this? For example, are the current simulators unsatisfactory or is that a calibration problem?

",,"The paper is clearly written and provides many details. Experimental validation is mostly convincing. As learning in simulation is claimed as a contribution, I was expecting to see an ablation study for domain randomization (DR). What's the effect of DR on results in simulation and on the real robot?

As this is mostly an experimental paper, it will be valuable if the authors publish the code and the experimental setup enabling to reproduce their results.
",Agreement accepted,"To add clarity to the paper, it would be helpful for uninitiated readers if the authors could define some of the  terms including: visual foresight, visual dynamics.",,,,Agreement accepted,07/14 15:00,07/14 17:00,https://github.com/ryanhoque/fabric-vsf,,,,34.0,pocbz8FXnQ4
"Typical end-to-end formulations for learning robotic navigation involve predicting a small set of steering command actions (e.g., step forward, turn left, turn right, etc.) from images of the current state (e.g., a bird's-eye view of a SLAM reconstruction). Instead, we show that it can be advantageous to learn with dense action representations defined in the same domain as the state. In this work, we present ""spatial action maps,"" in which the set of possible actions is represented by a pixel map (aligned with the input image of the current state), where each pixel represents a local navigational endpoint at the corresponding scene location. Using ConvNets to infer spatial action maps from state images, action predictions are thereby spatially anchored on local visual features in the scene, enabling significantly faster learning of complex behaviors for mobile manipulation tasks with reinforcement learning. In our experiments, we task a robot with pushing objects to a goal location, and find that policies learned with spatial action maps achieve much better performance than traditional alternatives.",RSS2020_Author_Agreement-signed.pdf,"[Jimmy Wu](https://www.cs.princeton.edu/~jw60/), [Xingyuan Sun](https://people.csail.mit.edu/xingyuan/), [Andy Zeng](https://andyzeng.github.io), [Shuran Song](https://shurans.github.io), [Johnny Lee](http://johnnylee.net), [Szymon Rusinkiewicz](https://www.cs.princeton.edu/~smr/), [Thomas Funkhouser](https://www.cs.princeton.edu/~funk/)",Jimmy Wu (Princeton University)*; Xingyuan Sun (Princeton University); Andy Zeng (Google); Shuran Song (Columbia University); Johnny Lee (Google); Szymon Rusinkiewicz (Princeton University); Thomas Funkhouser (Princeton University),RSS2020_Author_Agreement-signed.pdf (204219 bytes); rss20.pdf (4046966 bytes),rss20.pdf,103.0,35.0,,2.0,Spatial Action Maps for Mobile Manipulation,https://spatial-action-maps.cs.princeton.edu,0.484196883187172,"This paper proposes an action representation (called ‘spatial action maps’) for robots learning to manipulate objects using deep reinforcement learning. This work is inspired by previous works using dense action representations. An agent is trained, using simulation, to push objects to a target location. While a standard algorithm is used for training (DDQN), the policy is represented using a Fully Convolutional neural net. Experimental results using a few baselines show some promise of the proposed approach.

The paper in general is well-written, and I was very excited when reading sections I and II. This excitement decreased from section III. For example, the reward function assumes that the distance between objects and the target location is known — which may not be the case in more complex scenarios. In addition and most importantly, the experimental setting is very toy-like environment. This environment assumes that the action representation is available in one image, which is a strong assumption in more complex  (real-world) environments. Last but not least, the experiments do not take into account strong baselines to compare against. It would have been interesting to see methods X, Y and Z with and without spatial action maps, where the use of spatial action maps makes a substantial difference — not only in a toy environment but in a more realistic one. ",Agreement accepted,"Originality

The authors propose a novel representation for actions in mobile manipulation settings and discuss several advantages of the proposed representation. They also present an empirical study that showcases the advantages of the approach. The action representation is indeed novel, to the best of my knowledge.

Quality

The results are impressive and the evaluation is thorough, especially the ablations. Figure 7 clearly shows the value of the proposed action space. Taken together, Tables 1,2,3, and 4 all show the effect of different kinds of ablations (using straight line paths instead of shortest paths in the movement primitives, using a fixed step size, etc). A consistent trend is that the design choices make the most difference in the Large Divider environment - this makes sense since the agent must navigate around the large divider in order to push all of the blocks successfully.

The section on limitations of the approach is an important inclusion and is appreciated. As for the supplementary website, the videos are useful to watch. The emergent behavior of grouping items against the wall and then sweeping multiple objects with long trajectory is indeed interesting, as discussed by the authors.

Clarity

Overall, the authors provide a very thorough explanation of their method, the state and action representations used, and their results. One minor point - the paper could use more details on how gradients are passed only through the state pixel corresponding to a selected action pixel.

Significance

Overall, this is a good paper that proposes a nice idea for an action space, and presents thorough validation that the proposed action space outperforms other choices.",Agreement accepted,"In this paper, the authors propose so-called spatial action maps for navigation tasks. Instead of predicting the low-level control signal, they learn Q-values for all pixels in the top-down map of the environment, where each pixel encodes an action, i.e. trajectory from the current position of the robot to the corresponding pixel. The trajectory may be represented as linear or shortest path. They showed that such reinforcement learning on top of such an action representation, allows faster and better learning compared to more conventional RL approaches.

As authors noted, their approach is inspired from the 'dense action representations' approach that establishes an explicit mapping between manipulation actions of the robot and the pixels of the top-down image of the environment in bin picking task. They extend and apply this approach to a different domain, navigation domain where actions have different costs and long-term planning is required. The method is trained in several simulated environments and validated in simulation and (partially) in real-world.

The approach is well-motivated, the paper is clearly written, the methods are sound and validated in various experiments where different methodological choices were systematically analyzed. I believe that although there is no huge leap from theoretical and novelty perspective compared to the inspired work, it has the potential to bring a new perspective and open a new research direction in reinforcement learning for navigation. Saying this, how the perceptual information is handled in simulation and processed for experiments in the real world are extremely simple. In both cases, world state that is encoded various 2d maps used as input to the neural network are noise-free and perfect. Although exactly which methods are used in the simulation are not clear, the construction of the 2d maps are based on noise-free front-looking camera and robot pose information, which is not realistic in real settings. Furthermore, the procedure that constructs 2d maps in the real world also use exact robot and obstacles poses that are obtained from markers and overhead camera and assumes that the sizes of the obstacles are all known as well. Such simplification in the perception side degrades the impact of the paper significantly. The authors at least could have tried to use a simple 2d depth sensor or a simple rgd-b camera with a view similar to the simulation setup. The authors discussed the influence of uncertainty was not in the scope of this paper ('uncertainty in state and observations is .. an active research topic,... is orthogonal to our investigation') , which I do not agree. 

The authors nicely investigated the impact of the using 'shortest-path' actions, and state information from 'shortest-path' heuristic in the experiments, and concluded that use of this heuristic (they call shortest path computations) helps the system in complex setups. Indeed, some problems might be very difficult such that naive approaches without well-designed reward/action/state shaping perform poorly. However is it really scalable to hand-design such heuristics for different tasks with different complexities. Should we design a completely new strategy instead of shortest path to the receptacle if the task is to push the objects in any receptacle in an environment with n receptacles? What happens if some objects are rollable and the definition of shortest path suddenly changes? Can the robot adapt to such changes? I would like to see a discussion on the scalability of this approach.

For a better baseline, the authors might benefit from comparisons with actor-critic algorithms, where for example images are used as states via convolutional neural networks in navigation tasks[1]. The advantage of action maps might be shown in comparison with direct policy optimization with Q-networks as critics. Comparison only with Q-networks, where Q networks evaluate the steering commands, limits the perception of the contribution. Additionally, the choice of step size is an important part of the proposed method and the explanation falls short, regarding how big the step size should be. 

The authors mentioned that robustness and generalization is evaluated to unseen dynamics in the real world (e.g. mass and friction). Please explicitly support this statement in the experimental results section.

Please provide the exact values of the partial rewards for replicability. Please provide how the reference in Fig. 7 is formed for replicability. 

The experimental analysis on 'Effect of actions with fixed step size' does not provide important insight. Furthermore, computing shortest path and using only a fraction of it is not intuitive. This part can be removed. 

[1] Jonas Kulhanek, Eric Derner, Tim de Bruin, Robert Babuska. Vision-based navigation using deep reinforcement learning. 2019 European Conference on Mobile Robots (ECMR) (Sep 2019). https://doi.org/10.1109/ecmr.2019.8870964

",,,,Agreement accepted,07/15 15:00,07/15 17:00,https://spatial-action-maps.cs.princeton.edu,https://spatial-action-maps.cs.princeton.edu,,,35.0,eiZE-dzuSuU
"In this paper, we present a new class of Markov decision processes (MDPs), called Tsallis MDPs, with Tsallis entropy maximization, which generalizes existing maximum entropy reinforcement learning (RL). A Tsallis MDP provides a unified framework for the original RL problem and RL with various types of entropy, including the well-known standard Shannon-Gibbs (SG) entropy, using an additional real-valued parameter, called an entropic index. By controlling the entropic index, we can generate various types of entropy, including the SG entropy, and a different entropy results in a different class of the optimal policy in Tsallis MDPs. We also provide a full mathematical analysis of Tsallis MDPs.Our theoretical result enables us to use any positive entropic index in RL. To handle complex and large-scale problems such as learning a controller for soft mobile robot, we also propose a Tsallis actor-critic (TAC). For a different type of RL problems, we find that a different value of the entropic index is desirable and empirically show that TAC with a proper entropic index outperforms the state-of-the-art actor-critic methods. Furthermore, to alleviate the effort for finding the proper entropic index, we propose a linear scheduling method where an entropic index linearly increases as the number of interactions increases. In simulations, the linear scheduling shows the fast convergence speed and a similar performance to TAC with the optimal entropic index, which is a useful property for real robot applications. We also apply TAC with the linear scheduling to learn a feedback controller of a soft mobile robot and shows the best performance compared to other existing actor critic methods in terms of convergence speed and the sum of rewards. Consequently, we empirically show that the proposed method efficiently learns a controller of soft mobile robots.",Scan_20200528_114019.pdf,Kyungjae Lee (Seoul National University)*; Sungyub Kim (KAIST); Sungbin Lim (UNIST); Sungjoon Choi (Disney Research); Mineui Hong (Seoul National University); Jaein Kim (Seoul National University); Yong-Lae Park (Seoul National University); Songhwai Oh (Seoul National University),Kyungjae Lee (Seoul National University)*; Sungyub Kim (KAIST); Sungbin Lim (UNIST); Sungjoon Choi (Disney Research); Mineui Hong (Seoul National University); Jaein Kim (Seoul National University); Yong-Lae Park (Seoul National University); Songhwai Oh (Seoul National University),Scan_20200528_114019.pdf (1267531 bytes); 2020_Lee_tac_for_soft_robot_v00 (13).pdf (2488084 bytes),2020_Lee_tac_for_soft_robot_v00 (13).pdf,18.0,36.0,,2.0,Generalized Tsallis Entropy Reinforcement Learning and Its Application to Soft Mobile Robots,[Not Answered],0.9718165195972259,"Entropy-based methods are very popular in RL due to improved exploration, stability and performance. As a result, improvements to them would have great impact. Changing the form of the entropy term seems like one promising way of improvement. 

The paper is predominately a theory paper. The main contribution (from my perspective) is formalizing the Tsallis MDP and proving convergence of the methods. This is a solid contribution. 

The theory would not be particularly helpful if it couldn't be used in the algorithms and if those algorithms didn't improve performance. Thankfully, Tsallis entropy can be naturally incorporated into methods such as SAC. I see this simplicity of extension as a benefit as it could widen usage. 

Similarly, the experiments show improved performance of SAC and other actor-critic methods in almost all domains. They also show robustness with different alpha values and their scheduled entropy method also performs well. 

The real robot experiments (and part of the story of the paper) is focused on soft robotics. This is a fine domain, but is motivated by the need to have better exploration due to the properties of the soft robots. Sure, but other exploration methods could be used here instead (e.g., Bayesian, curiosity). Therefore, it isn't really clear what the robot experiments add over the simulation results. 

The paper is generally well written, but there are a number of typos that should be corrected. ",Agreement accepted,"1. I find the paper well-written and well-developed. I am excited about the annealing of the Tsallis entropy parameter during training to reduce the entropic regularization in a controller manner. In this context, I also find the performance bound in Theorem 7 useful. There are potential connections of this idea with proximal algorithms https://link.springer.com/article/10.1007/s40687-018-0148-y. 

2. This paper seems like a direct application of the Tsallis entropy to the existing theory of regularized MDPs (reference 21 in the paper) and RL algorithms. The novelty is therefore marginal.

3. The experiments in Fig. 4 have a very large variance in some cases, how is one to understand their importance? It is also surprising that the TD3 algorithm gets zero returns for Humanoid-v2. It has been recently recognized that entropic regularization may not effective in these benchmarks, e.g., https://spinningup.openai.com/en/latest/spinningup/bench.html. Can you discuss how the Tsallis entropy-based regularization may be better in practice?",Agreement accepted,"Summary

Many reinforcement learning method use some kind of entropy regularisation. This usually employs a Shannon-Gibbs entropy term, although the sparse Tsallis entropy has also been used. This work generalises both and employs Tsallis entropy, which is a family of functionals parametrised by q for which Shanon Gibbs (q=1) and sparse Tsallis (q=2) are special cases. The paper finds that by properly tuning the additional parameter, or by defining a curriculum over it that slowly goes from q=1 to q=2, they can often outperform various variants that employ Shanon-Gibbs entropy. This is demonstrated both on simulated MuJoCo environments as well as a hard to control real-robot system. 

Technical Quality

The paper proposes a technically solid algorithm, and show how its qualities both in theoretical proofs as well as in empirical demonstrations. These seem well executed. The proofs, however, are 11+ pages of dense content separate from the main material of the paper, and as such I cannot review them in detail. Perhaps this indicate that the paper would be more suitable for another venue where the proofs could take the spotlight rather than being relegated to supplementary material. 
Some relatively minor remarks on technical quality:
-> It doesn’t become very clear why Tsallis entropy works better then SG Entropy. The paper discusses stronger / less strong regularization (more or less stochasticity), but if this was the whole story one would image that having a curriculum for the “alpha” coefficient or for the minimum entropy (in something like SAC-AEA) should get similar results. It remains an open question what ‘above’ just making the regularisation less strong causes the difference. 
-> Optimal solution for entropy-regularised learning attributed to a bunch of papers from the last couple of years, but it is skipping the older work from Peters et al. which focuses on the relative entropy (e.g. Peters et al., Relative Entropy Policy Search, AAAI 2010). 


Novelty, Significance, Relevance
My main concern about the paper is whether this is really a robotics paper. The topic of efficient reinforcement learning is relevant to the robotics community, and the method is tested on a real robot (which is actually an interesting system, see below). However, the main contribution of the paper seem the theoretical proofs on machine learning and the simulation studies. 
Whenever we add a hyperparameter, we expect that it can be tuned such that it improve performance. What makes this paper stronger is that the curriculum seems to be valid across multiple environment, potentially avoiding an extra tuning step. I would thus consider the results to be somewhat significant. 
I haven’t seen Tsallis entropy used in reinforcement learning before, so I would consider the method quite novel. The robot task used is also quite interesting and as the task would be challenging for traditional control it is a good motivation to use a learning approach. 

Clarity
The structure of the manuscript is mostly good. There are a couple of minor grammar errors in the manuscript (see below for some examples) and there are a couple of sentences which seem quite cryptic (again, some examples provided below). The list below is not meant to be exhaustive, and a good proofreading pass should be performed. 
The description of the robot platform in VI.B is brief to the point of being hard to understand. Wouldn’t it be better to refer to a more complete description elsewhere (appendix or another paper)?

Minor issues: 
-“the trial and error” -> trial and error
-“of policy” -> of the policy
-“whose element is a probability” -> there seems to be a noun missing?
-“an MDP with the maximum Tsallis entropy” -> I’m guessing the performance of a *policy* that maximises (3) is meant? That is, not just Tsallis entropy but the sum of this entropy with the reward?
-I wasn’t sure what is meant by “Since updating J_phi requires to compute a stochastic gradient, we use a reparametrization trick […] instead of a score function estimation”. Using a score function estimator also results in a stochastic gradient. There are many reasons why you might prefer a reparametrization gradient, but needing a stochastic gradient doesn’t seem to be one of them. 
-In Section VI.C. theta_t isn’t defined where it is first used
-The vertical axis of the plots are slightly different, making it a bit harder than necessary to compare lines between e.g. 3a, 3b, 3c, 3d. 
-Some of the equations where the re-parametrization trick is used do not seem quite right. For example, in VII.A, consider the equation directly following “the gradient fo the Tsallis entropy becomes”. This would be completely correct if the expectation is taken with respect to \epsilon and a is replaced everywhere with f(a; \epsilon), with f indicating the reparametrization. The current notation hides the dependence of a on \epsilon in the gradient term on the right. 
-The words “proportional” and “inverse proportional” seem to be used a bit loosely in VII.A. (basically stating that \pi^2 is proportional to \pi?) 
",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,[Not Answered],[Not Answered],,,36.0,q7TygkWHMZY
"Affordance models are widely used in robotics to represent a robot's possible interactions with its environment. However, robot affordance models are inherently quantitative, making them difficult for humans to understand and interact with. To address this problem, previous works have constructed affordance models by grounding (connecting) them to natural language, but primarily used expert-defined actions, effects, or labels to do so. In this paper, we use short text responses provided by humans and simple randomized robot manipulation actions to construct a labeled affordance model that defines a relationship between English-language labels and robots' internal affordance representations. We first collect label data from a combination of crowdsourced real-world human-robot interactions and online user studies. We then use this data to train classifiers predicting whether or not a particular quantitative affordance will receive a specific label from a person, achieving an average affordance prediction score of 0.87 (area under Receiver Operating Characteristic curve). Our results also show that labels are more accurately predicted by affordance effects than affordance actions---a result that has been hypothesized in prior work but has never been directly tested. Finally, we develop a technique for automatically constructing a hierarchy of labels from crowdsourced data, discovering structure within the learned labels and suggesting the existence of a more universal set of affordance primitives.",RSS2020_Author_Agreement.pdf,"[Adam Allevato](https://allevato.me), [Elaine Schaertl Short](http://eshort.tech), [Mitch Pryor](https://www.me.utexas.edu/faculty/faculty-directory/pryor), [Andrea Thomaz](http://www.ece.utexas.edu/people/faculty/andrea-thomaz)",Adam Allevato (UT Austin)*; Elaine Short (Tufts University); Mitch Pryor (UT Austin); Andrea Thomaz (UT Austin),RSS2020_Author_Agreement.pdf (91327 bytes); RSS_2020___Affordance_Classification__Allevato_.pdf (908499 bytes),RSS_2020___Affordance_Classification__Allevato_.pdf,1184.0,37.0,,2.0,Learning Labeled Robot Affordance Models Using Simulations and Crowdsourcing,,0.276919756238474,"Summary of the paper:

In this paper the authors propose a data-driven model for grounding human provided, natural language labels to robot manipulation actions and their effects as represented in the robot’s affordance model. Authors use crowdsourcing to collect a dataset that associates a robot's action-effect pair with a set of natural language labels. A statistical model is learned from this dataset to predict a distribution over viable labels given a manipulation action and/or the resulting effect. Experiments are performed to test the efficiency of the trained model. Ablation experiment is performed to figure out whether action or effect parameters serve as a better features to train the model. In addition, a probabilistic approach is proposed to build a label hierarchy which provides some insights into the human understanding of affordances for the group of people involved in the study. 


Overall comments and recommendations:

The paper attempts to address a sufficiently important problem and fits well within the scope of the conference. It is well written in general. Language provides intuitive ways to interact with collaborative robots. Learning language grounded affordance models is an interesting research direction towards achieving robots that can collaborate. Therefore, adding a discussion about how this model can be inverted and generalized or integrated into a system that enables understanding complete sentences instead of keywords would help strengthen the contribution of the paper. This is important because language can be used to provide instructions of various fidelity such as, a high level instruction ""clear the table"" or a low level instruction such as ""bring the end effector closer to the top of the object in front of you and move forward 10 cm"". These instructions doesn't explicitly mention the affordance label ( i.e. the action verb), but could imply the learned affordances such as ""push"" or ""knock over"" (as in case of the second instruction). 


Other general criticisms:

- In abstract, the line “human input .. randomized robot actions” is a bit unclear. What kind of human input ? What randomized actions ? Without reading the paper, its hard to understand this line.  

- Opening line of the paper (introduction) that talks about the merits of grounding language can be paraphrased to more effectively convey the point. The ability to understand natural language commands enables efficient human robot collaboration in general. It has less to do with adapting to dynamic environments in my opinion. 

- Section 1, Para 3, Line “Our method provides insights into human perceptions of affordances …”. I am skeptical about this claim. The observation that the proposed model performs better when trained using the effect features instead of the actions features is not enough to talk about human perception of affordance. It is totally possible that this result is emerging due to the underlying learning mechanism used.

- Additional related work that’s worth considering: Recent language grounding approaches [1] that leverage crowdsourced datasets. A recent approach [2] that learns object affordance from combined language and vision modalities.

- Providing an example of action and effect parameters while describing the affordance triplet in Section 3.A would make it easier to conceptualize the experiments a little earlier in the paper.

- variable ‘o’ referring to the object features need to be removed from the R.H.S of equations 3 and 4.

- In section 3.D. Para 1, a wrong variable is used to represent the set of labels. ‘L’ should be used instead of ‘l’ .

- Section 3.D Paragraph 3 states that a single multi-class classifier is learned, but section 4.D states that multiple one-class SVM classifiers were trained using SVM for the experiment.

- Arrows in Figure 2 need to be explained. Are the two single sided arrows between A-E for the proposed model any different than the bidirectional arrows between A-O and O-E ? What is M(a) ? 

- Describing a general action using word “push” seems unnecessary and creates confusion. I would suggest just referring to them as actions.

- Section 4.B states that participants also provided answers such as “the robot failed to pick up the object”. How did the participants provide this answer if they were only provided with templates to fill in the blanks. 

- It seems that there are more data points for certain classes such as push, touch, move etc. than that for catch, nothing, flip etc. Does that impact training ? 


Grammatically incoherent sentences and typos:

- Section 1, Para 1, Line “Ideally, a robot’s set of symbols …. “ is incoherent
- Section 1, Para 2, Line “In contrast to prior affordance learning and… “ should be broken down in to simpler sentences. 
- Section 3.F, Para 1, Line “The labeled affordance... “ has two instances of the word “to”.
- Section 4.A, Para 1, Line “To build...” has period missing at the end. 


Hope you find these comments helpful.


[1] Paul Rohan, Jacob Arkin, Derya Aksaray, Nicholas Roy, and Thomas M. Howard. ""Efficient grounding of abstract spatial concepts for natural language interaction with robot platforms."" The International Journal of Robotics Research 37, no. 10 (2018): 1269-1299.
[2] Daniele Andrea F., Thomas M. Howard, and Matthew R. Walter. ""A Multiview Approach to Learning Articulated Motion Models."" In Robotics Research, pp. 371-386. Springer, Cham, 2020.",Agreement accepted,"The authors state that the main contribution of this paper id a data-driven method for grounding human-provided, natural language labels to robot manipulation actions and their effects by creating labeled affordance models. 

Some motivation is derived from the grounded semantics literature, particularly in human-robot interaction scenarios. I appreciate that embeddings won't necessarily be helpful in a grounded task like this because embeddings don't take the physical world into account--hence language grounding. I take issue with the claim that many works are ""top-down"" with this work being one among others that seeks to be more ""bottom-up"". The works cited did limit the tasks (and therefore vocabulary), but it is clear that this work had limited representation of possible affordances. Moreover, claiming that the ""natural language"" was somehow freer in this work than previous work, if I understand their method correctly, is not substantiated as the types of valid labels taken from the first study (i.e., local participants) was imposed on the crowd workers. Had the crowd-workers been given the freedom to come up with their own labels, the claim that this is ""bottom-up"" might be substantiated (it could be that the second question for the MTurkers did allow them to have freedom to write, but the word ""decisive"" led me to believe that it was in fact more constrained than the allowed canonical responses in the first question). 

The authors moreover make claims about what may or may not hold semantic meaning for humans (i.e., numbers). That leaves symbolic approaches, but symbolic representations alone contain no connotative meaning (hence the need for embeddings or grounded semantics) beyond what humans assign to them (i.e., Johhn Searle's Chinese Room). I think the authors wished to portray that readable string symbols have meaning to humans and they were attemping to learn mappings from affordance-required situations so the humans could read the symbols. That's the whole point of this paper: labels (i.e., human-interpretable labels); i.e., affordances mapped from action/object/effect to labels, but that's precisely what many grounded semantics approaches are, a mapping from some state of the world to a word label. The authors would do well to compare their approach with [2] which, though not done within a robot, the grounded model learns a mapping from low-level object features to word labels. Their binary SVM classifiers do something very similar. 

The authors did not cite [1] which, importantly, uses far less constrained interaction for learning affordances in an interactive setting. Though more of a work-in-progress, the interactive nature makes for less constrained labels (i.e., ""bottom-up""). 

I also have concerns about the probabilistic model. The function in (1) maps directly to a conditional probability. That is followed by an explanation that they wish to uncover affordances that are applicable ""across multiple objects"". The result is not a joint probability function as the authors claim, it's a marginal probability, marginalizing over the objects in equation (3):

f(o,a,e,l) = \sum P_{o in O}(l|a,e,O=o) 

If the authors simply omitted the objects in in the final model, then that needs clarification. Moreover, it's not clear how the variables are represented or what their range of values are. It seems that they are represented symbolically, but that's not clear (nor is it how recent grounded models work; i.e., mapping from symbols to symbols). 

Section 4.D explains further the approach in constructing ad-hoc conditional probabilities between labels. The reasoning is clear and the resulting relationships are useful (i.e., Figure 8), but this is essentially a simplification of Bayesian Network structure learning. 

The authors display correlations between labels in Figures 6 & 7, and AOC curves for labels trained on various features (Figure 9). My main concern with the paper is that the reported results don't substantiate the claims of the paper contributions. Figures 6 & 7 show how labels internally correlate with each other, but it's not clear how that supports that the labels are correct or useful to human users. A user study with the model in action (even pre-trained on the collected data) would have given more credence that the model does what the authors claim. 

I encourage the authors to continue down this line of work, but be more careful about comparing their work with the grounded language/semantics literature. I really like that they derive inspiration from that literature, but much more could potentially be gained (see [1,2] taken together, for example). 

[1] Hough, J., & Jamone, L. (2018). Predicting Object Affordances within a Continuous Dialogue State Update Process for Human-Robot-Interaction. Proceedings of the First International Workshop on Computational Models of Affordance in Robotics. Retrieved from https://afford.gitlab.io/rss-workshop/2018_submissions/papers/Hough_et_al_IWCMAR2018.pdf

[2]  Kennington, C., & Schlangen, D. (2015). Simple Learning and Compositional Application of Perceptually Grounded Word Meanings for Incremental Reference Resolution. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 292–301. Retrieved from http://www.aclweb.org/anthology/P15-1029",,"A major limitation of this study is that it explores such a narrow spectrum of affordance data.  This occurs because:

1) The objects used are simple and self-similar.  They are not complex enough to explore affordances such as ""open"", ""close"" or ""pour"".

2) The actions performed by the robot are very simple, consisting of just random linear movements.  This again leads to the explored space being highly limited, largely consisting of pushing objects around.  We are unable to see more interesting affordances develop.

3) As a result of the combination of (1) and (2) above, the labels that are obtained are again self-similar, resulting in actions like knock, push, touch, bump and move.  

The combined effect is that many of the arguments in the paper are not well supported.  For example, the central argument that collecting data from humans is necessary in order to capture a wide range of used terms is not as convincing as it could be since none of the terms are surprising.  If hand-coded labels were used in their place, for example, it doesn't seem like there would be much detriment to the current system. 

I do find the idea of auto-generated label hierarchies very interesting, and could see that becoming more critical in a more complex domain.  

In section III.B. the authors state that in most prior work affordance data is stored in a representation that is not directly accessible to humans. This is not a valid statement.  While true for a subset of works (mostly from the haptics community like [5] and [25]), many others use human-interpretable terms like ""pushable"" and ""pickupable"" directly.   Many of these papers are cited in section II.

A major result of the paper is that affordance labels are better predicted by effects of actions than by the action trajectories themselves.  This is not entirely surprising given the way affordances are described.  An affordance such as ""open"" or ""pickup"" directly describes the effect that it has on the object, regardless of how it was achieved.  An interesting follow-on study would be to ignore all trials in which no contact with the object was made, and focus more specifically on learning mappings from object properties to labels as a result of more complex object interactions, such as opening a box.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,,,,37.0,FTBDZNA2dY4
"Embodiment is an important characteristic for all intelligent agents (creatures and robots), while existing scene description tasks mainly focus on analyzing images passively and the semantic understanding of the scenario is separated from the interaction between the agent and the environment. In this work, we propose the \textit{Embodied Scene Description}, which exploits the embodiment ability of the agent to find an optimal viewpoint in its environment for scene description tasks. A learning framework with the paradigms of imitation learning and reinforcement learning is established to teach the intelligent agent to generate corresponding sensorimotor activities. The proposed framework is tested on both the AI2Thor dataset and a real world robotic platform demonstrating the effectiveness and extendability of the developed method.",RSS2020_Author_Agreement_signed.pdf,"Sinan Tan, [Huaping Liu](https://sites.google.com/site/thuliuhuaping)Di Guo, Xinyu Zhang, Fuchun Sun, ",Sinan Tan (Tsinghua University); Huaping Liu (Tsinghua University)*; Di Guo (Tsinghua University); Xinyu Zhang (Tsinghua University); Fuchun Sun (Tsinghua University),RSS2020_Author_Agreement_signed.pdf (159968 bytes); rss2020.pdf (6685092 bytes),rss2020.pdf,76.0,38.0,,2.0,Towards Embodied Scene Description,,0.20226647421983301,"The paper is overall clear, although it could be improved in the following aspects:

- The explanation on how the sequences are generated for the learning by imitation setting could be improved (in the text it shortly refers to the Floyd-Washall algorithm, but some intuition could be given);
-  Figure 3 is not informative and it takes a lot of space, on the other hand I found the explanation on how you simplify ResNet18 a little to verbose and difficult to follow, maybe a figure here could help;
- There is no reference (unless I have missed it) to the specific object detector used in the computation of the score (and on which dataset it was trained);
- There is no reference (unless I have missed it) to the stopping criteria for the best view point selection;
- I have found the use of the ""embodied"" terminology unnecessary, in the end it is just an active system with an active strategy for best viewpoint selection, I suggest you remove reference to ""embodiment"" (which needs to be explained) and just refer to ""active"" strategy.

Overall I think the paper makes an interesting scientific contribution, but its relevance for robotics should be better motivated.

There are some typos in the text, please read proof carefully.",Agreement accepted,"Positive:
This paper is well written and easy to follow. The paper addresses an interesting new problem by a thoughtful way. The learning strategy using IL followed by RL is technically sound. While I believe an agent or a person has intrinsic motivation or follows extrinsic commands (find objects or go to some location etc.) ultimately rather than to go to the best viewpoint, I believe the problem addressed in the paper is very important. 

Concerns:
Although the paper provides ablative studies, which is useful, comparisons and evaluation are rather sparse. It would be nice to compare the proposed method (using the final view found) against previous scene description techniques (perhaps using all the images and taking the best). It would also be nice to have human evaluation and evaluate how much the proposed method is close to humans. Also, it would be nice to compare against next-best-view techniques (like [1]) designed for scene (place or room) classification (I am not sure if this kind of methods exist previously). 

I believe it would be better to use a “global” score that incorporates how much the current view represents the space or “room” agent resides, instead of measuring the similarity between the object detection results and captioning results plus the number of objects. A global score should probably be calculated from all the view images of the room. Or by assuming that we know which room type the agent is now in, then the similarity of the objects found and the room type could be coumputed and aggregated. 

Comments:
The lines of works in target-driven navigation based on semantics [2-4] should be incorporated. In particular, [4] also uses a semantic similarity measure based on cosine similarity. 

[1] Pairwise Decomposition of Image Sequences for Active Multi-View Recognition
[2] W. Yang, X. Wang, A. Farhadi, A. Gupta, and R. Mottaghi, “Visual semantic navigation using scene priors,” in ICLR, 2019
[3] M. Wortsman, K. Ehsani, M. Rastegari, A. Farhadi, and R. Mottaghi, “Learning to Learn How to Learn: Self-Adaptive Visual Navigation Using Meta-Learning,” in CVPR, 2019
[4] R. Druon, Y. Yoshiyasu, A. Kanezaki and A. Watt, ""Visual Object Search by Learning Spatial Context,"" in IEEE Robotics and Automation Letters, 2020.

Minor:
“Ref.” is a not a usually way to refer papers. Please change to some other ways for citing such as…et al.  
",,"Strengths:
The paper proposes a novel problem with a solution, which can find application in mobile robots. 

The figures are very illustrative. 

Doubts:
Scene segmentation is encoded as (probably pre-defined) RGB colors rather than one-hot embedding in the framework. Does this make sense? 

“There are 4 kinds of residue building blocks in the original ResNet18 ...” This may lead to confusion, ResNet18 has 4 layers with the same “residual” building block.

According to the demonstrations, the scene is discretized as a grid map. This is not consistent with the action space description. The robot will need to travel (sqrt(2) * delta_m) length diagonally. 

Which RL algorithm is used in this paper? This is important.

Which “off-the-shelf” object detector is used to score the scenes?


What captioning models are used in experiments?

To achieve the proposed problem, a simple way would be traveling along the room perimeter and looking at the room center. Consider that as a baseline, what would its performance be?

The “good viewpoint” might not exist in some scenarios. How do you handle that?

All in all, I like the contributions in this paper, as the task and solution are very practical. The authors obviously used many existing tools (semantic segmentation, object detection, scene description). However, many are not specified in the paper. 

Typo(s):
The key to fine-tune the model -> The key to fine-tuning the model

Using the REINFORCE algorithm -> using reinforcement learning",Agreement accepted,"The paper targets the task of embodied scene description (ESD). The proposed method seems reasonable and the provided ablation study justifies design choices of the proposed approach. However, the task of ESD is closely related to the task of Embodied Question Answering (EQA)  [9,10] and the work on video description, hence, important baselines are missing. 

(a) ESD can be seen as a special case of EQA, where the agent is posed a question such as ""What is the scene description?"". While [9,10] output a discrete set of answers, modifying the loss of such methods to generate scene captions does not appear as a major difficulty. Comparison to EQA-based baselines is needed to justify advantages of the proposed approach.

(b) Video description is an active area where recent methods, e.g.,  [A,B,C] use attention to select frames and parts of the image for caption generation. Application of such methods to a randomly generated path in this work (Random baseline in Table I) would be a natural baseline.

(c) Random selection of actions for the Random method in Table I appears as a too naive baseline. One can think of other simple strategies such as going to the center of the room and turning around. Such default blind strategies could be automatically derived from demonstrations used for IL in this work. Without experimental evidence, I am not convinced such simple baselines will be inferior to the proposed method.

In summary the paper proposes a new task together with a new method, however, important baselines are missing, hence more work is needed to justify the proposed approach.

[A] Hori C, Hori T, Lee TY, Zhang Z, Harsham B, Hershey JR, Marks TK, Sumi K. Attention-based multimodal fusion for video description. InProceedings of the IEEE international conference on computer vision 2017 (pp. 4193-4202).

[B] Tu Y, Zhang X, Liu B, Yan C. Video description with spatial-temporal attention. InProceedings of the 25th ACM international conference on Multimedia 2017 Oct 19 (pp. 1014-1022).

[C] Yan C, Tu Y, Wang X, Zhang Y, Hao X, Zhang Y, Dai Q. STAT: spatial-temporal attention mechanism for video captioning. IEEE transactions on multimedia. 2019 Jun 24.",Agreement accepted,,07/15 15:00,07/15 17:00,,https://youtu.be/KEeUmyhOL2o,,,38.0,zWCrgM7VWkc
"Autonomous driving has achieved significant progress in recent years, but autonomous cars are still unable to tackle high-risk situations where a potential accident is likely. In such near-accident scenarios, even a minor change in the vehicle's actions may result in drastically different consequences. To avoid unsafe actions in near-accident scenarios, we need to fully explore the environment. However, reinforcement learning (RL) and imitation learning (IL), two widely-used policy learning methods, cannot model rapid phase transitions and are not scalable to fully cover all the states. To address driving in near-accident scenarios, we propose a hierarchical reinforcement and imitation learning (H-ReIL) approach that consists of low-level policies learned by IL for discrete driving modes, and a high-level policy learned by RL that switches between different driving modes. Our approach exploits the advantages of both IL and RL by integrating them into a unified learning framework. Experimental results and user studies suggest our approach can achieve higher efficiency and safety compared to other methods. Analyses of the policies demonstrate our high-level policy appropriately switches between different low-level policies in near-accident driving situations.",RSS2020_Author_Agreement-1.pdf,"[Zhangjie Cao](https://caozhangjie.github.io/), [Erdem Biyik](http://stanford.edu/~ebiyik/), [Woodrow Z. Wang](https://www.linkedin.com/in/woodrow-wang-214043150/), [Allan Raventos](https://www.linkedin.com/in/allan-ravent%C3%B3s-19b962138/), [Adrien Gaidon](https://www.linkedin.com/in/adrien-gaidon-63ab2358/), [Guy Rosman](http://people.csail.mit.edu/rosman/), [Dorsa Sadigh](https://dorsa.fyi/)",Zhangjie Cao (Stanford University); Erdem Biyik (Stanford University)*; Woodrow Wang (Stanford University); Allan Raventos (Toyota Research Institute); Adrien Gaidon (Toyota Research Institute); Guy Rosman (Toyota Research Institute); Dorsa Sadigh (Stanford),hreil.pdf (5432177 bytes); RSS2020_Author_Agreement-1.pdf (149948 bytes),hreil.pdf,1196.0,39.0,,2.0,Reinforcement Learning based Control of Imitative Policies for Near-Accident Driving,,0.022594059127502005,"The paper is very clearly written, and the user studies show significant preference for the proposed model. My main concern with the applicability of the general framework (which in the main body of the paper includes an ensemble of n low-level policies) in the driving domain is that only the case n=2 is actually evaluated experimentally in this paper, where the two driving modes are safe vs efficient. This limits the scope of the paper. What's the main challenge behind having more policies available in the ensemble and showing how a switching master policy performs?

I would have liked to see references to some of the following works, if they are not cited already:
https://arxiv.org/abs/1905.10681
https://dl.acm.org/doi/10.5555/1838206.1838300
https://arxiv.org/abs/1908.00722
https://arxiv.org/abs/1812.00025
https://arxiv.org/abs/1809.06305
https://arxiv.org/abs/1910.11956

I would also have liked to see references to older work on reinforcement learning with Dynamic Movement Primitives, which is conceptually relevant to this approach. E.g. https://ieeexplore.ieee.org/document/6100841
 
",Agreement accepted,"This paper is well presented. The proposed hierarchical approach that combines RL and IL provides a valid way to learn policies for continuous systems. The policies learned by low level IL functions like basis functions, which provides a different kind of discretization for the RL problem (conventional discretization usually discretizes the state or control space). This approach is novel. The authors are suggested to address the following issues in future revisions.

#1
The proposed H-ReIL requires that all demonstrated trajectories have labels. However, classified demonstration is difficult to obtain in the real world. The authors are suggested to discuss impacts on the method if some demonstration comes from a multi-model distribution and is not classified.

#2
The “basis policies” obtained from IL may not fully span the solution space for RL. For example, suppose in the demonstration, there is only aggressive and very aggressive case. Then there is no hope to learn timid behavior. The authors are suggested to provide some guideline on how to ensure or determine that the solution space for RL is fully covered in IL.",Agreement accepted,"The approach in the paper is a simple combination of reinforcement learning and imitation learning. In my taste, it is too simple to be interesting. Using macro-actions in RL is a well known approach. The only new contribution that the authors advocate is that the macro-actions are learned by imitation learning. However, I do not think this is enough of a contribution as this is trivial to do. Moreover, most of the expeirments would  not even need to use imitation learning´as they rely on a hand-coded controller. Please see more comments below:
- The experiments are very simple where only 2 hand-coded base behaviors are used. The approach would get more interesting if we have more base behaviors and the data would not already be clustered into these behaviors. 
- There is no comparison to plain RL and variants of RL that would fit the given setup well. For example, Residual Reinforcement Learning could be used in combination with the given hand-coded controllers. 
- It is hard to estimate how far away the learned controller is from the ""optimal controller"" that would be learned using plain RL. I would agree that plain RL requires many more training samples, but the quality of the learned policy should be better as we do not have to rely on hand-coded controllers.
- The theoretical results such as theorem 1 are trivial. I am not sure why this needs to be written explicitly in a theorem. 
- More experiments with real human data is needed. The experiments with the hand-coded controllers poorly motivate the algorithm. Why do we need imitation learning if we have a hand-coded controller?
  ",Agreement accepted,,,,07/15 15:00,07/15 17:00,https://github.com/Stanford-ILIAD/CARLO,https://youtu.be/CY24zlC_HdI,,,39.0,6iEi4PDLQ8w
"Performing acrobatic maneuvers with quadrotors is extremely challenging. Acrobatic flight requires high thrust and extreme angular accelerations that push the platform to its physical limits. Professional drone pilots often measure their level of mastery by flying such maneuvers in competitions. In this paper, we propose to learn a sensorimotor policy that enables an autonomous quadrotor to fly extreme acrobatic maneuvers with only onboard sensing and computation. We train the policy entirely in simulation by leveraging demonstrations from an optimal controller that has access to privileged information. We use appropriate abstractions of the visual input to enable transfer to a real quadrotor. We show that the resulting policy can be directly deployed in the physical world without any fine-tuning on real data. Our methodology has several favorable properties: it does not require a human expert to provide demonstrations, it cannot harm the physical system during training, and it can be used to learn maneuvers that are challenging even for the best human pilots. Our approach enables a physical quadrotor to fly maneuvers such as the Power Loop, the Barrel Roll, and the Matty Flip, during which it incurs accelerations of up to 3g. ",RSS2020_Author_Agreement_signed.pdf,"[EliaKaufmann](https://kelia.github.io/), [Antonio Loquercio](https://antonilo.github.io/), [Rene Ranftl](http://), [Matthias Müller](https://matthias.pw/), [Vladlen Koltun](http://vladlen.info/), [Davide Scaramuzza](http://rpg.ifi.uzh.ch/people_scaramuzza.html)","Elia Kaufmann (ETH / University of Zurich)*; Antonio Loquercio (ETH / University of Zurich); Rene Ranftl (Intel Labs); Matthias Müller (Intel Labs); Vladlen Koltun (Intel Labs); Davide Scaramuzza (University of Zurich & ETH Zurich, Switzerland)",RSS2020_Kaufmann.pdf (7362891 bytes); RSS2020_Author_Agreement_signed.pdf (274612 bytes),RSS2020_Kaufmann.pdf,7.0,40.0,,2.0,Deep Drone Acrobatics,,0.96658280388486,"This paper proposes a complete learning system which allows a drone to fly acrobatic maneuvers. It learns a sensorimotor agent purely in simulation and performs zero-shot sim-to-real transfer. Impressive results are demonstrated on a real drone. 

This paper is very well written. I was able to fully understand the paper without any difficulties. The paper has a clear technical contribution: preprocessing the raw observations to a feature representation that has smaller sim-to-real gap. This is an important contribution because sim-to-real gap is a universal problem for almost all robotic applications, and is a major obstacle that prevents the use of simulation. This paper presents an effective method, and a different perspective to bridge the gap. The paper showed successful deployment of simulation policy on a real drone and conducted thorough analysis to show the effectiveness of the proposed approach. 

The only thing that I am not confident about is the difficulty of the problem: acrobatic maneuvers for drones, because this is not my immediate research area. If the problem is indeed hard, and this is the first demonstration of such maneuvers, I would vote for acceptance of the paper.",Agreement accepted,"The authors propose a deep learning-based approach to training acrobatic motor skills of drones. The key idea is to apply the imitation learning technique (more specifically, DAGGER) to the reference motion while abstracting the visual inputs with feature extractors. The feature extractor computes the motion of salient keypoint in the visual inputs, which would provide better state estimation to the robot. Then all the features (visual inputs, IMU, reference trajectories) are asynchronously fed to the policy network. The reference trajectory is obtained by training a “privileged expert”, which has access to all the ground-truth state information, using MPC. The framework trains the agent solely in the simulation (Gazebo) and transfers to the real world. The authors demonstrate a few agile skills on real drones (AscTec Hummingbird), including a Barrel Roll, a Power Loop, a Matty-Loop.

I am personally impressed by the results presented by this paper: it seems to be pretty agile, without suffering from the sim-to-real transfer. I’m not too familiar with the state-of-the-art in acrobatic motions of drones, but it seems to be a great contribution to have the first acrobatic motion without additional sensory inputs in the robotic community. In this sense, I’m pretty positive about this submission.

On the other hand, the main contribution of the work is the sim-to-real transfer technique using visual points, as claimed by the authors. But their feature tracker (FT) does not improve the results (tracking errors or success rates) in Table I: the drone achieves near-perfect success rates for all four tasks (btw, it seems too obvious that it cannot perform good motor skills without reference motions or IMU). Now Figure 4 provides a bit contradictory results, which are only analyzed in simulation with a specific training/testing setting. Therefore, it is not very clear whether the proposed key contribution, a sim-to-real technique, is crucial in the proposed work.

And the architecture of asynchronous policy networks seems to be standard. It extracts features from raw inputs, concatenates them into history, applies convolutional operations, and generates outputs with feedforward networks. It might not be a very significant contribution to the community.

The paper itself is very well written and reads smoothly.

I would suggest the authors adding the following reference to the paper, which also discusses the sim-to-real of aerial vehicles.
Xu, J., Du, T., Foshey, M., Li, B., Zhu, B., Schulz, A. and Matusik, W., 2019. Learning to fly: computational controller design for hybrid UAVs with reinforcement learning. ACM Transactions on Graphics (TOG), 38(4), pp.1-12.
",Agreement accepted,"The paper demonstrates some very challenging acrobatics maneuvers of drones. I think the researchers working on drone control will find useful insights from the paper. The paper is mostly well-written, and the results and comparison are compelling enough to me. 

The key idea is to use an optimal control that has access to full information to generate expert demonstrations, and to train the control policy to learn those demonstrations with abstract state representations. Frankly, I do not think this is quite a novel idea. Similar approaches has been used in other domains, such as character control. 

I find the theoretical analysis that the abstract representation is critical to the sim-to-real transfer is particularly interesting. However, I am not fully convinced by Sec IV.C. The paper assumes that the mapping f in equation (11) exists, which is not quite obvious to me. I think it also need to be justified that the abstract representation used in the training meets the requirement of (11). 
",,,,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/uzh-rpg/deep_drone_acrobatics,https://youtu.be/2N_wKXQ6MXA,,,40.0,VTojN06OR7g
"Designing reward functions is a challenging problem in AI and robotics. Humans usually have a difficult time directly specifying all the desirable behaviors that a robot needs to optimize. One common approach is to learn reward functions from collected expert demonstrations. However, learning reward functions from demonstrations introduces many challenges: some methods require highly structured models, e.g. reward functions that are linear in some predefined set of features, while others adopt less structured reward functions that on the other hand require tremendous amount of data. In addition, humans tend to have a difficult time providing demonstrations on robots with high degrees of freedom, or even quantifying reward values for given demonstrations. To address these challenges, we present a preference-based learning approach, where as an alternative, the human feedback is only in the form of comparisons between trajectories. Furthermore, we do not assume highly constrained structures on the reward function. Instead,  we model the reward function using a Gaussian Process (GP) and propose a mathematical formulation to actively find a GP using only human preferences. Our approach enables us to tackle both inflexibility and data-inefficiency problems within a preference-based learning framework. Our results in simulations and a user study suggest that our approach can efficiently learn expressive reward functions for robotics tasks. ",RSS2020_Author_Agreement-2.pdf,"[Erdem Biyik](http://stanford.edu/~ebiyik/), [Nicolas Huynh](https://www.linkedin.com/in/nicolas-h-24816b18b), [Mykel J. Kochenderfer](https://mykel.kochenderfer.com/), [Dorsa Sadigh](https://dorsa.fyi/)",Erdem Biyik (Stanford University)*; Nicolas Huynh (École Polytechnique); Mykel Kochenderfer (Stanford University); Dorsa Sadigh (Stanford),active_gp.pdf (5287136 bytes); RSS2020_Author_Agreement-2.pdf (145944 bytes),active_gp.pdf,1298.0,41.0,,2.0,Active Preference-Based Gaussian Process Regression for Reward Learning,,0.583988976156548,"Summary:
The work proposes a method to use Gaussian Processes (GPs) to learn reward functions with limited data. Prior work has learned reward functions with pairwise comparisons, but they often assume a specific structure on the reward function (e.g., linear model). More complex models have also been used, but they require lots of data. The authors propose to use GPs to learn nonlinear reward functions without needing as much data as a neural network. Experiments are performed on simulated tasks (Driving and Tosser) as well as on a mini golf robot task. Results show that their active GP method is more data efficient than random querying and performs better than a prior active linear method when the true reward function is polynomial.

Originality:
There is a lot of work on reward function learning and several that use pairwise comparisons. This work does extend prior work on reward function learning restricted to linear functions, but I’m not sure if the extensions are sufficiently different and novel from all of the work that exists in this space.

Clarity:
The paper is well-written. The figures are clean and well-done. Figure 4 is a nice addition to better understand the evaluation set that is used through Poisson disk sampling. Figure 2 can probably be skipped to get extra space, given that the domains are pictured in later figures.

Quality:
The paper is of high quality. Specifically, it was good to see simulation experiments across two domains and user studies on another third domain.

Significance:
The area of reward function learning from human data is very valuable for the community. However, the area is quite crowded and it’s unclear how significant this particular extension could be for the field. Further, the method requires defined features, even if it relaxes the need for a linear function. It would be important to understand how the method can be combined with approaches that do not require defined features (as is briefly hinted in the conclusion).

Comments:
- When the function mapping the trajectory to features is unknown, how would the approach apply?
- Could the authors include a citation to the “commonly used probit model”? 
- I didn’t fully understand the following sentence: “minimizing the second entropy term...encourages the ease of responding to the queries by the user meaning the user should be certain about their choices.” Could you add in an extra sentence briefly explaining this more?
- GP does worse for linear models. Is it possible to combine the benefits of both by using active-linear when the reward function is predicted to be linear and GP otherwise?
- Why does active-linear have such wide error bars for the Tosser domain?
- Five queries is a small test set in the user study domain. Is there a reason why the authors decided to have such a small test set?
- In Fig 7a, it doesn’t look like the prediction accuracy is very different between the 3 conditions. What are the numbers on these results?

Overall, the paper is well-written and clear and could be useful to the community for extending literature on reward function learning. This is a large, crowded space though, so it would be important for the authors to show that this particular work is novel enough to be its own contribution.",Agreement accepted,"The paper is quite clear and organized. I do not think the work is extremely original, particularly because of [17], but the authors do show that the method improves upon [17] in fundamental ways.
However, I believe the paper provides a significant contribution to the field, in particular, due to the explicit derivations and arrangement of ideas, which I think makes it a good candidate for someone trying to implement a preference-based method to learn rewards.

A few questions that were not clear to me follow below.

I was surprised by how much more efficient the linear model is under the linear reward. The GPs seem to have a quite slow convergence in comparison. Surely, learning the true nonlinear reward is where the GP shines, but it makes me wonder if it would be possible to use both models or bootstrapping the GPs with the linear model.

Not sure this sentence is fully understood unless you provide some equation on the nonlinear reward or a bit more detail.
""On the other hand, if the reward model was nonlinear, one can capture all possible configurations with only two features: speed for how far the ball will be thrown, and angle for which direction to shoot""

What are the trajectory features in practice? Can you provide the concrete features you used in each experiment?

Regarding the experiment with the Fetch under the ActiveLinear, does the robot tend to throw the ball outside the boundaries as shown in the video? I wonder if that is the reason this method got such a bad score in terms of user rating (Figure 7(b)), which is even worse than the randomGP.

How did you decide to stop at 15 queries in the Fetch experiment? From the Tosser and Driver experiments, it looks the nonlinear method would require a bit more queries than just 15, but that is just my intuition. Do you think if you continue the queries for longer (say 50) the prediction accuracy would increase?

The video could be improved by showing the human preferred intended target. 

[Minor comment: relation with Interactive/Iterative Learning]
Although the paper frames the problem of learning a reward, other works optimize control actions directly and because of that, they seem to require fewer user queries. I do not think they are direct contenders of the method here proposed as they lack the reward learning part. Nevertheless, from the perspective that the final goal is to generate the action that satisfies the human hidden preference they do seem similar. I think it is worth considering the relation of the proposed method with methods such as [1][2] to position your work within a broader field of interactive methods in robotics.

[1] A. Jain, B. Wojcik, T. Joachims, and A. Saxena, “Learning trajectory preferences for manipulators via iterative improvement,” in Advances in neural information processing systems, 2013, pp. 575–583.
[2] C. Celemin, G. Maeda, J. Ruiz-del-Solar, J. Peters, and J. Kober, “Reinforcement Learning of Motor Skills using Policy Search and Human Corrective Advice,” International Journal of Robotics Research (IJRR), vol. 38, no. 14, pp. 1560--1580, 2019.",Agreement accepted,"== Strengths ==
The paper is very well-written in terms of both organization and flow. The key problems addressed are important and relevant for the RSS and robotics community. The method was verified with interesting experiments (e.g., mini-golf) in both simulation and a real-world setup with human-subjects. The presented analyses well-describe the method's performance and demonstrates it works in practice. 

== Areas for Improvement ==
On the downside, the paper has novelty and technical issues that I hope the authors can address: 

*Novelty*: Although I agree that classical IRL methods used linear reward functions with hand-crafted features, modern IRL methods (e.g., Guided Cost Learning [1], Adversarial IRL [2]) learn non-linear reward functions. Using GPs to model reward functions has been proposed before (GP-IRL [3]), which this paper doesn't cite nor discuss. Moreover, preference-based GP learning is not new (see [4,5]). 

I believe there are novel ideas in this paper, but they should be discussed in relation to this body of prior work and the literature on GPs for other active learning tasks. To my knowledge, these prior studies do not perform active reward learning. Nevertheless, non-linear reward function representation with GPs has been proposed before. 

*Kernel Function*: It's unclear that the proposed kernel is PSD; it is a difference between two kernels, and its PSD-ness is not immediately obvious from the standard closure properties. I would suggest including a statement about PSD in the main text and a proof in the supplementary material. Of lesser importance, the proposed kernel is not exactly the RBF kernel and I would suggest calling it something else. 

*Experiments*: I enjoyed reading the experiments, but the active-linear model is a very weak baseline since the ground-truth reward is inherently non-linear. Potentially, the approach could be compared to other modern IRL approaches (esp. in the simulations), or to Active-linear with a polynomial feature expansion.  

Overall, the paper proposes some interesting new ideas and great experiments, but I'm concerned about correctness (PSD) and relation to prior work on using GPs for IRL and preference learning. 

== References ==
[1] Finn, Chelsea, Sergey Levine, and Pieter Abbeel. ""Guided cost learning: Deep inverse optimal control via policy optimization."" International conference on machine learning. 2016.
[2] Fu, Justin, Katie Luo, and Sergey Levine. ""Learning robust rewards with adversarial inverse reinforcement learning."" arXiv preprint arXiv:1710.11248 (2017).
[3] Levine, Sergey, Zoran Popovic, and Vladlen Koltun. ""Nonlinear inverse reinforcement learning with gaussian processes."" Advances in Neural Information Processing Systems. 2011.
[4] Chu, Wei, and Zoubin Ghahramani. ""Preference learning with Gaussian processes."" Proceedings of the 22nd international conference on Machine learning. 2005.
[5] Houlsby, Neil, et al. ""Collaborative Gaussian processes for preference learning."" Advances in neural information processing systems. 2012.
",Agreement accepted,,,,07/15 15:00,07/15 17:00,https://github.com/Stanford-ILIAD/active-preference-based-gpr,https://youtu.be/SLSO2lBj9Mw,,,41.0,Vri-AxFMl10
"We consider shared workspace scenarios with humans and robots acting to achieve independent goals, termed as parallel play. We model these as general-sum games and construct a framework that utilizes the Nash equilibrium solution concept to consider the interactive effect of both agents while planning. We find multiple Pareto-optimal equilibria in these tasks. We hypothesize that people act by choosing an equilibrium based on social norms and their personalities. To enable coordination, we infer the equilibrium online using a probabilistic model that includes these two factors and use it to select the robot's action. We apply our approach to a close-proximity pick-and-place task involving a robot and a simulated human with three potential behaviors - defensive, selfish, and norm-following. We showed that using a Bayesian approach to infer the equilibrium enables the robot to complete the task with less than half the number of collisions while also reducing the task execution time as compared to the best baseline. We also performed a study with human participants interacting either with other humans or with different robot agents and observed that our proposed approach performs similar to human-human parallel play interactions. The code is available at https://github.com/shray/bayes-nash.",RSS2020_Author_Agreement.pdf,"[Shray Bansal](https://shraybansal.com), [Jin Xu](http://), [Ayanna Howard](https://howard.ece.gatech.edu/), [Charles Isbell](https://www.cc.gatech.edu/fac/Charles.Isbell/)",Shray Bansal (Georgia Institute of Technology)*; Jin Xu (Georgia Institute of Technology); Ayana Howard (); Charles Isbell (Georgia Institute of Technology),RSS2020_Author_Agreement.pdf (151517 bytes); Nash_Equilibrium_for_Parallel_Play___RSS_2020 (8).pdf (1600490 bytes),Nash_Equilibrium_for_Parallel_Play___RSS_2020 (8).pdf,1341.0,42.0,,2.0,A Bayesian Framework for Nash Equilibrium Inference in Human-Robot Parallel Play,https://shraybansal.com/research/bayesnash,0.481268825516829,"In this submission, the authors present a novel model for human-robot task planning that casts the problem as a general-sum game in which multiple Nash equilibria are weighted against each other using a Bayesian formulation. The formulation includes an expert-crafted and domain-specific social norm and a agent-specific individual preference that is inferred online. 

The authors also present a set of three related studies in which the approach is evaluated against baselines. This includes a simulated human study, a human-human study, and a human-robot study. Through the studies, the authors show that the developed approach leads to a good balance of safety and efficiency, reducing the number of safety stops while also lowering the time to complete the task. While this was true in the simulated human study, the developed approach actually led to more safety stops in the human-robot study. The authors present plausible reasons for this results and make suggestions for follow-on work.

The paper is placed well within context of prior work, and presents the method in a clear and concise manner. The approach appears novel and is technically sound. The ability to infer agent preferences online and leverage domain-specific norms to select from multiple equilibria is an interesting and useful idea. The experiments and analysis are interesting and informative.

The main drawback I see is that while the presented approach is designed for any N number of agents collaborating together, all of the analysis is done with just two agents. It is difficult to determine whether the conclusions and claims made by the authors about the approach would actually carry to a larger multi-agent scenario. Would the robot be able to infer the preferences of multiple people successfully and maintain safety and efficiency? Would the approach be computationally tractable in that case? (A discussion of computational complexity and real-time performance is needed).

The next drawback is that the analysis/explanation of the studies is a bit lacking. There is no statistical analysis of the simulated studies (so it’s hard to tell what differences are actually significant) and the descriptions are incomplete. For example, what are the error bars in the figures? (standard deviation, SEM, etc.). Also, how are some of the parameters set, and what were the selected values? (e.g., tau in Eq 3 and 6). These are critical for reproducibility. 

Finally, the paper has a few typos/grammar issues:
- Section V.B. cuts off mid-sentence
- Section VI. “Metrics” starts off with a run-on sentence
- Section VIII.A. there is a reference to Figure 8, that is actually referring to Figure 6.",Agreement accepted,"*****
Strengths and weaknesses:

The paper considers a relevant and topical problem, which is of interest to the conference audience. As stated in the summary of contributions, the authors identify that very few formalisms exist that utilize game theory for HRI and provide a novel approach towards this gap. The research plan is well designed: borrowing insights from human interactions (via human-human studies), designing algorithms building upon prior formalism (i.e., game theory and Bayesian inference), and evaluating them with humans. The paper is overall well written, with a good coverage of related work, description of the approach, and results.

The key weakness is the evaluation of the proposed approach, where the evaluations are carried out with small sample size, without baselines from the relevant prior art, and (although less importantly) only in simulation. The small sample size makes assessing the generalizability of observed trends difficult. Further, over the past few years, several approaches have been developed for generating robot behavior in shared workspace tasks. In the absence of evaluations against a representative baseline (see detailed comments for suggestions), it is difficult to assess the utility of game-theoretic formalisms in general and the proposed approach in particular. 

Please see the detailed comments and suggestions listed below.

*****
Comments and suggestions:

1) (Abstract) The abstract states that the proposed approach outperforms the best baseline. This statement should be better qualified as this is observed only in agent-agent studies, with key differences in human-agent studies. Further, no comparisons are made to baselines from the prior art.

2) (Related Work) Despite the presence of several HRI formalisms, the authors provide good coverage of related work. However, space permitting, a few highly related papers would be useful to add to this discussion:

2.1) (Game-theoretic approaches) The following paper, which formalizes HRI problem using game theory, is highly related and currently missing from the discussion: Nikolaidis, Stefanos, et al. ""Game-theoretic modeling of human adaptation in human-robot collaboration."" Proceedings of the 2017 ACM/IEEE international conference on human-robot interaction. 2017.

2.2) (Theory of mind-based approaches) While the theory of mind- based approaches do not explicitly compute or reason about equilibria, they reason about the influence of human on a robot and vice-versa. For instance, please see: Devin, Sandra, and Rachid Alami. ""An implemented theory of mind to improve human-robot shared plans execution."" 2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 2016.

3) (Results of Human-Agent Study, Section 8) The experiments, despite their small sample size, are well designed. However, the results of the human-agent study and its difference with the agent-agent study (Section 6), also question several assumptions of a formalism based on game-theoretic equilibrium. For instance,
- Do human-robot interactions necessarily follow an equilibrium, especially given that both the human and robot can adapt?
- One interpretation presented in Section 9 highlights that humans indeed adapt and modify their policy in response to that of the robot. This observation raises the question, ""Can the proposed approach identify if a stable equilibrium has been reached and, if so, correctly estimate its value?"" The results indicate otherwise.
As noted above, the observed results are informative for the design of HRI algorithms as well as to understand the utility of game-theoretic formalisms for computing robot policies. Consequently, it will be useful to include additional discussion, which addresses the above questions.

4) (Relation to decision-theoretic approaches) Several decision-theoretic approaches have been developed and demonstrated to perform effectively in shared workspace tasks (for instance, see list below). Similar to the game-theoretic approach proposed in the submission, these approaches maintain an estimate of the human's latent state (either preference or goal) and arrive at robot policy. However, they do not require the presence of equilibrium and can tackle larger problem spaces (e.g., continuous spaces in the case of Javdani et al.) as compared to the proposed approach. Further, they can algorithmically generate spatio-temporal behavior that is typical of human interaction (e.g., wait and then go), which is absent in the implementation of the Bayes-Nash approach.

To demonstrate the utility of the proposed approach, consider including a comparison to one representative decision-theoretic approach from the following,
- Chen, Min, et al. ""Planning with trust for human-robot collaboration."" Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction. 2018.
- Unhelkar, Vaibhav V., et al. ""Human-aware robotic assistant for collaborative assembly: Integrating human motion prediction with planning in time."" IEEE Robotics and Automation Letters 3.3 (2018): 2394-2401.
- Javdani, Shervin, et al. ""Shared autonomy via hindsight optimization for teleoperation and teaming."" The International Journal of Robotics Research 37.7 (2018): 717-742.
- Cheng, Yujiao, et al. ""Towards Efficient Human-Robot Collaboration With Robust Plan Recognition and Trajectory Prediction."" IEEE Robotics and Automation Letters 5.2 (2020): 2602-2609.

5) (Section 3) Typically, the action refers to an atomic action, which is chosen and then executed without modification. However, in the current formalism, action corresponds to RRT plans (which can be changed mid-execution). Please clarify if this understanding is correct. If so, consider including a footnote mentioning that actions can be modified mid-execution.

6) (Equation 6) Does the equation only apply to timestep 0? The description following the equation ""Comparing the distance ... equilibrium performance."" was difficult to follow. Please consider rephrasing this description.

7) (Section 4, Clarification question) Does the formalism assume equilibrium is achieved and remains constant over the task execution, and only the belief over the equilibria changes? Or, does it also apply to cases in which the equilibrium has not been achieved (and is changing during the interaction)?

*****
Minor comments on the clarity of the presentation:

The submission is overall well written and easy to follow. Minor suggestions and typos are listed as follows:
- (Abstract) Bayesian should be capitalized.
- (Introduction) Consider including a reference for the term `parallel play` from psychology literature.
- (Section 3) The phrase 'set of all actions' is ambiguous, as it refers both to a (the action profile) and A (the set of joint actions).
- (Section 4) ""we take its joint"" is informal. Please change to ""we take its joint distribution.""
- (Section 5A) In the current formalism, does N correspond to 2? If the approach is indeed general and extends to beyond 2 agents, consider mentioning it explicitly in the text.
- (Figure 4) The phrase Bayes-Nash has not been defined in the text. Please note that the proposed approach is referred to as Bayes-Nash. 
- (Figure 5) Similarly, the phrase Fair-Nash has not been defined in the text. I assume that it refers to the baseline ""Selfish-Nash.""
- (Section 6) Both the phrases Bayes-Nash and Nash-Bayes are used in the paper. Consider using only one to maintain uniformity.
- (Section 6) Typo: ' We measured measured...''
- (Figure 5b) In the description of Figure 5b, it is ambiguous which human is replaced (the control, the participants, or both). Please consider rephrasing this description.

*****",Agreement accepted,"Originality: The paper presents a novel solution to interaction in a shared space. The solution was interesting as were the results. Especially novel was using both norms and personality types to select from multiple equilibria.

Quality: The modeling efforts were justified and well-reasoned. The norms and personality types were grounded in well-recognized approaches in game theory (e.g., minimax, fairness).  The planning algorithm was appropriate for the problem. 

There are a few ways that the paper could have been improved:
(1) The game theory model was based in single-stage games. The problem, however, seemed more aligned with games where human and robot would interact repeatedly. The paper should mention that in repeated play there are many more equilibria (from the folk theorem in repeated play games), including equilibria where the two agents take turns receiving their most preferred outcome. The paper should justify why only equilibria for single stage games were considered.
(2) The three types of studies provided evidence that the solution approach has merit. However, as identified by the authors, the human-human and human-robot studies had too few participants to allow any statistical conclusions. That is unfortunate because it decrease confidence in the conclusions.
(3) I wasn't sure about some of the aspects of the study with real humans. Specifically, was the order with which the humans interacted with the strategies counterbalanced? If not, it is impossible to know whether the trends in the data are simply from a learning effect.

Clarity: The paper is really well written and includes an excellent review of the literature. Assumptions were clear, modeling choices were clear, and limitations were clear.

Significance: The paper makes a solid contribution to human-robot interaction, expanding the state-of-the-art.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,,,,42.0,y9Pu9c1mcyE
"Flapping wing aerial vehicles rely heavily on accurate models for a variety of different tasks. There have been significant efforts in creating both analytical and data-driven models for many of these types of vehicles including ornithopters and small aerial vehicles mimicking insects. However, very few works have explored modeling for aerial vehicles with a skeletal structure throughout the wings and a single flexible membrane that covers the wings and tail such as is found in robots with bat morphology. In this paper, we build upon previous efforts to model a bat robot using a combination of first-principles and data-driven tools. We record a series of load cell tests and free-flight experiments, and we optimize the model parameters to improve long-term flight prediction. We introduce several extra terms in the model including a term explaining the coupling between wings and tail in order to maximize the effectiveness of collected flight data. The result is a model that performs well in prediction for a range of different tail actuator configurations as demonstrated by our flight results using a bat robot.",RSS2020_Author_Agreement_signed.pdf,"[Jonathan Hoff](http://jehoff2.web.engr.illinois.edu/), [Seth Hutchinson](https://www.cc.gatech.edu/~seth/)",Jonathan Hoff (University of Illinois at Urbana-Champaign)*; Seth Hutchinson (Georgia Tech),RSS2020_Author_Agreement_signed.pdf (359283 bytes); rss-2020_v3.pdf (7752898 bytes),rss-2020_v3.pdf,1314.0,43.0,,2.0,Data-driven modeling of a flapping bat robot with a single flexible wing surface,,0.7873607225258091,"The article represents a good deal of work that is well written. Its strongest point is a well-rounded combination of analysis, data, and working hardware. 

The biggest improvement would be the a more thorough discussion of the accuracy of the modelling approach. Specifically, I would like to see how the RMSE results of the test set. Specifying that figure 7 and 8 were from the test set in their respective captions would also help for clarity. 

Two points I'm curious about that could be included in the discussion or extended further: 
1) How much data is sufficient? Figure 5 seems to suggest that there is not much benefit after training on one data set. 
2) Some analysis or demonstration that this physics-based model does in fact generalize to flight conditions it was not trained on. If this was the intent behind showing results for different values of qDV, this point could use clarification. 

It is worth defining what you consider to be ""long term flight"" since this could mean very different timescales for different types of aircraft. 

Minor suggestions: 
- Using a legend for Fig 6 and 7 would be easier than reading in the caption. 
- Table for [qy px pz qFL qPS qDV] would be useful as well, similar to Table II. 

Typos 
- ""due to flapping an flexibility in the wings""",,"Detailed comments below:

Can the authors show an image of the load cell experimental setup? It’s important to be able to see what was done. A figure showing the optimisation using the force data should also be included.

The modelling assumptions (thin airfoil, quasi steady, rigid plate) are not well justified, some references or a little extra text would help.

How consistent are the launches? How is the motor controlled and how accurately? The latter is relevant to evaluating the model performance.

It would also be helpful to have some velocity plots to see how flight speed changes during the tests. This also has some relevance to the optimisation, as input data points are collected a fixed sample rate over a finite total distance, which biases the optimisation slightly towards slower speeds (there will also be a slight bias to flights with a higher arc vs level flights).

Is there any normalisation of the terms in eq.8? Or do the units change the effective weighting of terms?

In section III the authors describe their modelling choices, but the reader is given little quantitative information about the relative importance/efficacy of these choices in the modelling approach, ie. it would be helpful to see model optimisation results with certain parameters removed, rather than inferring their role qualitatively from flights, perhaps in terms of objective function across a range of initial conditions.

What is the distinction between training and testing flights? Can any flight dataset not be used for either purpose? Why not test the model against all of the recorded data.

I’d like to hear a little further comment on the optimisation. The rapid improvement after a single  flight followed by little further improvement could be expanded upon. What about only using a partial flight? A single flap cycle? The large initial improvement is also to some extent a measure of the unsuitability of the initial guess, but the lack of further improvement suggests the first flight is plenty.

Could the authors expand a little on the limits of their technique? What flights does it fail to capture? Perhaps some test flights could be recorded which have sharper changes in pitch, or even complete stall. How much does the diversity of flight conditions in the optimisation data change the accuracy of the predictions?

The statement towards the end of section VI about reduced aerodynamic area resulting in reduced aerodynamic force is a little trivial. Could the authors  say something more about the origins of the change in parameters, perhaps with  regards to the modelling assumptions (inflexibility, thin airfoil c.o.p, quasi-steady etc.).

Overall, I enjoyed the paper and I wish the authors all the best as they continue their work.
",Agreement accepted,"The paper presents an improvement on the previously introduced model of the flapping-wing drone that allows to achieve considerable improvement in flight prediction. The authors also formulate a general methodology how such precise models of complex aerodynamical systems can be constructed by combining analytical physical models and data-based optimization of a selected set of parameters. 

The paper is written in an excellently clear and detailed way and can serve as a tutorial on the topic. 

The contribution is significant, since the presented methodology can be used to construct and fine-tune models of other aerial vehicles with complex dynamics. The analysis of the changes in parameters, obtained after model optimization, provides interesting insight in the mechanics of flight of the bat-like robot. 

In Fig. 4, it would be good to mark trajectories that correspond to different tail configurations, e.g. by color, to see variability present for the same configuration. 

There were a couple of languages ""slips"", e.g. ""their are unsteady"", ""an flexibility"", ""of different of different"", ""require collect"", ""that collected"", ""in previously"", ""the full the trajectory"".",Agreement accepted,,,,07/15 15:00,07/15 17:00,,https://youtu.be/mDiCt2Tjnck,,,43.0,cN89DTL31R0
"This paper presents a game-theoretic path-following formulation where the opponent is an adversary road model. This formulation allows us to compute safe sets using tools from viability theory, that can be used as terminal constraints in an optimization-based motion planner. Based on the adversary road model, we first derive an analytical discriminating domain, which even allows guaranteeing safety in the case when steering rate constraints are considered. Second, we compute the discriminating kernel and show that the output of the gridding based algorithm can be accurately approximated by a fully connected neural network, which can again be used as a terminal constraint. Finally, we show that by using our proposed safe sets, an optimization-based motion planner can successfully drive on city and country roads with prediction horizons too short for other baselines to complete the task.",RSS2020_Author_Agreement_AL.pdf,"[Alexander Liniger](https://vision.ee.ethz.ch/people-details.MTQ4MzY1.TGlzdC8zMjQ3LC0xOTcxNDY1MTc4.html), [Luc Van Gool](https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html)",Alex Liniger (ETH Zurich)*; Luc Van Gool (ETH Zurich),RSS_SafeSet_Final.pdf (1640204 bytes); RSS2020_Author_Agreement_AL.pdf (1026516 bytes),RSS_SafeSet_Final.pdf,1331.0,44.0,,2.0,Safe Motion Planning for Autonomous Driving using an Adversarial Road Model,,0.5630446510162359,"In general I believe this is a nice paper that is well-written.  I worry that there is not a lot of novelty, and the parts that are new to me (using a function approximation over different bounds on the disturbance) are not rigorous enough to guarantee safety.  I think this paper would be better suited for a conference like ICRA or IROS.

Notes on related work: 
Contrary to the authors’ assertion, I believe that guaranteed following of a path is studied quite a bit, often by using an adversarial game formulation similar to the one proposed in this paper. Some examples:
Majumdar, Anirudha, and Russ Tedrake. ""Funnel libraries for real-time robust feedback motion planning."" The International Journal of Robotics Research 36.8 (2017): 947-982.
Herbert, Sylvia L., et al. ""FaSTrack: A modular framework for fast and guaranteed safe motion planning."" 2017 IEEE 56th Annual Conference on Decision and Control (CDC). IEEE, 2017.
Singh, Sumeet, et al. ""Robust online motion planning via contraction theory and convex optimization."" 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017.
Smith, Stanley W., He Yin, and Murat Arcak. ""Continuous abstraction of nonlinear systems using sum-of-squares programming."" 2019 IEEE International Conference on Decision and Control (CDC). IEEE, 2019.

Questions
- I am unclear what the general assumptions on the dynamics might be.  In this paper a discrete bicycle model is used with steering and acceleration inputs
Why is the information pattern giving an advantage to the control player, rather than the disturbance?
- In (7), are assuming we have a discrete set of control and disturbance actions?  If so, how finely discretized are they?  How does this affect the computation of the set?  Later you say that they are discretized.  Can you comment more on how the discretization affects the computation? 
- How did you choose the discretization of the state space?  What impact does that have on the computation?
- Why did the computation times vary so much?
- Do you have any theory on if the sets will be guaranteed to be continuous across a continuous range of k_max? This is an assumption that seems to be made
- I don’t see how the neural net implementation is guaranteed to not have false positives (where I define false positives as assuming a state is safe when it is not).  In fact, in the tests each method did in fact have false positives.  Given that this paper’s emphasis is on safety and recursive feasibility, I think this should be addressed.",Agreement accepted,"This paper is well-written and introduces a novel approach to address the design of the terminal constraint for nonlinear MPC. The application considered in this paper is path following. While the method and result presented in this paper are all convincing, there are some suggestion to improve the paper.

#1
Presentation of the results. I would suggest to make the subplots in Fig. 5 and 6 landscape instead of portrait. Currently, it is very hard to see the details in a printed version.

#2
What is the difference between the mixed dynamic-kinematic model and the model in (1)? Is the purpose here to test the robustness of the method on a mismatched model?

#3
Need to add unit to Table VI.

#4
Extendability of the method to other dynamic models and under collision avoidance constraint?",Agreement accepted,"The most original part is the problem formulation of safe motion planing in autonomous driving as a game between the motion planner, which aims to follow the road unknown ahead, and the road as adversarial player with bounded curvature, which aims to get the car off road. Under this formulation, the paper employs viability theory and game theory to design a motion planner to follow a path with safety guarantee. Both analysis and MPC simulations are provided to validate the proposed algorithm. Overall, it is a nice contribution to safe planning in autonomous driving.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/alexliniger/AdversarialRoadModel,,,,44.0,IW5ILP14k5o
"To represent motions from a mechanical point of view, this paper explores motion embedding using the motion taxonomy. With this taxonomy, manipulations can be described and represented as binary strings called motion codes. Motion codes capture mechanical properties, such as contact type and trajectory, that should be used to define suitable distance metrics between motions or loss functions for deep learning and reinforcement learning. Motion codes can also be used to consolidate aliases or cluster motion types that share similar properties. Using existing data sets as a reference, we discuss how motion codes can be created and assigned to actions that are commonly seen in activities of daily living based on intuition as well as real data. Motion codes are compared to vectors from pre-trained Word2Vec models, and we show that motion codes maintain distances that closely match the reality of manipulation.",RSS-2020_signed_agreement.pdf,"[David Paulius](https://www.davidpaulius.me), [Nicholas Eales](http://), [Yu Sun](https://cse.usf.edu/~yusun)",David Paulius (University of South Florida)*; Nicholas Eales (University of South Florida); Yu Sun (University of South Florida),RSS-2020.pdf (1547393 bytes); RSS-2020_signed_agreement.pdf (242105 bytes),RSS-2020.pdf,1235.0,45.0,,2.0,A Motion Taxonomy for Manipulation Embedding,,0.475462830775226,"The construction and the choices for constructing this encoding scheme are well explained and documented, however this work lack of an example(even not very difficult) of task that successfully use this encoding because while the construction of the encoding seem coherent it is not sure that this will have a significant effect on a learning task, or at least showing that it increase the learning speed. Creating a link between the geometric/physical/motion and semantic associated of the action is difficult but could be done by multiple ways, ones could be done by using for example siamese networks, by exploiting the similarities in the appearance of the actions(like in a action recognition task) in the dataset in order to create the embedding from the videos. Use both embedding in another task to show the significance of the encoding scheme presented and the benefit from using such encoding would have been appreciated instead of comparing it with word2vec that do not contain motion information about the action.",Agreement accepted,"This paper explores motion encoding using motion taxonomy based on the mechanics of motions, showing how the motion code assignment corresponds to actual data. And as a result, a new embedding method has been developed. The new method translates manipulations into a machine language called motion codes according to some attributes based on contact and trajectory information, whereas the popular word embedding technique Word2Vec embedding model is trained by context. In order to compare motion codes to Word2Vec embeddings, the authors used dimension reduction with PCA and then used t-SNE to visualize these embeddings and their relative distances in 2D. Experiments were carried out on the two methods respectively, and the results demonstrated that these motion codes, when compared to Word2Vec (which uses natural language for training and gives no innate information at all to compare the differences between two labels in a mechanical point of view), produce embeddings that provide better metrics for classification. A number of problems had been solved:
1.	different forms of motion of a same word (e.g. mixing (liquid), mixing (non-liquid))
2.	ignoring the synonyms of label (different labels but similar mechanical motions, multiple synonyms are oversimplified into a single label, e.g. ‘chop’, ‘cut’ and ‘slice’)
3.	multiple meanings of a single word (noun and verb of a word may not share a same meaning, e.g. tap).
In addition, motion codes reduce the amount of features needed to label motions and contain more meaningful information about distances between motions. 
The aim of this research is clearly stated and fully addressed. But I still have some concerns as detailed below:
For clarity:
1.	In Abstract, it is mentioned that binary codes establish a road-map to transfer learned skills to unlearned skills that share similar properties. Please explain it more clearly. 
2.	In the introduction, we were told that this taxonomy can consolidate motion aliases. A further explain on the seasons for consolidation would be helpful.
3.	In this paper, the author defines two weighted values to set the priority of contact or trajectory types when measuring distances, I suggest that the author elaborates on how this is achieved.
4.	In the beginning of section II, a formal definition of motion feature and motion feature space should be introduced, as it may leads to confusion without a definition before having read through the whole paper.
5.	I suggest the author presents the experimental results in a more intuitive form, such as circling the motions that belong to the same cluster.
6.	Figure 4 illustrates how to extract revolute properties for the motion of loosening a screw. However, one more picture with coordinate axis as well as arrows indicating the direction of motion would make it easier to understand.
7.	In Section IV B, this paper compares motion codes to pre-trained Word2Vec models. Please explain how to convert a motion code vector or a Word2Vec vector to the corresponding point in Figure 5.
8.	PCA and t-SNE are used in this paper, I suggest that the author introduce these methods properly for non-expert readers.
9.	In the conclusion (Section V), it says, ""with a suitable model, motion codes can be automatically generated.” Please explain how to obtain this model.

For quality:
1.	The engagement type (rigid or soft) is connected with the structural outcome (deforming or non-deforming). The classification of these two attributes may be duplicate and there may be simpler code.
2.	As mentioned just before the section III, the proposed motion taxonomy is not the ideal way of representing a motion. Considering this as a drawback, can you please indicate what are the most important features that forms a good motion representing method? Or Could you please quantitatively or qualitatively evaluate why this motion taxonomy has this drawback. Are there any other potential drawbacks can be seen? (since they are not explicitly explained in the paper)
3.	In this paper, a criterion for the performance of the embeddings is proposed. This criterion is intuitive; however, it would be more convincing if this paper explains how to use the motion codes for motion recognition, analysis and generation rather than just comparing with Word2Vec embedding.
4.	Section IV compares motion codes to Word2Vec embedding. Please explain why choosing Word2Vec embedding, why not other embedding methods?
5.	In Section IV B, this paper compares motion codes to pre-trained Word2Vec models from Concept-Net, Google News and Wikipedia. However, the data sets used to train these models are not created for robotics. It would be better to use a data set that is created for robot manipulation.
6.	In the conclusion (Section V) and section II, the authors say that their future work will be a neural network that can automatically generate codes for manipulations in video sequences. It would be more impressive to explain the relationship between the newly proposed motion taxonomy and this neural network.
7.	When defining the contact duration, it mentioned that the duration can be measured visually or physically with sensors; However, the threshold or boundary was not given. A better definition on the threshold may be vital for a robot to generate this code automatically.
8.	The text needs to be re-checked, since I found missing of punctuation mark in the paper.
Other suggestions:
1.	I suggest that the author adds the overall framework of the proposed method in the article.
2.	It would be interesting if actions using double-active tools can be considered. E.g. bending a long stick using two hands.
",Agreement accepted,"
*The robot is clearly written and easy to follow.
*Their proposed motion taxonomy represented as a binary vector is interesting, as they can compactly represent many different types of motion. However, it is unclear in what situations it would be useful to use it.

*My main qualm with the paper is that the authors do not clearly validate the paper. They claim that the motion codes can be extracted directly from demonstrations. But they do not provide statistics of how frequently the robot extracted the correct motion codes. One possible validation would be for the robot to automatically create motion codes for a number of different demonstrations. And compare how close the automatically generated motion code is to motion codes hand-coded by an expert.
*I am not sure if the comparison to Word2Vec is the fairest comparison as Word2Vec was created for a very different domain (NLP) rather than describing how a robot or people move. 
*In their comparison to Word2Vec, they claim that their taxonomy is better, but they have no statistics backing this claim. The authors do not present any statistics showing that one was more accurate than the other, instead, they compare several selected examples. A better validation would be to have an expert label how similar different motions are, and how distant Word2Vec and their proposed method are to the expert labels.
*A strong validation might have been to learn several motion taxonomies and see how well the robot could generalize it to new motions (motions that it had never done before).
*I would have liked to see several specific examples of how this taxonomy would improve current robotic applications.
",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,,,,45.0,94FdeKrsVq0
"Aerial manipulation aims at combining the maneuverability of aerial vehicles with the manipulation capabilities of robotic arms. This, however, comes at the cost of the additional control complexity due to the coupling of the dynamics of the two systems. In this paper we present a Nonlinear Model Predictive Controller (NMPC) specifically designed for Micro Aerial Vehicles (MAVs) equipped with a robotic arm. We formulate a hybrid control model for the combined MAV-arm system which incorporates interaction forces acting on the end effector. We explain the practical implementation of our algorithm and show extensive experimental results of our custom built system performing multiple `aerial-writing' tasks on a whiteboard, revealing accuracy in the order of millimetres.",RSS_1199_Author_Agreement.pdf,Dimos Tzoumanikas (Imperial College London)*; Felix Graule (ETH Zurich); Qingyue Yan (Imperial College London); Dhruv Shah (Berkeley Artificial Intelligence Research); Marija Popovic (Imperial College London); Stefan Leutenegger (Imperial College London),Dimos Tzoumanikas (Imperial College London)*; Felix Graule (ETH Zurich); Qingyue Yan (Imperial College London); Dhruv Shah (Berkeley Artificial Intelligence Research); Marija Popovic (Imperial College London); Stefan Leutenegger (Imperial College London),RSS_1199_Author_Agreement.pdf (80983 bytes); RSS_1199_paper.pdf (4387358 bytes),RSS_1199_paper.pdf,1199.0,46.0,,2.0,Aerial Manipulation Using Hybrid Force and Position NMPC Applied to Aerial Writing,,0.8035531097025821,"The paper ``Aerial Manipulation Using Hybrid Force and Position NMPC Applied to Aerial Writing'' presents an MAV-arm platform and nonlinear model predictive control approach for writing tasks.  While I find the paper interesting, there are a few things that hold back the clarity and rigor of the presentation. My two main critiques are (1) the presentation of the ""algorithm"" is convoluted, and (2) the experiments fail to offer any performance comparisons.

MAIN CONCERNS:
- THEORY: The presentation of the theory lacks clarity. Perhaps the most prominent example of this is the authors claim their algorithm is easily-extended to other work, but nowhere in the paper is an algorithm
-- For equations, all variables should be introduced and defined before the equation is presented
-- How does the framework of this NMPC compare to other methods?

- EXPERIMENTS: While the authors present an extremely detailed literature review, there is little tangible analysis between various approaches. A summary comparison of performance characteristics (tracking accuracy, speed, etc) would make the experimental analysis stronger.
-- It is unclear from the experiments why this particular delta arm is appropriate for the writing task
-- It would be helpful to present some of the error metrics as a percentage on the accuracy of the trajectory. 
 

MINOR COMMENTS:
- It seems like the related work could be condensed to give more room to technical content
- The paper ""Nonlinear Model Predictive Control for Aerial Manipulation"" (Lunni et al, 2017) seems relevant to this work. Can the authors comment on the differences in approaches?
",Agreement accepted,"This paper presents an important contribution to the field of aerial manipulation by demonstrating an impressively accurate tracking result for direct force feedback in combined position and force control for an underactuated MAV with an actuated arm. With fast arm dynamics to compensate for error in the underactuated base, tracking of the end effector is significantly improved. Further compliments to the team for achieving this result with mostly low cost and easily available parts. The reviewer sees this work as original, high quality, clear, and very significant to the aerial manipulation community.

Title + introduction:
“Aerial manipulation” might be a bit strong for the title. Would suggest “Aerial Interaction” or simplifying to “Combined Force and Position NMPC Applied to Aerial Writing”. See comment about hybrid position and force control below...

“Millimeter accuracy” should refer to accuracy of 1mm, in this case it is around 1cm, so would be centimeter accuracy. It’s just a name, but should honestly reflect the result. Otherwise, just mention accuracy of about plus/minus 10mm.

“In contrast to the second approach, we achieve on par precision while ...” → Introduction section shouldn’t really include results. Also this statement seems to highlight a superior approach, when better performance could be attributed to a nicer hardware implementation, control method (NMPC), or better tuning.

Typos:
page 2:
	- an underactuated MAVs → an underactuated MAV
page 5, section VI: 
	- a trust stand → a thrust stand
	- T_{wT} → T_{WT}
page 8:
	- feasible plann → feasible plan
	- tranformation → transformation

Equations:
Page 3:
	- Revisit the formulations of (1b) and (1e). 
	- The line of text after eq (4) should refer to _{T}r_{E_z}, instead of _{C}r_{E_z}.
Page 4, section C: 
	- Equations describing {A}r{J} from the geometry of figure 4, the reviewer believes should use only R instead of (R – r).
Page 5, Fig 4:
	- (Front view) Frame F_A should be at the center of the delta structure
	- (Side view) {A}r{I_1} should be {A}r{J_1} 

Comments:
- There is little discussion on the limitations of an underactuated system in terms of force exertion. The reference (and experimentally measured) forces are very small, particularly for contact inspection applications. There is clearly a relationship between higher force exertion and stability, that is not discussed in this paper. What are the limitations of force control for an underactuated MAV? How are force magnitude, position error, and stability coupled when we push these limits?

- The term hybrid force and position control usually refers to Raibert and Craig’s implementation involving a selection function to control force in the constrained direction and motion in the orthogonal directions. Is this relevant here? It seems that this control approach combines both without selection, which would mean that the wall and end effector position must be exactly where expected. Perhaps the author could revisit the terminology and discuss the limitations of this environment model in an unstructured world (the discussion point that the whiteboard is not perfectly flat is already in this direction, and whiteboards are indeed quite flat!).

- Experimental tuning of the costs Q is mentioned. The experimental values would be interesting for the research community, and useful for repeating results. Also, what are the effects of varying the prediciton horizon?

- All error plots show end effector error above 1cm at some point, so it isn’t exactly sub-centimeter accuracy, but certainly on the order of 1 cm! The text states several times that the error does not go above 10cm, please revisit this. Interestingly, the higher error tends to occur when the system is in free flight, any thoughts on this?

- Force trajectory generation is not discussed, but from the results plot seems to be a step function. Would smooth force trajectories give a better result, or is the predictive model element able to handle this very well?

- The last paragraph in VI.C. mentions that the control model assumes the position of the end effector can be controlled infinitely fast, meaning that a step response would not be handled well by the MPC formulation. Some comment to address this? Should the MPC be reformulated so these can be reflected in control input constraints?

- The statistical evaluation approach with multiple trials for different trajectories is well presented and highly appreciated!",Agreement accepted,"The paper discussed aerial manipulation systems of MAVs and proposed a new method to solve the problem of end effector trajectories tracking of a MAV equipped with a manipulator, where the task is to control both the vehicle and the manipulator for ""aerial-writing"". It introduced a novel formulation for the hybrid system, in which a set of standard Newton-Euler equations are used for modeling the dynamics. In particular, the effect of the external contact force that is introduced by the manipulator and acted on the MAV is modeled in the dynamics, where the forces are approximated via a linear spring model.

A nonlinear MPC was used for the trajectory tracking task. The author also talked about the trajectory generation method they used for mapping arbitrary sets of characters to end effector trajectories, where they assume the accelerations are constant.

The author(s) conducted a list of experiments and demonstrated the effectiveness of the proposed approach. The proposed approach achieved high accuracy (millimetre-level accuracy) in writing different characters, such as RSS or E=mc^2, on a whiteboard given a perfect state estimation of both the vehicle and the board from a motion capture system. Experiment setups are discussed. Detailed explanations of the experimental results are provided by the author(s). The author pointed out the implementation details, technical difficulties they encountered during the experiments, and limitations of the method.
The paper was written in clear and formal English, with a well-organized structure and concise expressions. Overall, the paper contributes to aerial manipulations by combing a novel hybrid dynamical model with nonlinear model predictive control.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,https://www.youtube.com/watch?v=iE--MO0YF0o,,,46.0,slBsougeZhc
"Given a desired object trajectory, how should a robot make contact to achieve it? This paper proposes a global optimization model for this problem with alternated-sticking contact, referred to as Contact-Trajectory Optimization. We achieve this by reasoning on simplified geometric environments with a quasi-dynamic relaxation of the physics. These relaxations are the result of approximating bilinear torque effects and deprecating high-order forces and impacts. Moreover, we apply convex approximations that retain the fundamental properties of rigid multi-contact interaction. As result, we derive a mixed-integer convex model that provides global optimality, infeasibility detection and convergence guarantees. This approach does not require seeding and accounts for the shapes of the object and environment. We validate this approach with extensive simulated and real-robot experiments, demonstrating its ability to quickly and reliably optimize multi-contact manipulation behaviors.  ",RSS2020_Author_Agreement.pdf,"[Bernardo Aceituno-Cabezas ](http://aceituno.mit.edu/),  [Alberto Rodriguez ](https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU)",Bernardo Aceituno-Cabezas (MIT)*; Alberto Rodriguez (MIT),2020_RSS_QuasiDynamicModel (4).pdf (4313691 bytes); RSS2020_Author_Agreement.pdf (161326 bytes),2020_RSS_QuasiDynamicModel (4).pdf,1188.0,47.0,,2.0,A Global Quasi-Dynamic Model for Contact-Trajectory Optimization in Manipulation,,0.7633139542530609,"Originality:
The paper considers a simple problem and provides an interesting approximation to solve it with guarantees of global optimality and infeasibility. The work is original, and provides a simplified solution to a 2D robotic manipulation problem. In fact, I would suggest the authors to highlight in the title and abstract the 2D character of the solution, as the extension to 3D is not straightforward (as they also recognize in the future work section).
The authors rely on a quasi-dynamic description of motion, where they drop the inertial terms to simplify the formulation of the rigid body dynamics. The same approach is typical in the robotic locomotion community, where similar simplifications are assumed on the so-called floating-base dynamics, for isolating the effects of external forces on the motion of the center of mass. A wider discussion on the relation of their formulation to this free-floating locomotion models would be beneficial.

Quality:
The approach is mostly well described and properly supported. I have some comments that will hopefully help to improve the quality of the paper by clarifying some aspects of the approach:
-The model requires as an input the trajectory of the object. While this trajectory can be seen as snapshots of the object motion in time, it would be interesting to know how the authors would generate such trajectory in a more complex case, for instance, a peg-in-hole problem where multiple sequential contact possibilities exist to insert the peg, and providing different sets of trajectory snapshots would lead to different contact-trajectories. Also, note that using a trajectory as input implies that the problem formulation includes already information relative to the object, environment and task, so the algorithm is not really agnostic to them (page 3, sec III.B).
-The authors state that “the ability to report when a task is infeasible, either because it requires more than one finger or because it is physically impossible, also provides useful insight on the primitive itself.” It would be great if this explanation is extended, i.e. how can they identify the root cause for infeasibility of a given contact-trajectory.
-Connected to the previous point, simulation results nicely show the application of the method. However, it is not clear from the text what was the approach to solve problems that are unfeasible on a first try, and require, for instance, an increased friction coefficient or a higher number of contacts. Was it an automatic choice of the model? (as, for instance, the experiment with the triangle seems to suggest).
-The experimental validation with the robot Yumi shows an open-loop execution of some of the simulated examples. In this sense, the robot is merely being used to follow a prescribed trajectory, so the section does not really enhance the impact of the paper. One interesting point that would increase the interest of this section would be, for instance, reporting what are the speed limits that still lead to the expected behavior, so that the bounds for a “slow enough” motion can be quantified in the real world. Also, bounds on uncertainties in the contact location are another point that could be quantified on real experiments.
-It would be interesting to add the required time to solve each individual problem, as this provides an idea of their computational cost. Also, please double check the numbers, as in page 7 it is reported that the trajectories are optimized “in the range of 0.04 to 0.44s,” while page 8 indicates that “trajectories are found between 0.04 s to 1 s.”
-The problem requires some assumptions on the environment, e.g. friction coefficients between fingers and object, and between object and environment. Could the authors provide some type of sensitivity analysis for these factors?

Clarity:
The paper is in general well-written, and explanations are easy to follow. I have just minor suggestions here:
-Please improve the explanation in this sentence, page 8: “…many simple trajectories are infeasible to generate, such as lifting a steep triangle.” While the paragraph is dealing with the limitation of the approach stemming from the fact that it requires a trajectory as input, it is not clear if the above sentence is referring to the contact-trajectory optimization problem…specifying a trajectory for lifting the triangle should not be problematic per se.
-Please check for some remaining typos on the paper. Some examples:
-Page 2, column 1, line 3: “…for planar tasks consistently in a less than 1 s”
-P2, C2, paragraph 3: “if y=0 then …is enforced, if y=0 then it is not”
-P2, caption of fig.3 “…friction cone on of these…”
-P7, C1, Par.2: “…are required rotate the block around int geometric center”
-P8, C1, L1: “Our model is accounts for…”
-P8, C2, L6, “…the friction cone border constraints is non-convex and it would have to be…”

Significance:
The paper solves a simple yet interesting problem in 2D robotic manipulation. As mentioned before, the 2D character of the solution should be better highlighted both in the title and the abstract. ",,"Planning through contacts for manipulation has been considered difficult because solvers often find bad local minima and takes huge computation load. This work provides global optimality and fast computation speed without sacrificing too much modeling accuracy. Although it doesn't fully solve the planning problem (object motion is given), the contribution is still important.

Overall the paper is well written and easy to follow. The problem formulation and key idea are clear. Results are well presented.

A few things to think about:

* MIQP doesn't scale well with the number of contacts and time steps, such as in [16]. Why is this formulation fast? Is it because the problems are not big enough, or is there something in this work that greatly influences the computation?

* The approximation produces necessary conditions. Before testing in experiments, is there a way to check if a solution is actually feasible?

* Is the Non-penetration constraint necessary? Is CT7 (non-penetration) redundant since you already have finger position constraint in CT3 (contact constraints)?

* How do you pick the initial solution for the optimization? Is it random? Although the algorithm can find the optimal solution, does the computation time depend much on the quality of initial solution?

* The experiment is done by robots ""guided through position commands"". How is the force related command executed?


A list of small issues:
----------------------
In Abstract:
""approximating bilinear torque effects and deprecating inertial effects and impacts."" Not clear what that means

""Since we provide the object motion as an input and we
assume an uniform pressure distribution on the object facets,
the contact schedule between the object and the environment is
also known. ""
Why do you need uniform pressure assumption? You can use point approximation to edge contacts even when the pressure is not uniform.

typo in VI.A: ""the model chooses to place a second ﬁnger to push on the ground a generate additional reaction force""

""one ﬁnger results sufﬁcient."" -> "" one ﬁnger is sufﬁcient.""

All the double quotes: ”end” ->  “end”

More typos
""However, two ﬁngers are required rotate the block around int geometric center.""
""Our model is accounts for the environment and object shape""
",Agreement accepted,"
The paper addresses an important and timely topic: optimization for contact-based manipulation. The method generates contact trajectories for a wide variety of manipulation primitives, which is impressive. I appreciate the clarity with which the paper is written; it was a pleasure to read.

Contact-based optimization is definitely still an open problem and we need more work in this area. In that sense, this paper is a valuable contribution, as it proposes new and potentially useful techniques. But the paper can be more clear on how/why the proposed method is better than other recently proposed trajectory optimization methods (many of these methods are referenced in the paper and for example include methods from Todorov's and Tedrake's groups). I understand that the problem is a bit different, i.e. the current work assumes the motion of the object is given, but this should in general make the problem easier to solve, not more difficult, and existing methods can be used with the object motion variables fixed. It is not clear why fixing the object motion variables is an advantage.

The results are impressive in terms of the variety of primitives the method can solve for, but in terms of the two key problems most contact optimization methods suffer from - computational expense and difficulty of execution in real world - the results do not suggest much better performance.

A few other points to clarify are:

Can finger contact points jump between timesteps? Is continuity enforced? This would be important for realistic solutions when there is gravity. I can see there is a smoothness cost on the finger motion, but that is not a hard continuity constraint.

In Figure 6a, why does the optimizer choose to push down from the top, instead of pushing from the side? The optimization cost penalizes high contact forces and pushing from the side should require less force.

It would be good to describe in a bit more detail how the real world execution is performed. Is it pure position control? Are the force outputs of the optimization used at all?

A possibly relevant piece of work is from Lee, Lozano-Perez, and Kalebling, ""Hierarchical planning for multi-contact non-prehensile manipulation"". It is not exactly an optimization approach but it attacks a similar problem, and it solves the problems of generating object motion and contact forces separately in a hierarchical manner, which may be relevant to the particular approach this paper is taking.
",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/baceituno/QuasiDynamics,https://youtu.be/-b_e18kwIdw,,,47.0,UIPT-8UCoNA
"We present Nav2Goal, a data-efficient and end-to-end learning method for goal-conditioned visual navigation. Our technique is used to train a navigation policy that enables a robot to navigate close to sparse geographic waypoints provided by a user without any prior map, all while avoiding obstacles and choosing paths that cover user-informed regions of interest. Our approach is based on recent advances in conditional imitation learning. General-purpose safe and informative actions are demonstrated by a human expert. The learned policy is subsequently extended to be goal-conditioned by training with hindsight relabelling, guided by the robot's relative localization system, which requires no additional manual annotation. We deployed our method on an underwater vehicle in the open ocean to collect scientifically relevant data of coral reefs, which allowed our robot to operate safely and autonomously, even at very close proximity to the coral. Our field deployments have demonstrated over a kilometer of autonomous visual navigation, where the robot reaches on the order of 40 waypoints, while collecting scientifically relevant data. This is done while travelling within 0.5 m altitude from sensitive corals and exhibiting significant learned agility to overcome turbulent ocean conditions and to actively avoid collisions.",RSS2020_Author_Agreement (1).pdf,Travis Manderson (McGill University)*; Juan Camilo Gamboa Higuera (McGill University); Stefan Wapnick (McGill University); Jean-François Tremblay (McGill University); Florian Shkurti (University of Toronto); David Meger (McGill University); Gregory Dudek (McGill University),Travis Manderson (McGill University)*; Juan Camilo Gamboa Higuera (McGill University); Stefan Wapnick (McGill University); Jean-François Tremblay (McGill University); Florian Shkurti (University of Toronto); David Meger (McGill University); Gregory Dudek (McGill University),RSS2020_Author_Agreement (1).pdf (102068 bytes); Vision_based_goal_conditioned_policies_for_navigation_and_obstacle_avoidance_in_underwater_vehicles.pdf (2800665 bytes),Vision_based_goal_conditioned_policies_for_navigation_and_obstacle_avoidance_in_underwater_vehicles.pdf,1324.0,48.0,,2.0,Vision-Based Goal-Conditioned Policies for Underwater Navigation in the Presence of Obstacles,http://www.cim.mcgill.ca/mrl/nav2goal/,0.7057492081425519,"The proposed approach is a novel integration of several existing methods. The approach that is used for balancing exploration and exploitation (i.e., “uncertainty guided exploration”) does not seem to be very sophisticated. There is also a lack of evaluation metrics for the experimental data. A comparison with benchmarks, such as a method based on traditional mapping, planning and control approaches would be beneficial. 
The manuscript also has several typos and grammatical errors. Here are a few examples:
Page 5: “As well, in contains an Inertial Measurement Unit (IMU)…”
Page 7: “With these trajectories, we created a dataset of X images, goal, action tuples to train the goal-conditioned policies3. We split this dataset into a training set of X samples and a validation set of Y samples.”
Page 8: “, as it the robot’s motion is now influenced by…”
",Agreement accepted,"In addition to the comments above, the paper is rather well presented, well organised and easy to follow. The experiments are well designed and convincing and the experiments in simulation provide some information about the performances of the system.

The following list questions or details that may need to be addressed in future iterations of the paper.

III.A: patch -> pitch, backpropegation -> back-propagation.
End of IV: ""dataset it was sued to generate it"". I do not understand this sentence. How can a dataset be sued?
V.A: in contains -> it contains
Fig. 8: the black crosses are hard to read on the dark background of coral mounds.
VI.B: and collect new data -> and collected new data
as it the robot's motion -> as the robot's motion

Still in VI.B: the system is said to be trained from a new set of collected data. It might have been interesting to evaluate the feasibility of fine-tuning the policy from the model trained in simulation. Is this something that was tried? considered? rejected? A few lines on this consideration would be very interesting. 

Additionally, in VI.B, a number of failure cases are implicitly reported by stating the success rate of the approach. It would actually be very interesting to describe and understand these failure cases in more details, in particular to understand the limits of the trained approach.

It would also be interesting to describe/observe the performance of the methods at the edge of its observation space. Does it break dramatically as soon as the environment changes? 

This last point is particularly relevant because the conclusion claims that the system is robust for practical deployment in the field. Robustness is a property which is hard to quantify for such a system. The reported performance hint at a possibly robust system but no experiments was produced to explicitly test the robustness. 


",Agreement accepted,"Overall, the paper is reasonably well written and the figures serve to illustrate the proposed system and results.  There were a few typographical and grammatical errors that should be addressed in revising the paper.  There were also a few design decisions that need to be explored in more detail to allow a reader to grasp the rationale for these choices.

It was a bit unclear how the ground-truth data for the low-level controller was generated.  Was this based on human-labelled examples or previous robot experience?  How can the performance of the robot be guaranteed in terms of obstacle avoidance if the labels are being supplied by an expert?

The simplification of the environment to a simple 2D goal location is a bit questionable, particularly in a coral reef environment where the interesting regions tend to be significantly more rugose than surrounding areas of sand.  It is not clear why a 3D goal was not used - nor what the implication of taking depth/altitude into account would be on computational efficiency and goal convergence.

The choice of specifying goals relative to the current position may make it difficult to direct the system towards specific geographic locations.  What would the implication be of specifying desired waypoints in global coordinates - or is the problem that the robot doesn't have a notion of its global location and so must rely on navigating within its current local frame of reference?

Fig. 10 is designed to illustrate how achievable trajectories are stitched together form more complex trajectories of waypoints.  The training trajectories look highly asymmetric and are likely to be impacted by effects such as currents and wave motion.  It is not clear why a parametric model couldn't be used in this case - nor why the robot wasn't capable of turning on the spot (given that it has this capacity).  It appears that the desired behaviour was to maintain a constant forward speed but this appears to come at the expense of the ability to follow more complex trajectories.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,http://www.cim.mcgill.ca/mrl/nav2goal/,http://www.cim.mcgill.ca/mrl/nav2goal/,,,48.0,qpcmwb_7QA4
"There is a rising interest in Spatio-temporal systems described by Partial Differential Equations (PDEs) among the control community. Not only are these systems challenging to control, but the sizing and placement of their actuation is an NP-hard problem on its own. Recent methods either discretize the space before optimziation, or apply tools from linear systems theory under restrictive linearity assumptions. In this work we consider control and actuator placement as a coupled optimization problem, and derive an optimization algorithm on Hilbert spaces for nonlinear PDEs with an additive spatio-temporal description of white noise. We study first and second order systems and in doing so, extend several results to the case of second order PDEs. The described approach is based on variational optimization, and performs joint RL-type optimization of the feedback control law and the actuator design over episodes. We demonstrate the efficacy of the proposed approach with several simulated experiments on a variety of SPDEs.",RSS doc2.pdf,Ethan Evans (Georgia Institute of Technology)*; Andrew Kendall (Georgia Institute of Technology); Georgios Boutselis (Georgia Institute of Technology ); Evangelos Theodorou (Georgia Institute of Technology),Ethan Evans (Georgia Institute of Technology)*; Andrew Kendall (Georgia Institute of Technology); Georgios Boutselis (Georgia Institute of Technology ); Evangelos Theodorou (Georgia Institute of Technology),RSS doc2.pdf (432689 bytes); RSS_submission__Policy_and_Actuator_Placement_Optimization_for_Stochastic_Spatio_Temporal_Systems (13).pdf (2926478 bytes),RSS_submission__Policy_and_Actuator_Placement_Optimization_for_Stochastic_Spatio_Temporal_Systems (13).pdf,113.0,49.0,,2.0,Spatio-Temporal Stochastic Optimization: Theory and Applications to Optimal  Control and Co-Design,,0.8097827783935809,"In this paper, the authors consider the task of jointly co-design an optimal control policy and actuation architecture (specifically, actuator locations) for first and second order stochastic partial differential equations.  They formulate the problem as an optimization on Hilbert Spaces of nonlinear PDEs, and show that through an appropriate change of measure, the problem can be formulated as one that can be tackled through a spatio-temporal reinforcement learning based approach.  They demonstrate the efficacy of their method on four simulated case studies: a temperature reaching task on a 1D heat equation, a velocity reaching task on a Burgers equation, a voltage suppression task on a Nagumo equation, and an oscillation suppression task on the Euler-Bernoulli equation with Kelvin-Voigt damping.

Overall, I found the paper to be well written, motivated, and to address an important problem.  The proposed solution is theoretically sound, novel, and empirically well supported, and I believe that it would make a nice contribution to the conference.  Below I have some minor suggestions that the authors may wish to take into consideration for the final submission.


In the literature review, the authors may wish to mention some of the work addressing architecture co-design for LTI systems.  Relevant references include:
- Matni, Nikolai, and Venkat Chandrasekaran. ""Regularization for design."" IEEE Transactions on Automatic Control 61.12 (2016): 3991-4006.
-Lin, Fu, Makan Fardad, and Mihailo R. Jovanović. ""Design of optimal sparse feedback gains via the alternating direction method of multipliers."" IEEE Transactions on Automatic Control 58.9 (2013): 2426-2431.

In definition III.1, I don’t think that the operator \mathscr{L} is ever defined.

In Section IV: The result in this section is not initially well motivated: why is a measure theoretic view of variational optimization useful, and why is a change of measures result the appropriate technical tool for achieving this?  In fact, it was not until the final few paragraphs that it was clear that these methods were needed to translate the co-design goal into one amenable to RL techniques — I would recommend a slight reorganization of this section to help motivate the technical results being presented to the reader in terms of formalizing eq (1) into an actionable cost function.


",Agreement accepted,"My sense is the scope of the paper is too broad. The paper is contributing both fundamental theory for stochastic partial differential equations, as well as algorithms for optimization of actuator placement and optimal policy design. I find there is too much here, and I question if the paper can have impact if theoretical readers will gloss over the optimization algorithms while practical readers will gloss over the math. I personally would suggest to write the theoretical results in a more tutorial style (the authors could submit those results to a theory journal). Then explain clearly how the theory can improve the solution of the optimization problem. I have to say I was quite lost in the technicalities of the paper to make sense of the overall result.",Agreement accepted,"The paper has made significant contribution in tackling the challenging coupled task of joint policy optimization and actuator co-design. The approach is based on a general principle from thermodynamics, which is very interesting. The paper has also extended existing formulation of Girsanov Theorem to a second order version, which is of research significance. The proposed algorithm has also been validated by experiments on reaching tasks and suppression tasks. ",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,https://youtu.be/pqryLc4wCuU,,,49.0,CuoxQPqGBcs
"We propose a principled kernel-based policy iteration algorithm to solve the continuous-state Markov Decision Processes (MDPs). In contrast to most decision-theoretic planning frameworks, which assume fully known state transition models, we design a method that eliminates such a strong assumption which is oftentimes extremely difficult to engineer in reality. To achieve this, we first apply the second-order Taylor expansion of the kernelized value function. The Bellman equation is then approximated by a partial differential equation, which only relies on the first and second moments of the transition model. By combining the kernel representation of value function, we then design an efficient policy iteration algorithm whose policy evaluation step can be represented as a linear system of equations evaluated at a finite set of supporting states. We have validated the proposed method through extensive simulations on both simplified and realistic planning scenarios, and the experiments show that our proposed approach leads to a much superior performance over several baseline methods.",RSS2020_Author_Agreement.pdf,"[Junhong Xu](https://junhongxu.github.io/), [Kai Yin](https://scholar.google.com/citations?user=oh9wZ9sAAAAJ&hl=en), [Lantao Liu](http://homes.sice.indiana.edu/lantao/)","Junhong Xu (INDIANA UNIVERSITY)*; Kai Yin (Vrbo, Expedia Group); Lantao Liu (Indiana University, Intelligent Systems Engineering)",RSS.pdf (6067833 bytes); RSS2020_Author_Agreement.pdf (65587 bytes),RSS.pdf,1285.0,50.0,,2.0,Kernel Taylor-Based Value Function Approximation for Continuous-State Markov Decision Processes,,0.444073308574443,"The paper provides a number of interesting contributions to the MDP-solving literature. The ability to deal with continuous state spaces and partially known transition functions (represented by their mean and variance) in a computationally efficient was will be of interest to the planning under uncertainty and reinforcement learning community. The paper is well written and easy to understand (at least by a relatively knowledgeable reader in the area). The simulations are sufficient, but could potentially be expanded to more environments (though it is understandable as is given space constraints). Additional comments are listed below:

-It would be worthwhile to provide just another paragraph in the intro describing an example of how one would find the mean and variance of the function. Further down in the results, is your algorithm robust to distributions that are somewhat bi-modal and do not follow a Gaussian type distribution? I would imagine this would cause your algorithm to break?

-How many trials would you expect to need to properly estimate the mean and variance? How robust are you if these values are not correctly estimated? Furthermore, what if they varied significantly across the environment (e.g. in different terrains)? Some of these issues should be addressed as assumptions or as future work.

-The complexity of the solving the problem is not addressed. How formally does it scale as the size of the problem increases (i.e. how would the solution to the linear equations scale)?

-The insight provided into parameter selection is particularly welcome. Oftentimes papers simply select the parameters through a parameter sweep without giving the reader sufficient context to interpret them. This section is nicely done.

Minor comments
-Typically ""decision making"" is only hyphenated when used as an adjective (not as a verb)
-Some of the graphs are now black and white friendly
-The citations have a number of weird capitalizations (e.g. too many caps in ref [13] and lack of proper caps in words like UAV and MDP that should be capitalized). Also, the citations have a lot of redundant information like repeated dates and repeated publishers. Citations should not just be copied from google scholar, but should be reformatted to RSS specifications.",,"Overall, the paper is very well written.  The rationale for the proposed approach is well supported and placed within the context of existing literature.  The results and accompanying figures do a good job of illustrating the performance of the proposed method.

The paper is original and the quality of the presentation is very high with a clear description of the proposed method.  The results are compelling and the contrast against other methods is well supported.",Agreement accepted,"The purpose of the paper itself is not clear.  One could assume that the authors have a general solution for continuous MDPs, but they define their function approximations without mentioning error bounds or convergence properties.  In the second half of the paper it is clear that their intended application is path/navigational planning, so their framework and approximation criteria makes more sense from this applied perspective.

An interesting contribution is that their function approximation is based on only the mean and variance of the probability distribution of actions so, as the authors suggest, they reduce the dependence on a complete transition model as is typical in planning problems.  However the limitations of both MDP planning and RL seem a bit exaggerated: *cumbersome* training trials, *unrealistic* transition models.  In contrast, this model is heavily reliant on a prebuilt set of supporting states, and sensitive to at least two hyperparameters, an issue that is only casually addressed as part of the experiments and not as part of the principles.  I think the tradeoff is justified for navigational planning tasks, but the paper starts from a different premise.  Consider on the other hand a paper like:

Bonet, Blai. ""An e-optimal grid-based algorithm for partially observable Markov decision processes."" Proc. of the 19th Int. Conf. on Machine Learning (ICML-02). 2002.

which is intended for POMDPs but addresses the general case of how a grid-based approximation affects value functions in large state spaces.

In general the writing quality could be improved, it is somewhat repetitive (eg. first paragraph of section IV) and overexplained (eg. item 2 in section V-A is unnecessary, average returns are standard practice).  There are also several cases of incorrect use of articles (eg. grids in vicinity, characteristics of state space, the value iteration, etc.) that interrupt the reader.  Some of the mathematical notation is also not properly introduced (m and N in eq. 4, x and y in IV-A, etc.).

In eqs. 6a, 6b, 13 and 14, what is the meaning of (s' - s)?  It looks like a difference between states. I suppose they mean (v(s') - v(s)) or the equivalent function.  Also are both 6a and 6b defined as a sum over a *discrete* set of states?

The results show two sets of simulated navigation problems, one with obstacle avoidance and another with punishing Martian terrain.  Not sure why they need to say that the first problem is goal oriented; all planning is goal oriented (even if the goal is solving an optimization problem).

In the first problem they test four techniques on a varying number of supporting (grid) states, in which their Taylor-based approach performs similarly to the Direct Kernel-Based approach, and both of these obtain higher average returns than a neural network approach and some unspecified grid-based PI.  The proposed Taylor method and the direct Kernel method also have very similar runtimes, although the Taylor method requires only the mean and the variance. 

There are some qualitative evaluations like a ""reasonable approximation"" and an ""aggressive policy"" that seem largely subjective.  An important thing to address here is whether all four methods used the same supporting states.  If so, this might constitute a bias that could have affected the real performance of methods that perform their own state abstractions, such as the NN method (maybe) or a more sophisticated grid-based PI.  Not enough information is provided.

In the Mars navigation problem they test three state sampling methods and show that importance sampling (based on the terrain slope) produces better results than even/uniform sampling, but this should be expected.  Notice that this kind of sampling assumes the existence of a full (and accurate) map in advance, which (could be argued) decreases the benefit of not requiring a full transition model.

The main conclusion made by the authors themselves is that the performance of their algorithm is mostly determined by the number and distribution/selection of supporting states, but I think this is a general conclusion applicable to all grid-based approximation methods.  The proposed method seems appealing for motion and navigational planning tasks, and appears to perform at least as well as the direct kernel-based method.  The issue of whether this Taylor-based method requires less prior knowledge (or assumptions) than competitive approaches is probably domain dependent.  It requires only the mean and variance of action outcomes but assumes there is sufficient correct information in advance to perform a meaningful selection of supporting states, which turns out to be the most significant factor.

I think the paper is somewhat disorganized in both ideas and structure, and the experimental results aren't very convincing.  It does have an interesting contribution but the paper quality needs to be substantially improved.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,,,,50.0,LaS3dK2Lk88
"We present a novel approach to generate collision-free trajectories for a robot operating in close proximity with a human obstacle in an occluded environment. The self-occlusions of the robot can significantly reduce the accuracy of human motion prediction, and we present a novel deep learning-based prediction algorithm. Our formulation uses CNNs and LSTMs and we augment human-action datasets with synthetically generated occlusion information for training. We also present an occlusion-aware planner that uses our motion prediction algorithm to compute collision-free trajectories. We highlight performance of the overall approach (HMPO) in complex scenarios and observe upto 68% performance improvement in motion prediction accuracy, and 38% improvement in terms of error distance between the ground-truth and the predicted human joint positions.",RSS2020_Author_Agreement.pdf,Jaesung Park (University of North Carolina at Chapel Hill)*; Dinesh Manocha (University of Maryland at College Park),Jaesung Park (University of North Carolina at Chapel Hill)*; Dinesh Manocha (University of Maryland at College Park),HMPO_final.pdf (952996 bytes); RSS2020_Author_Agreement.pdf (476035 bytes),HMPO_final.pdf,1272.0,51.0,,2.0,HMPO: Human Motion Prediction in Occluded Environments for Safe Motion Planning,,0.015380656657415,"
Comments:

*. This paper seeks to address the important problem of generating collision-free paths for a robot by predicting human motion in occluded scenes. The idea of introducing occlusion-based constraints in the objective function for motion planning is well motivated.

*. Please consider revising the description in the paper to clearly state the inputs and outputs of the different components. For instance, until Section IV, it is unclear what the inputs and outputs are of the human motion prediction component, although this component is referenced multiple times in the initial few sections.

*. Could you please highlight the specific novel contributions instead of claiming that the entire approach is novel? The use of deep networks for human motion prediction is not really new, especially when you seem to be using pre-trained features. The novelty here seems to be in the inclusion of the ""occlusion masks"" to augment the input data vectors. In a similar manner, the optimization-based algorithm for robot trajectory planning is not new; in fact, even the inclusion of additional occlusion-based constraints is not really new. The novelty here seems to be in the particular formulation (and the associated heuristics) introduced in the paper. 

*. Is the insertion of occlusion (based on forward kinematics) in the human motion tracking datasets accurate? How is the corresponding ground truth determined for experimental evaluation? If this projection is accurate, is it potentially possible to build on such an approach to determine regions of occlusion in images without having to use the deep networks? This would be a more classical approach for predicting human motion, and it may be more computationally efficient.

*. The stated improvement in performance, especially in action classification, in the text of the paper does not seem to match the numbers in the table. Does HMPO really improve classification accuracy by 63% or 86% and if so how/why? 
",Agreement accepted,"The authors present an approach for motion planning when working alongside a human. I think this is a good problem to solve. The idea is that the authors train a neural net to solve the problem. The model architecture is based on a CNN component, which uses pretrained ResNet features.They train an LSTM to predict (a) human action, (b) joint positions, and (c) degree of occlusion. They made predictions out to 3 seconds in to the future. I would've liked some extra details about the neural net and training parameters

The paper focuses on occlusions caused specifically by the robot arm. This means that instead of collecting a new dataset, they can use simulated images and generate their own augmented datasets. They report prediction results on three different datasets with occlusion. I would have liked to see plots of accuracy over time, instead of just the single accuracy measure reported in Table 1. Error still seems extremely high to me -- at best being 31.8 cm -- but the authors did a good number of comparisons against different baselines.

The motion optimization algorithm isn't too novel, but seems thorough and well-explained. I think the biggest problem I have with this is that I'm not sure how this would be used in the real world. The neural net is given both the images with and without the robot occluding the scene, which is a problem. The authors describe some real robot experiments, but they don't show it in their video and it's not clear to me exactly how this would work.

In the end, I thought it was a good paper, but not an amazing one. More thorough results would help a lot.

Minor notes:
- There are some weird artifacts and spacing. On pg. 8, for example, there's a really big gap between two paragraphs. I think the authors could always expand the paper, add more images of their data or experiments, and generally better use space.
- ""Small caps"" captions for tables are pretty annoying, kind of hard to read. 
- pg. 6: ""prevents the robot to occlude"" --> ""prevents the robot from occluding""",Agreement accepted,"This paper introduces a framework for generating collision-free robot trajectories for robots operating in close proximity with a human. In such scenarios, the robot motion often yields self-occlusions, resulting in reduced accuracy of human motion prediction. To tackle this problem, the paper introduces a deep learning-based framework for human motion prediction under occlusions and an occlusion-aware motion planner that generates collision-free robot actions. The whole pipeline is evaluated in a series of scenarios demonstrating motion prediction accuracy and robust collision avoidance.

Strengths
-The paper presents a complete pipeline closing the loop between perception of occluded human activities to the generation of collision-free, occlusion-aware robot motion plans.
-The approach is validated on a significant number of human activity datasets with significant performance achievements.
-The paper is well-written and easy to follow.
-The problem is widely interesting to the robotics (and RSS) community and of potential future impact.

Weaknesses
-One of the major limitations, as also noted by the authors has to do with the fact that the training dataset was artificially augmented to introduce the robot motion plans. In practice, a human working in close proximity with a robot would probably act differently. An experiment would have helped us see if the learned framework could transfer to the real-world.
-It was not clear to me how the virtual robot motion plans were generated. What kind of planning problems were being solved to generate the virtual robot trajectories? Does the type of robot motion affect performance in occlusion detection and avoidance? It would appear to be so.
-Another limitation, also pointed out by the authors, has to do with the potential local minima of the introduced cost functions for collision avoidance and occlusion penalization. While the reported results appear compelling, the robustness of the cost functions considered was not investigated, leaving a question regarding the limits of the proposed framework.
-Table I lists average error distances but does not show errors. Errors should be computed (e.g., standard deviations) and added.
-In general, this is an application paper –the system components are not novel, and no new theory is extracted. The evaluation is extensive but lacks real-world performance.

Recommendation
Overall, this is an interesting paper making an algorithmic system design contribution accompanied by an extensive empirical evaluation. While the system components are not novel per se, the proposed system provides a solid paradigm for occlusion-aware motion planning. A significant limitation is the lack of real-world experiments to evaluate the transfer of the approach to real-world situations including complex human-robot interactions.",,,,Agreement accepted,07/15 15:00,07/15 17:00,,https://youtu.be/X58KBq4PisY,,,51.0,kbLDpcTslGw
"Self-reconfigurable modular robots are composed of many modules that can be rearranged into various structures with respect to different activities and tasks. The variable topology truss (VTT) is a class of modular truss robot. These robots are able to change their shape by not only controlling joint positions which is similar to robots with fixed morphologies, but also reconfiguring the connections among modules in order to change their morphologies. Motion planning for VTT robots is difficult due to their non-fixed morphologies, high-dimensionality, potential for self-collision, and complex motion constraints. In this paper, a new motion planning algorithm to dramatically alter the structure of a VTT is presented, as well as some comparative tests to show its effectiveness.",RSS2020_Author_Agreement.pdf,"[Chao Liu](https://www.seas.upenn.edu/~chaoliu/)Sencheng Yu,  [Mark Yim](https://www.seas.upenn.edu/directory/profile.php?ID=107)",Chao Liu (University of Pennsylvania)*; Sencheng Yu (University of Pennsylvania); Mark Yim (University of Pennsylvania),RSS2020_Author_Agreement.pdf (108442 bytes); main.pdf (5999689 bytes),main.pdf,1138.0,52.0,,2.0,Motion Planning for Variable Topology Truss Modular Robot,https://www.modlabupenn.org/2020/06/03/motion-planning-for-variable-topology-truss-modular-robot/,0.161629145282335,"This paper addresses the problem of sampling-based motion planning for a parallel robot called the Variable Topology Truss (VTT). VTTs have edge modules, each with an active prismatic joint and passive joint ends. The robot configuration is defined by the edge lengths and node joint assignments. Reconfiguration motions are of two types: geometrical (changing of edge lengths) and topological (changing node connectivity through Split and Merge actions). 

The main challenge is that the search space is much larger than the reachable space. This is because motions are subject to the following constraints: the VTT has to be rigid, its nodes must be of degree three for controllability, and 18 members are the minimum.

The authors propose an extension of [12] where the search space is reduced by computing first the reachable C-Space for a single node and this space is decomposed into convex polygons. Here, the same method is used, but instead of dealing with a single node, pairs of nodes are considered to compute the reachable C-Space. This significantly accelerates the time to solve the problem in comparison to [12]. Also, the topology reconfiguration presented enables the algorithm to deal with harder problems.

The problem addressed in this paper is relevant to the community, the approach presented is generally sound and the paper is well organized. The way the search space reduction is achieved is insightful, and although the extension to [12] only considers pairs of nodes the speedup achieved is significant. Moreover, the topology

I recommend the authors to be more clear on the following aspects of their work:
- In the introduction, make a clear list of contributions and, in particular, explicitly state the similitudes and differences with respect to [12].
- try to find a way to better present Figure 6. A close-up to the nodes in the middle could help, also explain better why they are different.
- please improve section IV.B, it is currently a bit hard to parse.
- in the experiments, emphasize better what is possible now with the topological reconfiguration presented that the approach in [12] was not able to handle.

Also, please do a thorough proof read of the paper since there are several scattered typos.",Agreement accepted,"The paper presents a method for motion planning for reconfigurable truss structures where nodes can connect and disconnect (this idea is interesting and related to RSS).

Sampling based motion planning sampling and collision checking is defined in terms of the shape of the current truss to speed the algorithm. This is a great idea, however it was first presented in [18], the current paper extends this idea to multiple nodes, which is a fair contribution.

Results are generated for simulated problems using OMPL. This shows proof of concept, but nothing more.


The idea is relevant, the paper is easy to understand.

The experiments that are performed are in simulation. This is not inherently bad, but given the ease of performing simulations, it is expected that the new approach should be both:
(1) compared to the existing state of the art. 
(2) repeated trials performed and statistics presented. 


1) There are many interesting ideas shown in the paper, but how they improve the state of the art is not well explained. Comparison to existing methods would improve the paper. For example showing how each of the improvements presented in the paper affects runtime and number of samples required for a solution, on average, for different problems. 


2) Given that RRT uses randomness, it would have been nice to see how the proposed improvements affect the runtime by exploring tests for all combinations of: 
{naive, smart} x {collision checking, point sampling}
and then values reported for the mean time and number of samples required to find a solution. 
Right now, there is no evidence that the algorithm can be expected to perform well in general, only that one or two problem instances exists for which the algorithms worked well one time.


3) Related work exists that was not cited:

Komendera, Erik, and Nikolaus Correll. ""Precise assembly of 3D truss structures using MLE-based error prediction and correction."" The International Journal of Robotics Research 34.13 (2015): 1622-1644.

More discussion of the difficulty of extending the methods of [18] to the current manuscript is also necessary to justify that the new manuscript is sufficiently novel with respect to the existing state of the art. For example, how is extending collision checking of a single node to multiple nodes challenging or how are special aspects of the problem leveraged to do this well? What breakthroughs were necessary to achieve this result?
",Agreement accepted,"The paper is extremely well written and it was very exciting to read about such great work when presented with as much clarity as in this paper.

My major comments are related to section 6 (Test Scenarios):
1) If possible, could the authors address the problem of figuring out which nodes need to be moved (and how), if only the start and goal node configurations are known, without an accompanying one-to-one correspondence of start and goal for each node? In section 6.A the authors assume knowledge about the start and goal configuration of the 4 nodes that need to be moved. If instead they were given unlabeled start and goal configurations of the VTT, could their method be used to solve the planning problem?
2) What is the effect, on computational times and overall success rates, of randomly selecting the node groupings for geometry reconfiguration? For a reconfiguration problem that involves a sizable number of nodes, there can be a very large number of such groupings and randomly selecting could need a lot of time before a successful grouping is found.

Other comments are mostly concerned with getting a better understanding of the test:
1) In Section 1 (and elsewhere), the authors point to prior work to say a VTT needs 18 members for topology reconfiguration. It would be helpful if they could provide intuition about why this is so in text or with a figure.
2) In Section 3, the concept of the state of every member A^v(q^v) was a little tricky to understand. Am I correct in understanding that given a q^v, you could have different sets of member states (lengths) that achieve the same q^v? I was thinking of A^v(q^v) as a vector of lengths of the edge members connected to node v. Is that so?
3) Minor typo in Section 4.A before equation (3) - inconsistency in notation between \mathcal{O} and \hat{\mathcal{O}}.
4) In the same section, when two nodes v^i and v^j are connected by an edge member, are their respective obstacle spaces still computed by ignoring the members of the other as Figure 6 suggests? How is the edge joining the two nodes handled in this case?
5) In Section 6.B, the authors present three invariants they try to maintain during path planning for a group of nodes. With regards to claim about the last two invariants being hard to compute, if the motions of the two nodes are known, could they not compute the volume swept by an edge member across those motions and check for intersections between comparable triangles in spacetime (comparable in terms of the initial and final times of the motion)?
6) In Section 5.A I did not understand the claim about each polygon in C^v_{obs} being connected to only two enclosed subspaces.
7) If I could suggest one major change (addition) to the paper, it would be that a pictorial representation of the graph from Section 5.B be included.

I think this paper presents great work in a fantastic way and I strongly recommend this paper for acceptance.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/modlab-upenn/tether-tube_robots,https://youtu.be/u-1wvAqafHk,,,52.0,11A8w00BRv4
"Reinforcement learning provides a general framework for learning robotic skills while minimizing engineering effort. However, most reinforcement learning algorithms assume that a well-designed reward function is provided, and learn a single behavior for that single reward function. Such reward functions can be difficult to design in practice. Can we instead develop efficient reinforcement learning methods that acquire diverse skills without any reward function, and then re-purpose these skills for downstream tasks? In this paper, we demonstrate that a recently proposed unsupervised skill discovery algorithm can be extended into an efficient off-policy method, making it suitable for performing unsupervised reinforcement learning in the real world. Firstly, we show that our proposed algorithm provides substantial improvement in learning efficiency, making reward-free real-world training feasible. Secondly, we move beyond the simulation environments and evaluate the algorithm on real physical hardware. On quadrupeds, we observe that locomotion skills with diverse gaits and different orientations emerge without any rewards or demonstrations. We also demonstrate that the learnt skills can be composed using model predictive control for goal-oriented navigation, without any additional training.",RSS2020_Author_Agreement.pdf,"[Archit Sharma](https://architsharma97.github.io/), [Michael Ahn](http://), [Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/), [Vikash Kumar](https://vikashplus.github.io/), [Karol Hausman](https://karolhausman.github.io/), [Shixiang Gu](https://sites.google.com/view/gugurus/home)",Archit Sharma (Google)*; Michael Ahn (Google); Sergey Levine (Google); Vikash Kumar (Google); Karol Hausman (Google Brain); Shixiang Gu (Google Brain),camera_ready.pdf (8761868 bytes); RSS2020_Author_Agreement.pdf (197128 bytes),camera_ready.pdf,1282.0,53.0,,2.0,Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning,https://sites.google.com/view/dads-skill,0.05733375074890901,"For various reasons one might wish to learn generic (non-task specific) skills that allow a robot to exert control over its environment without optimizing a particular task. Several frameworks have been proposed, including the ‘DADS’ framework (dynamics aware discovery of skills). In this paper, an off-policy version fo the DADS algorithms is proposed, thus allowing for experience replay (increasing the sample efficiency). This increased sample efficiency allows using the modified algorithm (named Off-DADS) on real robot systems. 

Technical quality:
The proposed method is developed starting from general principles, with the authors clearly indicating the steps and where approximations or assumptions have been made. The efficacy of the method is indicated by two types of experiments: first, the method is evaluated on several simulated robotics systems and compared to two benchmarks (on-policy DADS and off-DADS where the dynamics were trained on-policy). Second, a proof of concept on the real robot is provided using the most promising settings. On the real robot, no comparison was performed. The real-robot version was also evaluated on a downstream model-based control task, although reported results for this evaluation are largely qualitative. 

Clarity:

The paper is mostly clearly written. There are a number of minor grammar issues that could be improved (some examples given below under minor points). These do not impact the understandability of the paper. 

Significance, Relevance, and Novelty:

The topic of skill discovery is relevant for robotics, as it could be an enabling technology to allow (reinforcement) learning method on larger-scale robotics systems. Algorithmically, the method is somewhat novel, although mostly consisting of elements seen in other algorithms (DADS with biased off-policy corrections). As far as I know the original DADS algorithm has not been shown to  work reasonably on real-robots systems, so the proof of concepts that off-Dads can work offers new insight to practitioners in the robotics field. 
",Agreement accepted,"Although this paper is somewhat incremental, having skill acquisition work on a real robot is a substantial achievement in its own right, and mitigating the sample complexity of these methods is of major concern, so the topic is well chosen. I am therefore mildly positive about the paper, though I have the following concerns.

One is that the comparisons are lacking to non-reward-driven skill discovery methods introduced subsequent to DADS, e.g. Deep Covering Options by Jinnai. Another is that several others have done skill acquisition on a real robot but are not covered (Riano, Hart, Kompella, for example), and of these I believe Hart and Kompella used intrinsic rewards. This line of work also does not appropriately cite or compare to the work of Georg Martius, who has been studying this question for more than a decade. 

Finally, it's not clear exactly what  the acquired skills /are/. There's a finite set of them, Z, described as a ""skill space"". So we are searching in ""skill space"", but it's not clear what that means. My reading of it is that they last for a single timestep, but are functions of state, so they are single timestep policies, in which case the agent is really learning |Z| discrete actions. Is that right? It would really help here to engage with the options framework - which, again, has been studying this question for two decades, and which provides a standardized and well-accepted vocabulary for talking about when a skill terminates, when it can be run, etc. For example, the q_phi function is an option model. Phrasing the work in terms of these well-known formalisms just makes everything much clearer and easier to understand.",,"Originality
===========
Making DADS off-policy is a logical step. Also using importance sampling corrections for that is pretty standard. The simplification is interesting.

Quality
=======
The derivation is sound, the experiments are convincing.

Clarity
=======
The paper reads very well.

Significance
============
A significant step towards making emergent RL applicable to real world robotics.

Comments:
=========
- One group of papers I was missing in the references is from the people around Ralf Der and Georg Martius who have published extensively on curiosity driven learning.

- Sect. IV.: The big underlying assumption is the stationary state distribution. In the real world experiments the parametrization as state differences seems like a way to ensure that this holds. How true is this assumption for the other considered experiments given the trial length etc.? (The approach seems to work well in practice, but would nevertheless interesting)

- Sect. V.B: It is not entirely clear what the 20 hours contain. Data collection: if we have one robot walking around for 1 hour and the other for 2 hours, would that be 3 hours or 2 hours? Split between data collection time and additional time for computations?

- I'd float Fig. 3&6 to the top of the page. The stray 1-2 lines of text above them break the reading flow.
",Agreement accepted,,,,07/15 15:00,07/15 17:00,,https://www.youtube.com/watch?v=b7oJSxujWIM,,,53.0,BEFHvOUCx60
"The successful application of general reinforcement learning algorithms to real-world robotics applications is often limited by their high data requirements. We introduce Regularized Hierarchical Policy Optimization (RHPO) to improve data-efficiency for domains with multiple dominant tasks and ultimately reduce required platform time. To this end, we employ compositional inductive biases on multiple levels and corresponding mechanisms for sharing off-policy transition data across low-level controllers and tasks as well as scheduling of tasks. The presented algorithm enables stable and fast learning for complex, real-world domains in the parallel multitask and sequential transfer case. We show that the investigated types of hierarchy enable positive transfer while partially mitigating negative interference and evaluate the benefits of additional incentives for efficient, compositional task solutions in single task domains. Finally, we demonstrate substantial data-efficiency and final performance gains over competitive baselines in a week-long, physical robot stacking experiment. ",RSS2020_Author_Agreement (1).pdf,Markus Wulfmeier (DeepMind)*; Abbas Abdolmaleki (Google DeepMind); Roland Hafner (Google DeepMind); Jost Tobias Springenberg (DeepMind); Michael Neunert (Google DeepMind); Noah Siegel (DeepMind); Tim Hertweck (DeepMind); Thomas Lampe (DeepMind); Nicolas Heess (DeepMind); Martin Riedmiller (DeepMind),Markus Wulfmeier (DeepMind)*; Abbas Abdolmaleki (Google DeepMind); Roland Hafner (Google DeepMind); Jost Tobias Springenberg (DeepMind); Michael Neunert (Google DeepMind); Noah Siegel (DeepMind); Tim Hertweck (DeepMind); Thomas Lampe (DeepMind); Nicolas Heess (DeepMind); Martin Riedmiller (DeepMind),TMP_RHPO_reduced.pdf (8264721 bytes); RSS2020_Author_Agreement (1).pdf (38423 bytes),TMP_RHPO_reduced.pdf,89.0,54.0,,2.0,Compositional Transfer in Hierarchical Reinforcement Learning,https://sites.google.com/corp/view/rhpo,0.580548482665317,"The authors propose an interesting approach to HRL and present a well-thought out approach to the problem of compositionally learning skills and their sequencing. The preliminary and methods sections are well-written and descriptive of the approach and extensive simulation and ablation studies are done. The approach is promising and I look forward to future iterations of it.

Regarding the technical content of the paper, I have two comments that may improve the paper if addressed. First, the authors acknowledge that the number of components have to be specified externally and demonstrate robustness in the appendix. However, the tasks studied and tested on are all within distribution and I wonder how well the approach would work for out of distribution tasks or variations. For example, if the block sizes were to change or their physical properties altered, or if the task was to make pyramids instead of a vertical stack, or to balance a block on its edge as its side rests on another block, how well would the sub-policies learned generalize?

Second, the approach’s sample-efficiency is clearly demonstrated when significant sequential sub-tasks are required to achieve a specific over-all task (see Fig. 3 Fig. 4 lower panels). I am curious why the authors did not choose to benchmark their approaches against the 3 citations below rather than the monolithic SAC or SAC-Independent. Comparing policies with additional information (existence and number of components) to those without would expect to yield performance increases (which is great); however, it would not necessarily demonstrate state-of-the-art results. I understand that implementing other people’s work may prove a significant challenge, but it would be great if it were possible to see the relative performance of the approaches. I am also curious, in the cases were the policies do not asymptote to the same expected return, is the optimal solution actually found, or a refactoring/change of components may yield a better policy with higher expected return?

@inproceedings{kulkarni2016hierarchical,
  title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
  author={Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  booktitle={Advances in neural information processing systems},
  pages={3675--3683},
  year={2016}
}

@inproceedings{vezhnevets2017feudal,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3540--3549},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{nachum2018data,
  title={Data-efficient hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang Shane and Lee, Honglak and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3303--3313},
  year={2018}
}",Agreement accepted,"The paper proposes a hierarchical deep reinforcement learning method building on an EM-based policy optimization algorithm. The hierarchical policy consists of two levels, one regarded as low-level “reflexive” behavior and a high-level policy that activates the lower-level policies. The idea is to learn re-useable subskills, leading to increased robustness, data efficiency, and generalization abilities. The method is tested in sets of real-world and simulated task, including blocks world tasks and continuous control tasks involving a simulated humanoid. An ablation study completes the experimental evaluation.

The idea of decomposing skills hierarchically and reusing lower-level skills for multiple tasks is absolutely the right one! Divide and conquer and hierarchical decompositions are possibly the single most effective way of taming computation and for identifying and encoding relevant structure in complex and high-dimensional spaces. In this regard, the paper is absolutely on the right track.
There are, however, a number of aspects in the paper that leave room for improvement; these improvements would enable a better assessment of the technical contribution and the significance of the results. 

Description of the algorithm

One important goal of scientific papers must be replicability of the results. The details provided in the manuscript do not permit a clear understanding of the method. I would like to provide more specific pointers, but I feel the effort I must spend on connecting the dots would be too substantial. Agreed, it is impossible to describe all details given the page limit. However, the reader should be able to understand how the conceptual goals (e.g. hierarchy) are translated into algorithmic choices. It would be helpful to explain how features of the algorithm were chosen to realize a conceptual goal for the learning procedure. In a way, one might view the algorithmic choices as implicit hypotheses about the relationship between algorithmic structure and learning problem. In this view, the experimental results can validate the match between algorithm and problem. This would enable us to move towards a principled understanding of the design space. The paper in its current form simply describes one point in the design space and reports on the results.

The paper states that the proposed method is a “combination – and extension – of MPO [2] and SAC-U [38]”. A succinct description of the differences and extensions, including their purpose and justification, would be helpful.

Versions of these methods also serve as base lines for the experiments. From the paper, it is unclear if these methods have parameters/inputs/structure that are shared with the proposed method or not. It would be necessary to know this to be able to assess if the reported performance increase is a result of better parameter tuning or points to a principled gain. One way to get around this problem is to focus on ablation within the method to demonstrate that the hierarchical features are offering advantages. One idea might be to study if learning performance improves as the number of available low-level policies increases.

Description of the experiments

Replicability also requires a clear description of the experiments and the experimental setup. The paper omits many details that make it impossible to assess the significance of the results. The simulated multitask experiments (Section IV A) is insufficient to understand what the individual tasks are. How is perception handled? Does the robot have ground-truth information from the simulator? Similarly, Section IV B does not provide sufficient information to evaluate the experiment. The humanoid tasks are not described at all and the reader must refer to reference [49] to know what is going on. 

Problem complexity

The significance of the results to a large degree depends on the complexity of the example problems. One might define the complexity of the task as the complexity of directly programming the task. Given that the problems are not described in much detail, it is difficult for me to make this claim, but I feel that the tasks listed, for example in Section IV A, could be programmed with relative ease. Also, in the real-world experiments, it might be that the fiducial markers eliminate the complexity of perception while the simplicity of blocks world and the simple parallel jaw gripper eliminate most challenges in control, leaving behind a problem that can be solved relatively easily. I would claim that stacking blocks in a blocks world with fiducial markers has been solved in many introductory robotics courses. 
Of course, I understand the argument that the point here is not blocks world. The goal is to develop a deep RL architecture that is in principle able to leverage hierarchical policy decompositions. Blocks world is just one of the problems used to demonstrate the effectiveness of the approach. However, this effectiveness, in my view, can only be demonstrated in situations where the problem seems to be unsolvable (or solvable but with much reduced performance) otherwise, i.e. through direct deep RL or – and this is important – any other available method. In comparing to the state of the art, all of the state of the art must be considered. On might therefore ask in response to the results: are you overcoming the problems of deep RL with the proposed approach? Or are you really expanding the reach of available methods? I believe that to answer this question, one must, in this case, fairly compare to an engineered blocks world stacking/sorting/etc. method.

Again, I understand that a novel method, such as the one presented here, cannot and should not have to beat all existing approaches right out of the box. That would be asking for too much. Existing approaches might be hyper-engineered and optimized, and a novel method demonstrated a conceptual advance might not be. But the novel method must show promise of going beyond the state of the art, and it would be the task of the authors to argue that it could be. To do that, the algorithm must be described in a way that explains this potential (please refer to the section “Description of the algorithm” above). One possible promise lies in the transferability, in the generality of the learned sub-policies. The sequential transfer experiments in Section IV C could play that role, but I don’t really know what the tasks are in detail, which makes it difficult to assess the reported results. Ultimately, the proposed algorithm does not have to beat existing solutions along all possible dimensions. But there should be a dimension along which it can beat existing solutions. And this is the dimensions the authors should focus their experiments on.

Experimental results

Given all the points above, I find it very difficult to assess the experimental results and therefore the significance of the proposed method.
It seems like the experiments reported in Section IV D might be reversed by tuning parameters. The choice of a single parameter changes from non-performance to performance. No explanation is offered. It leaves me wondering if for a third set of parameter choices the baseline would outperform the proposed algorithm. 
The main question the experiments should answer is: do the low-level policies represent re-usable, i.e. transferrable, skills? A more systematic attempt to answer this question would have made this paper much stronger. This probably would have required an algorithm that includes components with alternative realizations. Different instantiations could then have been tested on different families of related and unrelated tasks to identify how hierarchy contributes to data-efficiency, robustness, and transfer learning. This could have delivered a fundamental advance in our understanding of how hierarchical deep RL policies can be learned. 
",,"This paper proposes a way to train a hierarchical policy with compositional structure from off-policy data and in a multi-task setting.
The proposed structure factorizes the policy into a high-level one, which depends on the task index and determines a discrete option, and a low-level one, which is conditioned on high-level options but independent of tasks.
To make training amenable, the paper proposes to first estimate a non-parametric policy as an intermediate goal from off-policy data (SAC-U learning), and then perform EM update to fit the target policy towards this goal.
Both stages adopt trust-region-like constraints to regularize policy update for a robust optimization. 
The results, experimenting with piling and cleaning blocks in both simulation and real world, demonstrate the effectiveness of proposed compositional hierarchy and policy shift constraints.

The clarity of the paper is okay. The authors are trying to condensate many things to respect the page limit, with referring to many previous works and a lengthy appendix.
I don't have much experience about specific techniques that are used as building blocks. Hence I didn't feel this is an easy read but was still able to get the main messages. 

The topic of improving data efficiency in robot learning is important and the idea of composing/transferring modular sub-task skills is not ground-breaking novel.
However, the algorithm is developed with many practical considerations, such as allowing for off-policy data, flexible policy form, trust-region constraints and fewer asynchronous actors. These make a potential for the algorithm to be used as a standard framework.
The experiments are thorough to me, especially the one-week long real robot experiment. The reviewer also appreciates the experiment details and assessment about the regularization sensitivity, which facilitate reproduction for ones having sufficient hardware. 

I roughly checked the derivations in the paper and appendix. Looked all right to me.

It is not clear for me to see the point made in introduction (Page 1, 4th parag): ""(3) switching between the execution of policies for different tasks within a single episode leads to effective exploration"". There seems no related discussion about the exploration effectiveness in the experiment section. 

The results on multitask learning and composing low-level policies to solve new task show different difficulty levels of sub-tasks. It would be great that the authors could discuss the potential of conducting non-uniform task sampling, e.g. in the context of curriculum learning.

Also, reporting statistics with 3 runs concerned me a little bit. I would suggest trying 5 or 10 random seeds, at least for the simulation experiments. 

Other minor issues:

1. Annotations in Fig. 2 and Appendix Fig. 10 & 11 are hardly readable.

2. Appendix, Page 1, last parag, ""where \mathcal{D}..."": should be \mathcal{T}.

",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,,,,54.0,n77-qYuzRBM
"Scalable robot learning from seamless human-robot interaction is critical if robots are to solve a multitude of tasks in the real world. Current approaches to imitation learning suffer from one of two drawbacks. On the one hand, they rely solely on off-policy human demonstration, which in some cases leads to a mismatch in train-test distribution. On the other, they burden the human to label every state the learner visits, rendering it impractical in many applications. We argue that learning interactively from <i>expert interventions</i> enjoys the best of both worlds. Our key insight is that any amount of expert feedback, whether by intervention or non-intervention, provides information about the quality of the current state, the optimality of the action, or both. We formalize this as a constraint on the learner's value function, which we can efficiently learn using no regret, online learning techniques. We call our approach Expert Intervention Learning (EIL), and evaluate it on a real and simulated driving task with a human expert, where it learns collision avoidance from scratch with just a few hundred samples (about one minute) of expert control.",supplementary.pdf,"[Jonathan Spencer](https://jspencer12.github.io) [Sanjiban Choudhury](http://www.sanjibanchoudhury.com)Matt Barnes,  [Matthew Schmittle](https://www.mattschmittle.com) [Mung Chiang](http://edgelab.princeton.edu) [Peter Ramadge](https://ee.princeton.edu/people/peter-j-ramadge) [Siddhartha Srinivasa](https://goodrobot.ai/)",Jonathan Spencer (Princeton University)*; Sanjiban Choudhury (University of Washington); Matt Barnes (University of Washington); Matthew Schmittle (University of Washington); Mung Chiang (Princeton University); Peter Ramadge (Princeton); Siddhartha Srinivasa (University of Washington),supplementary.pdf (186824 bytes); spencer2020learning.pdf (1697180 bytes),spencer2020learning.pdf,1306.0,55.0,,2.0,Learning from Interventions: Human-robot interaction as both explicit and implicit feedback,,0.8320810672180571,"The paper introduces an imitation learning framework that the agent learns from the events of expert intervention in the control process. There are three different cases in this formulation (1) the human does not intervene, thus the state-action pairs are labeled ""good"", (2) the human intervenes and takes the control, thus several state-action pairs before and after the event of the intervention of the human expert are labeled ""bad"", and (3) the state-action pairs given by the human expert are labeled ""good"", and the agent is forced to value the actions by the human demonstration higher than any other action for the given states. 

It is not clear if the objective function shown in equation 10 can be applied in general to different problems. More specifically, the intervention term is problematic in case a human expert choose different actions for the same state. Let us assume that we can choose to move to the right or to the left to pass an obstacle. However, if the expert randomly chooses to move to the left or the right, then the objective function in equation 9 will not work since the training data samples are not consistent anymore. For one sample, Q(s, a1) > Q(s, a2) and the other sample, Q(s, a2) > Q(s,a1) To me, this problem also exists in the case of HG-Dagger but to a lesser degree.  

The term ""average distribution"" in the problem formulation section sounds odd to me. What do you mean by that? I guess you can use ""distribution"" instead of ""average distribution"". Alternatively, you may mean likelihood? ",Agreement accepted,"This paper motivates to reduce the burden of human demonstrators in a scenario that autonomous car learns from human driving behavior.
The idea is to not only learning from human corrections but also reinforce the behaviors that humans choose not to intervene.
It is expected such an agent can learn satisfactory policies with less human labeled data, improving the data efficiency comparing to DAGGER.
By relaxing the constraints scoring good and bad agent behaviors, the paper also proves a performance guarantee similar to DAGGER, that the algorithm is no-regret in aggregating online data.
The algorithm is validated by demonstrating car driving in both simulated and real wheeled robots. The simulation results clearly outperform baselines including naive behavior cloning, DAGGER and HG-DAGGER, an approach only considering the human intervention data.

The paper is well written and I enjoyed the read in general. The motivation and idea of taking non-intervention as implicit labels make a lot of sense. 
The performance guarantee is a great contribution for works on learning-based systems, although I only roughly went through the derivation which looked good to me.

One thing I think the paper may discuss a bit more is the effect of horizon for flagging bad or good behaviors. How will the algorithm perform if the parameter was not chosen in a good way such that a fraction of trajectory data is mislabeled? Will this be detrimental to the learning performance.
Is there anyway we may adapt this according to the experts' tendency of intervention?

Also, the results of EIL consistently outperformed HG-DAGGER in the simulation, while required a few more expert data in the real MuSHR robot experiment. I get the explanation that HG-DAGGER is only learning from biased recovery trajectories.
To me, it seems the goal of minimize trajectory jerkiness is implicit so experts of HG-DAGGER were not actively demonstrating it. As a result, per the goal of collision avoidance, both EIL and HG-DAGGER managed to learn eventually while HG-DAGGER excelled a bit because EIL is also considering the goal of keeping driving straight.
This sounds a bit like multi-task learning where an agent needs to account for multiple goals. It would be great if the paper could also discuss a bit more about this experiment observation.

Some minor issues:
1. The second reference is blank.

2. Last paragraph of Section II: duplicated ""used to"".",Agreement accepted,"This paper is built on the insight that when supervising a learning agent, both the actions and the in-actions of the expert communicate an evaluation of the learner’s current performance. Based on this insight, authors take an imitation learning approach to build an interactive algorithm that can teach robots to interact in the world.
The proposed approach allows an expert to take control over a robot interacting in the world at anytime. Based on the timing of the start and end of the take-over, authors cut each trajectory into a “good enough” part (to be conserved), a “bad” part (to be avoided) and an “intervention” part (to replicate). Authors present a way to interpret these states and trajectories and formally show that their approach lead to near optimal behavior. Finally, authors evaluate their approach against others both in simulation in a real experimentation.

I believe the work of the authors is interesting, and I appreciate the concept of “good enough” behavior, even if in the context of self-driving, “good enough” has actually a fairly high bar. Authors’ approach does build knowledge for the community, for example by building on HG-DAGGER to make a better use of the human presence. 

I would like to emphasize nevertheless that the “key insight” of the authors (the importance of implicit feedback) is not new in the Interactive Machine Learning (IML) community, and that many researchers in the Interactive RL, classic AI and HRI communities have developed a significant body of work on the use of implicit feedback, and even explored questions that the authors mention for future work. Thus, the statement: “our approach is novel in that it makes use of both explicit and implicit feedback in a human-gated mixed control setting.” might be incorrect. (As recommended by a few journals, I would suggest authors to refrain from claiming that something they present is novel.)
I would suggest authors to explore the IML literature [1,2], and report how implicit feedback is used in the AI community. Here is also some work making similar assumptions (while using a different learning mechanism), and exploring some of the issues authors mentioned in their future work:
- Human-gated mixed control for learning [3]: Informing the expert about the current intentions and giving them opportunities to intervene to learn quickly in high dimensional social environment.
- Impacts of using implicit feedback [4]: Teaching strategies and how to interpret different types of feedback vary between persons.  (Authors do explore this aspect a bit in simulation, but [4] actually models the expert’s policy to improve learning.)
- Non-stationary human evaluation [5]: Human evaluations depend on the current performance: e.g. for the driving task, a bad strategy could be at risk in an area where a good strategy would be fine. Additionally, something good enough for the time being, might be considered bad later on. (Authors quickly mentioned this in their future work too, but similarly to [4] authors in [5] actually model this and take it into account.)

I would suggest authors to complement their related work to highlight some of the important considerations when bringing the human inside the learning loop. This brings the learning online and in the real world, and the expert serves both as a safety mechanism and an oracle to provide a target to follow. Both roles can have serious implications on the interaction and the learning process, especially as humans are known to have large variability. Authors could mention some of these challenges and special considerations.

Approach:
The approach seems sound, novel and useful. However, some assumptions are quite strong, and would probably fail when used with real humans. For example: “3) As soon as the robot departs G , the expert takes over and controls the system back to G.” would probably not happens in a real setting as G is non-stationary and humans can be inconsistent. Similarly, α L and α E setting can actually be important and may vary between experts. These important variations might reduce the performance in a real-world setting (despite theoretical guarantees).

Evaluation:
I appreciate that the authors evaluate both in simulation and in a real example. The additional evaluation of the impact of the boundary (and by extension the frequency of intervention) also provides interesting insight on the situation. 
I have some concerns about the evaluation though. First, the units are unclear, what does query correspond to? Is it a sample where the expert provided a value, or a full trajectory? (cf. other comments.) For example, the simulation seems to show that even learning to drive in the hallway is complex, EIL takes more that 75 queries to learn to drive straight (which seems more than the number required to make a right turn with EIL). I believe authors could comment on this. 
The experimental evaluation is interesting, but it seems that the comparison to HG-DAGGER might be a bit unfair. For example 50% less samples and 30% less expert demonstrations are used compared to EIL. While I understand that some properties would not improve over time, this imbalance of training samples should be addressed (fixed or discussed more explicitly).

Discussion:
As mentioned earlier, these systems relying on humans create a number of real-world implications that need to be addressed. For example, humans cannot be constantly attentive to the current robot behavior, especially in situations where one or two seconds can have important impact (cf. crashes of supervised “self-”driving cars). While I would not expect the authors to address all the challenges of using humans as real-time safety mechanisms and all the possible variations in human teaching strategies, I believe it is important for authors to be aware of it, and maybe mention some of these inherent consequences of learning from real-time human interventions.

Overall:
Overall, I believe this paper is interesting and does push the state of the art. By refining the claims, integrating the paper is the larger body of work of Interactive Machine Learning and discussing more the limits of the current approach and its assumptions, I believe this paper could be a good contribution.

Other comments:
- p1: I believe authors confuse Interactive Machine Learning and Active Machine Learning in “While interactive learning addresses the distribution mismatch problem […] the learner needlessly queries the expert in states that the expert, and ideally a good learner, would never visit.” This is specific of Active Learning, on the contrary, Interactive Learning aims to give power to the expert to limit unnecessary requests.
- p2: As authors refer a lot to DAGGER (>30 times), they could describe with more details this approach.
- p3: “EIL does not require the expert to label every state the learner enters.” This is true, however the expert still has to observe all the learning process, in real-time, which can be fairly time consuming and still requires constant attention.
- p4: “It is relatively straightforward for the expert to specify α L upon looking back at the data.” if data has to be retrospectively analyzed, it’s not online anymore. Similarly,  α E can be hard to estimate.
- Algorithm 1: Linebreak probably missing in “forreturn”
- p3: Please precise that Q in cost in this context.
- Fig 5: It might be useful to synchronize all the x axes (for example from 0 to 600)
- p7-8: Authors use the terms samples, iterations, trajectories without providing explicit definitions (and ways to transform one into another), this could be clarified.
- References: [2] is missing, other have missing dates or capitalization issues (uav and other).

[1] Fails, Jerry Alan, and Dan R. Olsen Jr. ""Interactive machine learning."" Proceedings of the 8th international conference on Intelligent user interfaces. 2003.
[2] Amershi, Saleema, et al. ""Power to the people: The role of humans in interactive machine learning."" Ai Magazine 35.4 (2014): 105-120.
[3] Senft, Emmanuel, et al. ""Teaching robots social autonomy from in situ human guidance."" Science Robotics 4.35 (2019).
[4] Loftin, Robert, et al. ""Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning."" Autonomous agents and multi-agent systems 30.1 (2016): 30-59.
[5] MacGlashan, James, et al. ""Interactive learning from policy-dependent human feedback."" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. Org, 2017.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,,,,55.0,NjkcgB-yy0w
"Whether in factory or household scenarios, rhythmic movements play a crucial role in many daily-life tasks. In this paper we propose a Fourier movement primitive (FMP) representation to learn such type of skills from human demonstrations. Our approach takes inspiration from the probabilistic movement primitives (ProMP) framework, and is grounded in signal processing theory through the Fourier transform. It works with minimal preprocessing, as it does not require demonstration alignment nor finding the frequency of demonstrated signals. Additionally, it does not entail the careful choice/parameterization of basis functions, that typically occurs in most forms of movement primitive representations. Indeed, its basis functions are the Fourier series, which can approximate any periodic signal. This makes FMP an excellent choice for tasks that involve a superposition of different frequencies. We show that it is successful for tasks that involve a superposition of different frequencies.Finally, FMP shows interesting extrapolation capabilities as the system has the property of smoothly returning back to the demonstrations (e.g. the limit cycle) when faced with a completely new situation, being safe for real-world robotic tasks. We validate FMP in several experimental cases with real-world data from polishing and 8-letter tasks as well as on a 7-DoF, torque-controlled, Panda robot.",author_agreement_form.pdf,Thibaut Kulak (Idiap Research Institute)*; Joao Silverio (Idiap Research Institute); Sylvain Calinon (Idiap Research Institute),Thibaut Kulak (Idiap Research Institute)*; Joao Silverio (Idiap Research Institute); Sylvain Calinon (Idiap Research Institute),author_agreement_form.pdf (437122 bytes); fmp.pdf (4190295 bytes),fmp.pdf,1160.0,56.0,,2.0,Fourier movement primitives: an approach for learning rhythmic robot skills from demonstrations,https://sites.google.com/view/fourier-movement-primitives,0.035693830529693,"The suggested approach to represent complex periodic motion as a FMP clearly has some appeal. The motivation at the beginning of the paper, i.e., that periodic motion is hard to program, and thus requires  a more complex LfD approach is a bit overstated, as it is really easy to create periodic motion with a  couple of sine-waves, and also to create wiping patterns this way -- the evaluations of the paper do not make it clear that there any use to represent the distribution of the demonstrations. 

The technical development is easy to follow. What puzzles me a bit is the very high dimensional mixture of Gaussians, which should be  numerically quite brittle. Nothing is mentioned how the covariances are initialized. The paper mentions that Gaussians hardly overlap. But then, what is the purpose of the mixture model. Does the mixture actually contribute, or would one single Gaussian be sufficient? The covariance appears to be full matrices -- is this necessary? Numerically definitely not the best. This mixture modeling component is technically the least convincing.

The evaluations are useful to demonstrate and compare the approach. The improvement of ProMP are a good effort to keep the comparisons reasonable. Still I would be  curious about the numerical robustness, at which length of the number of samples in a segment  the mixture model becomes problematic, and also whether the mixture model is actually contributing anything. Also info about the parameter settings of the mixture model is missing. 

The panda wiping example is nice, but such a wiping task can be easily programmed in a different way. Are there any tasks where it is actually useful/important to have probabilistic representation, rather than only the mean?


some details:

Page 2, first column, last sentences: not clear what is really stated here, and a bit without context.

Page 4, top: ""tracking"" -- it is not quite clear what tracking is meant here. I assume a desired signal is tracked.

Page 5, bottom: hyperparameter h: isn't there an easy way to choose to have some overlap of the Gaussian for smooth interpolation? To what number is h actually set?

20Hz data is a very coarse sampling, only for motion that has low frequency.",Agreement accepted,"The paper is relatively concise and to the point, but the language is not very high level, example: drawing of the number 8, often referred to as figure 8, is called many things in the paper, including LETTER 8, 8-shape, etc. Also, something is IN figure, not on figure. Ok, these are minor things, but a language level-up is needed.
The paper proposes a relatively complex process of encoding the data, which is explained in a rather complex way, even though it is summarized in the Algorithms. Anyway, it is not at all intuitive, as the sequence of the required operations is, well, complex.
The results show the desired behavior, but some sentences from the start, saying that other approaches “trongly limiting the applicability in cases of varying amplitude, frequency and phase”, are not trues, as complex shapes and modulations are very much attainable with dynamical systems. It is true, that the intended representation (FMP) is for multiple demonstrations. In the results, however, figures do not really show any complex trajectories or extensive modulations. Starting from a different starting point is especially suited for dynamical systems. Figs 5 and 8 are not well explained. Figure 10 does not make sense, really, if the data for learning is spread out (b), the result is just some noise (well, it is not well elaborated).
In the end it comes down to whether this is “only” yet another variation of DMPs, or ProMPs, which it is, but I think it is sufficiently different, to make a contribution, even though it is not the most intuitive solution.
Literature review is quite thorough, even though there are some not cited, relevant papers: Adaptation and coaching of periodic motion primitives through physical and visual interaction, Gams, Petrič, Do, Nemec, Morimoto, Asfour, Ude, Robotics and Autonomous Systems 75 (2016) 340–351
This is the paper that sums most of “original” DMPs: . J. Ijspeert, J. Nakanishi, H. Hoffmann, P. Pastor, and S. Schaal,“Dynamical movement primitives: Learning attractor models for motorbehaviors,”Neural Computations, vol. 25, no. 2, pp. 328–373, 2013
",,"Overall the FMPs seem like an interesting approach to rhythmic skills. However, the paper focuses a lot of time showing their superiority over a specific type of ProMP on a rather limited domain. It would be useful to show more examples of the power of FMPs as well as some of their limitations. 

The use of multiple (low-dimensional) Gaussians should be better motivated and explained. Is there an intuition for using the multiple Gaussians? For the ProMPs it is because the trajectories have different shapes based on the starting location. Is this also the case for the Fourier primitives or what are they capturing?

A comparison to a rhythmic DMP would be useful. A lot of the illustrated issues with the ProMP seem to stem from the conditioning on the starting point. R-DMPs with a Gaussian distribution over the weights can be used to learn a figure eight and then use the spring and damper system to move to it smoothly. 

The main benefit of the FMPs seems to be that they can recreate the demo distributions. More emphasis on how the primitives generalize would be useful. The experiments emphasize reproducing the demonstrated distributions, but primitives should be able to generalize more broadly. Do the FMPs capture correlations in frequency and phase? Or allow for varying speeds over the long demonstrations? Can that be demonstrated? How well do the FMPs handle errors in the demonstrations, i.e., non-periodic motions in the middle? Or non-smooth rhythmic trajectories?

In addition to LFD, motion primitives are often used with RL. Having more local basis functions can often be useful for creating local variations when learning the policy. The authors should consider comparing the primitives on an RL task to show how well the FMPs can be learned. 
",,,,Agreement accepted,07/15 15:00,07/15 17:00,,https://sites.google.com/view/fourier-movement-primitives,,,56.0,Zi_Fb8_JHfs
"Publicly available satellite imagery can be an ubiquitous, cheap, and powerful tool for vehicle localisation when a prior sensor map is unavailable.However, satellite images are not directly comparable to data from ground range sensors because of their starkly different modalities.We present a learned metric localisation method that not only handles the modality difference, but is cheap to train, learning in a self-supervised fashion without metrically accurate ground truth.By evaluating across multiple real-world datasets, we demonstrate the robustness and versatility of our method for various sensor configurations. We pay particular attention to the use of millimetre wave radar, which, owing to its complex interaction with the scene and its immunity to weather and lighting, makes for a compelling and valuable use case. ",AGREEMENT.pdf,Tim Tang (University of Oxford)*; Daniele De Martini (University of Oxford); Shangzhe Wu (University of Oxford); Paul Newman (University of Oxford),Tim Tang (University of Oxford)*; Daniele De Martini (University of Oxford); Shangzhe Wu (University of Oxford); Paul Newman (University of Oxford),cr.pdf (3873795 bytes); AGREEMENT.pdf (432833 bytes),cr.pdf,15.0,57.0,,2.0,Self-Supervised Localisation between Range Sensors and Overhead Imagery,[Not Answered],0.756502028482137,"This paper addresses an important and interesting topic, and is very well written and clear. Overall it is a very good paper, but please find some comments here.

The related work section is generally clear and comprehensive. However, I would appreciate a discussion of the expected performance difference between the proposed method and the methods cited in II-A (e.g. 18,20) and II-D (35,42,43), in particular. In connection to references 23,25, it would also make sense to cite Parsley and Julier (""Towards the Exploitation of Prior Information in SLAM"", IROS 2010). In Section II-B it would make sense to also cite Mielle et al. (""The Auto-Complete Graph: Merging and Mutual Correction of Sensor and Prior Maps for SLAM"", MDPI Robotics 2019) in connection with references 37,7,38. 

Regarding the results shown in Fig 10, it is said that the robot is never ""getting lost"", but that is a vague term. In several cases, it seems that the position estimate is off by more than 10, or even more than 30, metres. Not hopelessly lost, perhaps, but certainly not correct. I think you should revise your statement and add some short discussion on this.

The paper does not mention training time, although it is indicated in the video. Please also discuss the amount of training needed in the paper.

In Sec IV, you describe how to find the rototranslation between the map and the live data, but as far as I could see, you do not mention scale. Do you assume to have accurate pixels-per-metre scale information in both modalities, and that the scale is uniformly correct? Please clarify or discuss this.

What is the significance of the parameter $n$ (number of rotations)? How have you selected it, and how does it affect the results?

There are some further places where clarifications might help:
1) In Figs 3-4, adding labels of what is A and B in the figure (not just the caption) would help.
2) The plots in Fig 10 could be clearer. E.g., make the larger (and maybe cut some of Fig 12) and/or make the lines thicker.

Minor edits/typos:
1) Fig. 4 caption: ""An loss"" -> ""a loss""
2) Sec IV-B, 3rd paragraph: ""two random image"" -> ""two random images""
",Agreement accepted,"The cross-modal (ground vs. satellite) data correlation approach in this paper appears to be original and useful, building off [36]; it adapts state-of-the-art neural network architectures to the cross-modality correlation problem by following a multi-stage approach in which rotation is first aligned, then translation alignment is performed with synthetic images that are rotation-aligned.  This is a key novel aspect of the paper.

It would be helpful perhaps to highlight which aspects of the Pose-Aware Separable Encoder Decoder CNN architecture (e.g. Figure 6) that the authors consider are most novel (beyond separating rotation/translation).  

The performance evaluation is fairly extensive, both quantitatively (Tables I, II, and II) and qualitatively (e.g. Figure 12) but I was expecting to also see precision-recall curves, to help build my intuition for how the technique performs, as key parameters are varied. For example (page 7, column 2): ""A large value of $d_{intro}$ indicates the generated images are erroneous.... our system falls back to using odometry for dead-reckoning when $d_{intro}$ exceeds a threshold."" -> what is the threshold, how does the system performance vary when that threshold is changed (ie too low vs. too high)?  Are there numerical values for key parameters that a researcher would need to know to replicate the results?  (Will a public implementation be made available?  I feel that there are some questions to try to reimplement this, such as how many layers in the encoder-decoder etc?)

The system only uses a single GPS pose at the start of the trajectory; in practice I wonder if its somewhat unrealistic to not use GPS in a real system; I think the question of how to robustly fuse many inputs including GPS in a such as a system is paramount.  Also, not making use of metrically accurate ground truth for training might be something that a practitioner might not do.

Overall I consider this an impressive system (but still a bit preliminary and would hope to see more details of the implementation in a longer version).
",Agreement accepted,"Section 1
- Can you add a citation in the second paragraph for the phrase ""Public overhead imagery such as satellite images...are readily available""?  It will help readers who may be unfamiliar with the state of art understand where the data is made available.
- The first line of the last paragraph does not seem grammatically correct:  ""To the best of our knowledge, this paper presents the first method to learn the cross-modality, metric localisation of a range sensor self-supervised.""  It may be clearer to say ""...metric localisation of a range sensor in a self-supervised fashion.""
- The presentation of the method in this section beginning with the paragraph ""Building on [36] we propose..."" would be more understandable by stating assumptions first (i.e., given a coarse initial pose estimate from place recognition) and then stating the approach (i.e., we generate a synthetic image with the appearance and observed scenes of a live range sensor image...and localize in a self-supervised fashion). 

Section 2
This section discusses many works that localize with overheads images and cross-modality localization, but the works that are discussed are not compared and contrasted with the authors methods.  It would be helpful if the authors discuss how does the proposed work improves upon the existing literature or why the methods proposed in existing literature are insufficient to solve the problem the authors pose.  In Sections 2C and 2D the authors begin comparing the proposed approach with existing literature but the paper would be strengthened if details were included that motivate the use of the selected techniques over others.

Section 3B-3C
The statement ""...generating synthetic images is a simpler and less ill-posed problem than directly regressing the pose..."" needs some context.  It seems that this statement is only valid if one assumes good initial pose estimate.  In general, how good must the initial pose estimate be?  
Figure 2 and the explanation of the figure are very clear.  

Section 4
It is stated that the translation offset gamma between B^1 and B^2 is known in the first sentence of paragraph 3 of part B.  Later the following is stated ""In other words, PASED discovers the translation offset between the two images passed as input to E_p..."".  More discussion about how gamma is computed is needed at this point.  Some discussion is provided in Section 5D but some additional questions are (1) For experiments prior to Section 5D is gamma estimated from place recognition? 
 (2) Is the use of gamma only required for experiments including and after Section 5D?

Section 5
Can you bold the best performing approaches in the tables? Also, what value of z is used to threshold ground point removal?  Many results are presented but there is very little analysis or discussion for the different approaches.  For example, the results in Table 1 demonstrates that the proposed approach outperforms the state-of-art approach only in terms of mean rotation error in the RoboCar dataset.  It outperforms the state-of-art approach only in y-error and y-pixel error for the MulRan approach. Is this meaningful?  Why?  Table 2 illustrates much better results for the RobotCar dataset as compared to the state-of-the-art. However, it seems to do worse for the KITTI dataset.  Do you have insights as to why this is the case? 

Figure 12 is useful but it seems rows 2-4 are redundant.  It may be a better use of space to eliminate these rows and include a discussion and analysis of the results.  For example, Figure 10 (left) seems significant because the proposed approach is able to correct for a large amount of drift, but this is not mentioned in the paper.  Overall, there are many results but the analysis of the results is lacking.  ",,,,Agreement accepted,07/15 15:00,07/15 17:00,[Not Answered],[Not Answered],,,57.0,ukyZFcgAhpc
"As the number of agents comprising a swarm increases, individual-agent-based control techniques for collective task completion become computationally intractable. We study a setting in which the agents move along the nodes of a graph, and the high-level task specifications for the swarm are expressed in a recently proposed language called graph temporal logic (GTL). By constraining the distribution of the swarm over the nodes of the graph, GTL specifies a wide range of properties, including safety, progress, and response. In contrast to the individual-agent-based control techniques, we develop an algorithm to control, in a decentralized and probabilistic manner, a collective property of the swarm: its density distribution. The algorithm, agnostic to the number of agents in the swarm, synthesizes a time-varying Markov chain modeling the time evolution of the density distribution of a swarm subject to GTL. We first formulate the synthesis of such a Markov chain as a mixed-integer nonlinear program (MINLP). Then, to address the intractability of MINLPs, we present an iterative scheme alternating between two relaxations of the MINLP: a linear program and a mixed-integer linear program. We evaluate the algorithm in several scenarios, including a rescue mission in a high-fidelity ROS-Gazebo simulation.",RSS2020_Author_Agreement.pdf,"Franck Djeumou, Zhe Xu, [Ufuk Topcu](http://www.ae.utexas.edu/facultysites/topcu/wiki/index.php/Main_Page)",Franck Djeumou (University of Texas at Austin)*; Zhe Xu (University of Texas at Austin); Ufuk Topcu (University of Texas at Austin),RSS2020_Author_Agreement.pdf (343052 bytes); RSS_Probabilistic_Density_Control_with_GTL_final.pdf (4447843 bytes),RSS_Probabilistic_Density_Control_with_GTL_final.pdf,1209.0,58.0,,2.0,Probabilistic Swarm Guidance Subject to Graph Temporal Logic Specifications,,0.528488049064845,"The authors consider a simplified swarming scenario, in which the agents move along the nodes of a graph. The specification is given as a formula in a logic called 
graph temporal logic (GTL), which constrains the distribution of the swarm over the nodes of
the graph. The authors propose an algorithm to control the density distribution of the swarm, which is based on 
a time-varying Markov chain
modeling the time evolution of the density distribution of the swarm. 

GTL and the Markov-chain modeling have been published before. The main contributions of this paper are to 
(1)  synthesize such a Markov
chain as a mixed-integer nonlinear program (MINLP), and (2) relax the MINLP to a mixed integer linear program (MILP).  My main concern with the paper is whether the contributions described above are enough to make this a RSS paper. My second major concerns is the simplicity of the examples throughout the paper. The running example us too simplistic, and the authors to not make any effort to connect this example to a robotic scenario. In the second example at the end of the paper, which is supposed to be more realistic, it is not cleat what the dynamics of the robot are.  

Aside from these, the paper is well written and, as far as I followed, technically correct. However, the related work misses three important and closely related works. First, there is no clear comparison with the plethora of papers on consensus. In the second paragraph of the Introduction, the author(s) say that ""as the number of agents comprising the swarm increases, the computational cost for assigning
the targets of each agent and generating all the optimal
trajectories one by one becomes prohibitively high. As a result,
these techniques suffer from scalability issues."" This is not the case for consensus-type papers, in which simple, identical local rules lead to guaranteed global behavior. 

Second, there are some papers on controlling the statistics of a swarm, e.g., (1) Calin Belta and Vijay Kumar, Abstraction and control for groups of robots, IEEE Transactions on Robotics, vol.20, no.5, pp.865-875, 2004, (2)  P. Yang, R. A. Freeman and K. M. Lynch, ""Multi-Agent Coordination by Decentralized Estimation and Control,"" in IEEE Transactions on Automatic Control, vol. 53, no. 11, pp. 2480-2496, Dec. 2008, which are not mentioned and discussed. 

Third, there exist spatial temporal logics, that seem to be related to the GTL logic proposed here, e.g., (1) L. Nenzi, L. Bortolussi, V. Ciancia, M. Loreti, M. Massink, Qualitative and Quantitative Monitoring of Spatio-Temporal Properties with SSTL, Logical Methods in Computer Science, vol. 14(4) (2018), (2) Iman Haghighi, Sadra Sadraddini, Calin Belta, Robotic Swarm Control from Spatio-Temporal Specifications, 55th IEEE Conference on Decision and Control, Las Vegas, NV, 2016 (3) Iman Haghighi, Austin Jones, Zhaodan Kong, Ezio Bartocci, Radu Grosu and Calin Belta, SpaTeL: A Novel Spatial-Temporal Logic and Its Applications to Networked Systems, Hybrid Systems: Computation and Control (HSCC) 2015

",Agreement accepted,"The paper is well written and structured.  The exposition, problem formulation, and technical approaches are clear.
The proposed technical approach is original and interesting.

The significance of the solutions is the weakness of the paper.  While the technical part is very strong, it is not clear why the proposed problem is important and impactful.
First, from an application point of view, it is not clear what new capabilities and functionality the proposed work enables.  This is compounded by the omission of recent work on swarm control from spatial and temporal logic specifications that addresses similar problems as the one in the paper, e.g., Counting LTL [1, 2, 3], swarm control with GR(1) [4, 5], and TSSL and SpaTel [6, 7, 8].  Most of your references are old.
Second, from a technical perspective, it is not clear why the formalized problem (as MINLP) is important, especially since it has such tractability issues.  It is not clear what properties the proposed distribution control problem (Markov chain synthesis) has that other approaches lack and are impactful in practice.  Scalability, expressivity of missions, decentralized and distributed operation are claimed by other approaches as well.

I would also suggest to perform comparisons between the performance of your framework against the other approaches from [1-8].  Comparing against general MINLP solvers is good, but not as insightful, because general solvers can never achieve good performance on large classes of problems (no free lunch theorem).  Your solution, on the other hand, is tailored to the swarm control problem.

[1] Y. E. Sahin, P. Nilsson, and N. Ozay, “Provably-correct coordination of large collections of agents with counting temporal logic constraints”, 8th ACM/IEEE International Conference on Cyber-Physical Systems (ICCPS), Pittsburgh, April 2017.

[2] Y. E. Sahin, P. Nilsson, and N. Ozay, “Synchronous and Asynchronous Multi-agent Coordination with cLTL+ Constraints”, Proc. 56th IEEE Conference on Decision and Control (CDC), Melbourne, Australia, December 2017.

[3] Y. E. Sahin, N. Ozay, and S. Tripakis, “Multi-Agent Coordination Subject to Counting Constraints: A Hierarchical Approach”, Proc. Int. Symp. on Distributed Autonomous Robotic Systems (DARS), Boulder, CO, October 2018.

[4] S. Moarref and Kress-Gazit, H., “Automated synthesis of decentralized controllers for robot swarms from high-level temporal logic specifications”, Autonomous Robots, 2019.

[5] J. Chen, Moarref, S., and Kress-Gazit, H., “Verifiable Control of Robotic Swarm from High-level Specifications”, in Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) , Stockholm, Sweden, 2018.

[6] Iman Haghighi, Sadra Sadraddini, Calin Belta, Robotic Swarm Control from Spatio-Temporal Specifications, 55th IEEE Conference on Decision and Control, Las Vegas, NV, 2016.

[7] Ezio Bartocci, Ebru Aydin Gol, Iman Haghighi, and Calin Belta, A Formal Methods Approach to Pattern Recognition and Synthesis in Reaction Diffusion Networks, IEEE Transactions on Control of Network Systems, Vol. 5, No. 1, pp. 308-320, March 2018.

[8] Iman Haghighi, Austin Jones, Zhaodan Kong, Ezio Bartocci, Radu Grosu and Calin Belta, SpaTeL: A Novel Spatial-Temporal Logic and Its Applications to Networked Systems, Hybrid Systems: Computation and Control (HSCC) 2015.
",,"This paper addresses the important problem of efficiently synthesizing
a decentralized controllers for a large number agents, i.e. a swarm,
such that together, they satisfy some high-level specification, here
in the form of a graph temporal logic sentence. Such systems have a
large number of applications, and by no means is this a solved
problem.

The prose and general outline of the paper is easy enough to follow,
although illustrations of the examples and theorems would have been
greatly appreciated.

The high level approach taken in this paper is 4 fold:

1. The swarm is ultimately abstracted as a graph where the nodes
   represent locations and densities (#agents/#total number of
   agents).

2. The decentralized controller is abstracted as a time varying markov
   chain describing how densities evolve on this graph over time.

3. The synthesis of this controller subject to the specification and
   dynamics is cast as a feasibility query in a Mixed Integer
   Non-linear Program (MINLP).

4. Once synthesized, an online low-level controller implements this
   strategy in the work space this graph represents.

This approach improves over prior work by either:

* Modeling densities rather than agents directly to improve
  scaling w.r.t. swarm size.

or 

* Synthesizing swarm strategies that satisfy bounded graph temporal
  logic specifications.

The primary technical contribution of this work seems to be the
handling of the complexity of the MINLP - being potentially much
harder than MILPs which are already NP-hard.

This is done by first observing that in the special case of a complete
graph, the constraints on densities can be used to determine a
feasible swarm strategy, resulting in a Mixed Integer *Linear*
Program.

While the proof seems straight forward, an interpretation would have
been appreciated. For example, physically, I understand this to be the
result of agents being able to always ""keep up"" with required
densities at each time step since they are always one step away from
there they need to be.

A general solution is then proposed which alternates between solving
two relaxations of the MINLP. The key insight is that once the swarm
strategy (markov chain) or densities are set, the complexity of the
MINLP reduces substantially. Thus at a high level, this algorithm
alternates between estimating these to quantities.

The paper then empirically demonstrates substantial improvements this
proposed solution has over using off-the-shelf MINLP solvers as well
we a few interesting case studies.

While I believe this approach to be well-founded and the experiments
convincing, I am left with a two questions:

1. While improving scalability, there doesn't seem any constraints
   ensuring that the Markov Chain densities can be realized - since in
   the actual swarm the densities cannot take on all real numbers.

   Is there a guarantee that the results will still hold with high
   probability?

2. Because the solutions need only be feasible, is it possible to
   leverage an SMT solver rather than optimization engine?  The
   non-linearities in the MINLP seem fairly tame, and the proposed
   algorithm reminds the reviewer of Counter Example Guided Inductive
   Synthesis based on SMT solvers.

   Similar works in synthesizing controllers from Signal Temporal
   Logic have shown that combining SMT solvers with MILPs can also
   lead to dramatic increases in performance.
   
",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/u-t-autonomous/RSS2020_SwarmControlGTL.git,https://www.dropbox.com/home/RSS%20Video,,,58.0,2pPzRb11lW4
"The effectiveness of Socially Assistive Robots (SAR) relies on their ability to motivate particular user behaviours, e.g. engagement with a task, requiring complex social interactions tailored to the needs and motivations of the user. Professionals from human-centred domains such as healthcare are experts in such interactions, but their ability to contribute to SAR development has traditionally been limited to the identification of applications and key design requirements. In this work we demonstrate how interactive machine learning offers a way for such experts to be involved at every stage of design and automation of a robot, as well as the value of taking this approach. We present a novel technical framework for in-situ, online interactive machine learning that can be used in ecologically-valid human robot interactions. Using this framework, we were able generate fully autonomous, appropriate and personalised robot behaviourin a high-dimensional application of assistive robotics.",RSS2020_Author_Agreement (1).pdf,Katie Winkle (Bristol Robotics Laboratory)*; Severin Lemaignan (); Praminda Caleb-Solly (); Paul Bremner (); Ailie Turton (University of the West of England); Ute Leonards (),Katie Winkle (Bristol Robotics Laboratory)*; Severin Lemaignan (); Praminda Caleb-Solly (); Paul Bremner (); Ailie Turton (University of the West of England); Ute Leonards (),_RSS_2020__Gym_Study (5).pdf (2155890 bytes); RSS2020_Author_Agreement (1).pdf (40987 bytes),_RSS_2020__Gym_Study (5).pdf,1151.0,59.0,,2.0,In-Situ Learning from a Domain Expert for Real World Socially Assistive Robot Deployment,[Not Answered],0.7676440390346341,"Positive: 1) creative way to learn via an expert-instructor; and the use of experts in the design process is novel;  2) robot clearly learned a different sequence of behaviors than the H condition as shown in the overall analysis; 3) paper is clearly written; 4) robot acted in autonomous condition successfully; 5) experiment in the wild over extended time is valuable; 6) overall working system is well done.
Negative: 1) We don’t learn whether the differences in the three types of robots mattered statistically to this user population; 2) There is no discussion of the actions lost or different in ML-A mode over ML-S mode (e.g. actions like get closer, sympathize); 
3) There is only detailed analysis of the learning process for 2 participants, but we don’t know how the robot performed with the other 7 participants;  4) while the authors suggest that the tailoring that the fitness instructor did mattered, there is only a small amount of anecdotal evidence of that; 5) the late reintroduction of the H condition in Phase 3 creates bias unless that re-introduction was counterbalanced (which is not mentioned--this could be fixed).  But because there is no user evaluation this is not really a major matter. 
Minor edits: on page 4 there is missing text in the figures; on page 2 there is discussion of robot actions based on user behavior, but we don't learn until page 4 that there are sensors attached to the user (as well as details).  A short sentence on page 2 would help to clarify before the full details on page 4.",Agreement accepted,"* The authors conduct a long term study (several months per participant) with several participants. There are very few studies involving robots that are as long term as theirs.
* The authors present an interesting approach that allows non-technical experts (but experts in the interested field) input rewards into the system.
*The algorithm was applied to a relevant and interesting field, as robots can help many people in motivating them during exercise.
*The paper was clearly written and easy to follow.

I have a couple of questions:
*Was the paper mainly intended as an interaction study, as a technical paper, or as a design paper? It was not clear where the main contribution was intended to be.
*Why did the authors use KNN over a perceptron?
*How did the authors get to table IV, and why was it different between participants? Why were the three phases designed as they were?
*My main qualms with the paper are that it does not clearly validate their proposed algorithm. How do you know that the actions chosen for participants are the correct ones? If the actions are valid because they were chosen by an expert, how come H differs so much from ML-A and ML-S in Figure 4? I am guessing that ideally, they would be very similar to each other because both were designed by the expert. And that eventually, the ML algorithms would learn the ideal average distribution (the one in H) of actions across all participants. If the authors are claiming that ML-S and ML-A result in a better action selection mechanism than H, how do we know that they are better, as both were created by the same expert?
*Do you have any results comparing the participants’ feelings towards the heuristic and the machine learning approaches?
",Agreement accepted,"The work is original in terms of the long-term study that shows how online-interactive machine learning could be used for long-term interaction and personalization. 
I highly value the effort that was made to run and prepare this study and from my point of view, the authors make a good example for the community preparing a long-term study in a more natural setting.
Even though I acknowledge the high efforts, I miss a more extensive/informative report of results that would help to evaluate the full quality of the work. I would expect that it should be within the scope of the paper to report results that support the validity of the approach. Thereby, I am not expecting that all the results that were captured are reported but rather that more meaningful results are selected. For example, the statements of the participants which are given in the result section appear slightly out of context and I could not understand the implications of these statements.
Even if I am aware that the following might be difficult, I could imagine that a representation of the different learned policies or a representation of the differences between the policies might reflect better what the trainer stated about the two chosen participants needing different approaches or in general give insights into personalization (something like they have done here: Gordon, Goren, et al. ""Affective personalization of a social robot tutor for children’s second language skills."" AAAI. 2016.).
Further, the authors state in the results section that a Chi-Square is not applicable, I wonder if all possibilities for tests were explored (e.g. Fisher's exact test). My impression is that this type of formulations might cause doubt on the qualification of the authors and I am confident that this is not the case and that this impression could be easily avoided. 
Therefore, I argue that the quality of the work is high but the reporting remains unclear.

The work makes a significant contribution to the community by providing insights into the design of a SAR in a participatory manner. The quality of the work is supported by the report on a long-term study. The clarity of the paper could be improved by selecting results that help to evaluate the quality of the work better and avoiding statements like 'out of scope' and 'test not applicable'.	",Agreement accepted,,,,07/15 15:00,07/15 17:00,[Not Answered],[Not Answered],,,59.0,F3wE-K96wEE
"Traditional dense volumetric representations for robotic mapping make simplifying assumptions about sensor noise characteristics due to computational constraints.  We present a framework that, unlike conventional occupancy grid maps, explicitly models the sensor ray formation for a depth sensor via a Markov Random Field and performs loopy belief propagation to infer the marginal probability of occupancy at each voxel in a map. By explicitly reasoning about occlusions our approach models the correlations between adjacent voxels in the map. Further, by incorporating learnt sensor noise characteristics we perform accurate inference even with noisy sensor data without ad-hoc definitions of sensor uncertainty. We propose a new metric for evaluating probabilistic volumetric maps and demonstrate the higher fidelity of our approach on simulated as well as real-world datasets.",RSS2020_Author_Agreement.pdf,"[Kumar Shaurya Shankar](https://sites.google.com/site/kumarshaurya/home), [Nathan Michael](https://nmichael.me/)",Kumar Shaurya Shankar (Carnegie Mellon University)*; Nathan Michael (Carnegie Mellon University),RSS2020_Author_Agreement.pdf (547907 bytes); root.pdf (5538810 bytes),root.pdf,1232.0,60.0,,2.0,MRFMap: Online Probabilistic 3D Mapping using Forward Ray Sensor Models,https://mrfmap.github.io/,0.13078855482963,"The contributions above provide a summary of the submission; the contributions are interesting and novel, and represent a new take on fast probabilistic occupancy map generation. However, it is difficult to tell the exact usefulness of this method beyond standard octomap, which is used as a baseline, without exhaustively testing this in both simulation and the real world. Certainly a new ""baseline"" than octomap would be interesting and useful to the robotics community, but the case isn't clear that this method is that due to the limitations of the presented results. For instance, it would be interesting to see this method applied in corridors, large rooms or cluttered areas, as would be typical for autonomous robotic operation, i.e. a typical octomap application area. An improvement in the accuracy of finely-gridded occupancy voxel maps is interesting and notable, but to what extent is it useful? This is especially important when it comes to evaluating this method as a replacement for other octomap-centric sensors, e.g. 2D or 3D lidar, which are also not mentioned. Finally, the addition of a GPU on a mobile robot to process depth data represents a significant cost over using octomap. If a mobile robot is already equipped with such a computational device, then it would be likely other dense reconstruction methods would have to be compared, e.g. surfel-based techniques. Overall, this is not meant to doubt the marginal improvement the method represents, rather call into question its overall utility.

More figures would help to clarify the operation of the MRFMap method. The graphs in figure seven contain several plots (the green, orange, and bold blue lines) that are not explained in the caption or text. This inhibits understanding of these figures. Tables 1, 2, and 3 use the name “MRMap” instead of “MRFMap.” There is a clear correlation between the map resolution and its accuracy score; this effect should be mentioned and explained in the text. Suspect a typo in Table 1: accuracy value for octomap at .01m resolution is 0.061 and this is suspiciously low. The actual meaning of the accuracy values in tables 1, 2, and 3 is not immediately clear; more explanation in the captions or the body of the paper could help here. The meaning of the +/- values in tables 1, 2, and 3 also isn’t clear; it may be clearer to use the mean and variance of error.",,"
## Summary:
This paper focuses on occupancy grid mapping. The main contribution is an explicit probabilistic model for the formation of sensor rays used to update a Markov Random Field representation of the environment via the belief propagation algorithm. Unlike most occupancy mapping algorithms, this work takes the correlation between map cells into account and makes the inference scalable via belief propagation. The proposed approach is demonstrated in simulated and real-world experiments using the ICL-NUIM dataset and a Realsense D435 camera.


# Recommendation:
This is a strong paper with new ideas that are likely to apply to other problems in robot mapping. The strengths of the paper include (1) a precise probabilistic model of forward ray sensing, suitable for RGBD and Lidar sensors, and its use in a Belief Propagation formulation of the occupancy grid mapping problem and (2) comprehensive simulated and real-world experiments and comparisons to Octomap. There are several weaknesses that should be addressed as discussed below. Overall, the paper makes a novel contribution to occupancy mapping, by formulating it as a Markov Random Field and applying Belief Propagation, that will be of interest to the robotics community.


# Major Comments:
 1. The paper lacks a clear problem statement that specifies the variables of interest and the mapping task rigorously. It is strongly recommended that such a statement is included in a separate section before Sec. III.
 
 2. The paper would benefit from a clear summary of the belief propagation algorithm, specified to the occupancy mapping problem. More precisely, the authors are encouraged to add an algorithm summary at the end of Sec. IV in order to make it possible for the results to be reproduced easily. 
 
 3. The scalability of the proposed method is a major concern as there might be millions of voxels and rays in a 3D mapping application. There is no discussion or evaluation of the scalability and memory requirements of the proposed approach as compared, e.g., to Octomap, which scales quite nicely to large environments.


# Minor Comments:
 -- Octomap and related methods are not described in the related work. In general, the related work may be strengthened by including more related work, especially approaches based on Gaussian Process occupancy mapping, e.g.,

S. O'Callaghan, F. Ramos, ""Gaussian process occupancy maps,"" IJRR 2012.
S. Kim, J. Kim, ""Occupancy Mapping and Surface Reconstruction Using Local Gaussian Processes With Kinect Sensors,"" IEEE Transactions on Cybernetics, 2013.
J. Wang, B. Englot, ""Fast, accurate gaussian process occupancy maps via test-data octrees and nested Bayesian fusion,"" ICRA 2016.
T. Whelan, R. Salas-Moreno, B. Glocker, A. Davison, and S. Leutenegger, ""ElasticFusion: Real-Time Dense SLAM and Light Source Estimation,"" IJRR 2016.
S. Guo, N. Atanasov, ""Information Filter Occupancy Mapping using Decomposable Radial Kernels,"" IROS 2019.
T. Schops, T. Sattler, M. Pollefeys, ""BAD SLAM: Bundle Adjusted Direct RGB-D SLAM,"" CVPR 2019.

 -- The review of the sum-product algorithm is not sufficiently self-contained. In general, the notation in the paper can be improved. For example, for a reader unfamiliar with BP, the unary factor in (7) might be unclear. What is the difference between $o_{N_r}$ and $o^r_{N_r}$ in Eq. (10)?
 
 -- There may be a typo in Eq. (15); there should be a negative sign behind the integration.
 
 -- The approach presented in Sec. III.D seems to use a much simpler model that the exact expression for the range measurement likelihood derived in eq. (18). In detail, the authors resort to a Riemann sum approximation of (18) which does not use $\omega(s)$. This means that essentially a simple model has been used, which could have been defined and explianed from the very beginning, instead of introducing a complex accurate model and abandoning it for the sake of computational complexity.
 
 -- It might be good to discuss, on the basis of the evaluation, to what extend is the independent ray assumption valid.
 
",Agreement accepted,"This paper resembles an interesting and novel approach to occupancy grid mapping. The key idea is to employ an MRF along the rays of the range scan beams. In order to model the dependencies in the state estimation along the rays of a range reading, a depth potential is considered along the ray. To perform inference belief propagation-based message passing is used in an iterative forward-backward fashion.

The papers gives a nice, fresh, and novel view to the problem of occupancy information estimation and clearly presents novel ideas. Furthermore, the approach is coupled with a learned sensor model for an IR-based depth sensor such that the sensor properties are taken into account appropriately.

What requires a series revision are the references. Several of them miss the journal/conference entry (eg 25, 21), others contain newlines, various ways for referring to similar conferences, the order is partially mixed up (13 and 14). It is a big mess that should be cleared up for the final submission.

Furthermore, some related work is missing. I am especially referring to the GP occupancy grid maps presented by Fabio Ramos' group already a while back. Ramos et al. used GPs to model occupancy grid, also allowing to consider dependencies of inferred occupancy values. The approach is clearly different, i.e., no negative impact on novelty here, but they must be cited; Several works exist here, a good representative is:
Gaussian process occupancy maps by Simon T O’Callaghan, Fabio T Ramos, IJRR, 2020.
See: https://doi.org/10.1177/0278364911421039

One may also consider other 3D modeling techniques using surfel maps, such as;
Efficient surfel-based slam using 3d laser range data in urban environments by Behley et al., RSS 2018.
I suggest to fix the references and invest more efforts into the related work - all that, however, can be fixed easily for the final version.

What I suggest to extend is the experimental evaluation showing more complex scenes and not only small toy examples to provide a better view of the capabilities of the proposed approach. Furthermore, a timing experiment compared to octomap would be nice as the MRFMaps approach seems to have increasing runtime requirements as more and more images are added. It would be interesting to see how that evolved to hundreds or thousands of scans as they occur in real-world data and not only 16 depth images as shown in the paper.

What I am curious to see is the ability to combine this approach with a real SLAM system.



",,"The paper is clear and very readable. I have not seen an MRF formulation like this applied to occupancy grid estimation with depth sensors, so it appears to be novel and to be a logical extension of work in this area. I have not checked the mathematical details of the formulation. Experimental evaluation is thorough and shows improved results with the method, which conforms to what one would intuitively expect from the more sophisticated modeling involved. The approach to learning sensor models is reasonable, but overlooks some relevant sensor physics; I don't see that as a major weakness in the paper, just a limitation in the work to date and applicability under variable conditions.",,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/mrfmap/mrfmap,,,,60.0,FsCC9pS6xbo
"Imitation learning is an effective and safe technique to train robot policies in the real world because it does not depend on an expensive random exploration process. However, due to the lack of exploration, learning policies that generalize beyond the demonstrated behaviors is still an open challenge. We present a novel imitation learning framework to enable robots to 1) learn complex real world manipulation tasks efficiently from a small number of human demonstrations, and 2) synthesize new behaviors not contained in the collected demonstrations. Our key insight is that multi-task domains  often present a latent structure, where demonstrated trajectories for different tasks intersect at common regions of the state space. We present Generalization Through Imitation (GTI), a two-stage offline imitation learning algorithm that exploits this intersecting structure to train goal-directed policies that generalize to unseen start and goal state combinations. In the first stage of GTI, we train a stochastic policy that leverages trajectory intersections to have the capacity to compose behaviors from different demonstration trajectories together. In the second stage of GTI, we collect a small set of rollouts from the unconditioned stochastic policy of the first stage, and train a goal-directed agent to generalize to novel start and goal configurations. We validate GTI in both simulated domains and a challenging long-horizon robotic manipulation domain in real world. Additional results and videos are available at https://sites.google.com/view/gti2020/. ",000001_001.pdf,"[Ajay Mandlekar](http://web.stanford.edu/~amandlek/), [Danfei Xu](https://cs.stanford.edu/~danfei/), [Roberto Martín-Martín](https://robertomartinmartin.com/), [Silvio Savarese](https://cvgl.stanford.edu/silvio/), [Li Fei-Fei](https://profiles.stanford.edu/fei-fei-li)",Ajay Mandlekar (Stanford University); Danfei Xu (Stanford University)*; Roberto Martín-Martín (Stanford University); Silvio Savarese (Stanford University); Li Fei-Fei (Stanford University),2020_RSS_Long_Horizon_Imitation (11).pdf (3172011 bytes); 000001_001.pdf (1048079 bytes),2020_RSS_Long_Horizon_Imitation (11).pdf,101.0,61.0,,2.0,GTI: Learning to Generalize across Long-Horizon Tasks from Human Demonstrations,https://sites.google.com/view/gti2020/,0.18302128706760998,"This paper addresses the problem of learning long horizon tasks from limited number of demonstrations. It proposes a two-stage approach to the problem: 1) learning a stochastic policy from demonstrations, and 2) using the policy from stage (1) to roll out mode demonstrations and then to train a goal-conditioned behavior cloning policy from the new roll outs.

Pros:
- The problem this paper is addressing, i.e. learning long-horizon tasks from visual demonstrations, is important and one of the hot topics in the field. It is pretty complex and hard with high-impact on enabling robots to operate in unstructured environments. 
- The paper is clearly written and reads well!
- I liked the PandaKitchen task, it is one of the meaningful robotics tasks that one may actually wants robots to do in future!

Cons:
(1) The contribution of the paper in Stage 1 is minor. Essentially the approach that is presented in the paper is almost identical to [29]. Here are the differences between the two approaches:
  a) [29] actually constructs the latent variable from a sequence of states vs this paper that only uses the last frame. In this regard [29] is more complete and in contrast to what has been claimed in the paper, it is actually less prone to mode collapse.
  b) This paper models the latent variable with GMM vs [29] that used a normal distribution. This might be the reason why this paper can handle multimodality despite lacking conditioning on a trajectory as described in (a).
  c) This paper uses an additional reconstruction loss during the VAE training, but the effect of this additional loss term is not described in the paper.

(2) Lack of ablation study. Although this is not a theoretical paper, it is still important to study the effect of each decision on the model architecture to the overall performance. As mentioned in (1), the difference between this paper and [29] is only on 2-3 changes. Without the ablation study and comparison to [29], it is hard to understand the impact of this paper.

(3) I found it a little confusing that the paper ignores all the trained cVAE model and latent goal-conditioning. The important question here is that: ""Why not using the additional roll outs as new demonstrations for cVAE model?"". Also it is huge downgrade to move from latent goal-conditioning to passing the target image as a goal in Stage 2. How the goal image can be provided to the robot in a real-life scenario where the scene is not exactly the same as the one in the demonstration?

(4) The scene during the demonstration and eval is the same, both in terms of objects and their arrangement. This reduces the significance of results.

Minor comments:
- Section 3:
  We consider a robot manipulation task a sequential -> We consider a robot manipulation task as a sequential

- Section 3:
intersecting states from different goals and to reach from intersecting states to different goals -> intersecting states from different initial conditions and to reach from intersecting states to different goals",,"Originality:
The paper presents an original idea that addresses a problem that might not be well known to the wider community but it definitely exists in the field of imitation learning. The authors also do a good job of providing a comprehensive overview of the related work. 

Quality:
This is a well-written paper that presents good results and presents a single, well justified story. The authors don't try to overreach for additional contributions but rather clearly present the problem that they're interested in and show an approach that addresses this problem.

I think that there are two aspects that slightly diminish the quality of the paper:
- the lack of strong baselines. The authors compare to well-known techniques that are destined to fail. However, they could introduce an LMP-based baseline [29] that should be able to better deal with the multi-modality in the data, as well as other versions of their method, e.g. where there is just a single Gaussian prior.
- the figures (esp. Fig. 2) are often confusing and don't provide additional help with understanding the concepts in the paper. For example, what do the colors refer to in Fig. 2? Which parts of the networks are shared? If this a conditional VAE, why isn't the prior conditioned on the on the goal, etc. The same applies to the algorithm boxes which could be significantly shortened and made more accessible.

Clarity:
The paper is relatively clear and well written but as mentioned above, I think it could benefit a lot from better figures and algorithm boxes. There are also minor issues such as typos and duplicates in the Related Work section.

Significance:
I would grade the significance of this paper as medium. It addresses a problem but it doesn't fully show comparisons to competitive baselines. Here are a few suggestions on how to improve it:
- introduce comparisons to simpler versions of your method: single Gaussian prior, single network that directly produces actions (even with the GMM prior)
- introduce comparisons to LMP[28] and GCBC with a multi-modal output
- the authors potentially missed an important point that even a multi-modal BC-like method might not be able to deal with the presented problem because of the mode-covering behavior of forward KL.
- discuss the comparison to Q-learning-based methods that technically are supposed to be able to merge trajectories like the ones presented in the paper. Why GC-Batch RL method can't achieve the same result?
- Remove IRIS in Fig. 3 description. It's supposed to be GTI.
- the architecture of Stage 1 is very much unclear given the current Figure. I believe that the prior should be goal-conditioned. I would also suggest a comparison to a single stage process with a multi-modal prior.
- motivation of the paper is rather strong but the authors cite the work of 28 as an example of large amounts of annotated demonstrations, which is not true, since it relies on unlabelled play data.",Agreement accepted,"The paper is generally well written and easy to follow. Below are some suggestions that will help improve the manuscript. 

1) Section 3: definition of trajectory intersection:
This definition isn't well integrated with the rest of the paper, and quite loosely defined. S_i^1 = S_j^2 will be hardly true in noisy stochastic systems especially in high-dimensional systems with continuous states. The paper later goes on to using image observations instead of states. Equivalence in observation doesn't necessarily imply equivalence in states. These details are currently overlooked, and the paper can benefit from paying close attention to it.

2) GTI doesn't explicitly model the temporal structure of the demonstrations. Some temporal details, however, need further clarification. Section 3: ""H timestep"" and ""T length subsequence"" are mentioned without much clarity. It's unclear how ""H/T"" is chosen, its assumptions and constraints with respect to the overall tasks horizon and trajectory intersection point.

3) The idea of leaving the trajectory intersection to amplify novel behaviors is quite interesting and is backed by real-world experiments in a few tasks. The current tasks are at its bare minimum consisting of 2 state-state, 2 goal-state, and 1 intersection point. The paper, however, lacks evidence on some critical questions. 
     a) As the complexity of tasks grows, there will be multiple intersection points. There is no clear evidence if appropriate intersection points can be identified and effectively leveraged in planning.
     b) The intersection point is currently explicitly provided in the form of a green bowl. There is only one intersection point between the task and the goal. The intersection point is temporally equidistant from the start and the goal. These assumptions are not necessarily always true in real tasks. 
There are multiple ways to understand these questions - a suggestion here could be to advance the experimental setup by one level -- 3 state-state, 3 goal-state, and 2 intersection point (one at an early stage of the task and one at the late stage of the task. This will be the minimal setup that can provide good insight into the method without significantly advancing the complexity of experimentation and engineering effort.

4) The paper can use some insights on the goal proposal model -- it can provide goals that are out of distribution for the low-level controller, or even provide a physically implausible goal. How does the method get around these challenges?

6) Figure 3 GTI is quite noisy reaching the goals. Some insights here will be helpful

5) typos
page1: independently -> independent
page1: ""different phases"" -> phase of a demonstration isn't defined
page 3: ""arrive at intersecting states from different goal "" -> arrive at intersecting states from different inital states 


",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,https://youtu.be/v5DqtK7sUOI,,,61.0,hlvRmLlYHZ0
"This work presents a significantly improved strategy for coordinated multi-agent weeding under conditions of partial environmental information. We show that by using Entropic value-at-risk (EVaR) together with the Gittins index, agents can make intelligent decisions about whether to exploit the estimated distribution of weeds in the environment or to explore new areas of the environment. The use of this method improves the performance of agents in comparison to previous methods, resulting in a system which can weed denser fields using fewer robots. Furthermore, we show that for the reward function and environmental dynamics which represent the weeding problem, our system is able to perform comparably to the fully observed case over the real-world range of seed bank densities, while operating under partial observability.",AuthorAgreement.pdf,Wyatt McAllister (University of Illinois)*; Joshua Whitman (University of Illinois); Allan Axelrod (University of Illinois); Joshua Varghese (University of Illinois); Girish Chowdhary (University of Illinois at Urbana Champaign); Adam Davis (University of Illinois),Wyatt McAllister (University of Illinois)*; Joshua Whitman (University of Illinois); Allan Axelrod (University of Illinois); Joshua Varghese (University of Illinois); Girish Chowdhary (University of Illinois at Urbana Champaign); Adam Davis (University of Illinois),RSSPaperFinal.pdf (2334210 bytes); AuthorAgreement.pdf (109768 bytes),RSSPaperFinal.pdf,17.0,62.0,,2.0,Agbots 2.0: Weeding Denser Fields with Fewer Robots,[Not Answered],0.874869831562999,"While the high-level motivation for requiring mechanical weeding is excellent and well supported, the actual contribution is not clear and many design decisions, which might affect the reported results seem arbitrary. The reported simulation results are only superficially analyzed and the supposed benefits of the proposed methods are marginal at best. For example, the strongest result seems that sometimes 8 robots can weed a particular field, 7 are never enough, and 9 are sufficient given any of the tested allocation frameworks. While the authors claim this is “fundamental” it seems like the numbers are dominated by the more fundamental limits resulting from robot speed, growth rate and field seize are never discussed.
 
In any sort practical scenario, the growth rate of weeds might vary so that 8 robots might or might not be suffice for a given 1-acre field, yet the paper does not report any robustness results with respect to model uncertainty. The simulations use a randomly initialized seed-bank size, yet do not discuss the sensitivity of the results w.r.t other parameters. For example, the text states both that weeds can ""grow explosively""  after a period of linear growth (pg 3) and that the results are not sensitive to the particular growth patterns (pg 8). As a reader, I have a difficult time reconciling these apparently contradictory statements, or finding any analysis or data to support the latter claim.   

The paper states that an observation radius or r=0 is not useful since robots can only “see” the parts of the filed that they are currently processing, yet actual fields would likely display strong spatial correlation, so that this information could likely be used for planning purposed. If that is not possible here, it again seems like a limitation of the problem formulation or the simulation environment.  What would a weed-height sensor look like, and why is r=1 a reasonable choice, do the same results hold with r=2? 

The choice of the reward function is never discussed or motivated. In the current framework, the robots try to “weed” as much as possible, so they receive a higher reward for weeding a patch that has more weeds on it.  While this makes sense as a local decision strategy, the optimal solution might encourage weed growth so that robots can reap more rewards when they visit those locations.  Assuming that there are weed-crop interactions, why is the optimization not framed as maximal crop yield, or revenue for the farmer? 
  
Troublingly, the proposed method also seems to outperform a planner that has complete information, which suggest an error in the simulation and test setup, or at least missing confidence intervals.  Yet, the text does not mention or explain this apparent inconsistency. 

While the motivation is clear, it is not clear that the current draft solves any of the practical problems or that the findings would transfer to the real-world system the model is based on in a meaningful way. The improvements over the previously published, or even the naïve lawn-mower planning, are incremental and the paper does provide enough evidence that the simulations is sufficiently accurate to evaluate what difference the methods would make in a real-world system. 

In terms of presentation, the writing is excellent and  the presentation is clear. However, the figures are difficult to parse and the current layout makes it difficult to tell the extensive figure captions from regular paragraphs.  

",Agreement accepted,"Sections:

Section 2B: ""In order to achieve this distribution, we first give each cell a random density between zero and twenty percent of S0..."", is this uniformly random?

Section 2B and 2C: The variables x and y are used as arguments in section B before they are defined in section C. It is also unclear if they are positions in R or indexes for the discretization of the space.

Section 2D: Gamma is not defined for Eq (19) and (20). I believe this is a probability about the stochastic process used in the Gitten's index but it should be defined in this manuscript to keep it self contained.

Section 2E: For the confidence in the reward estimate (alpha) why does it not include zero or one? As the authors state, as sampling goes to infinity (in theory) alpha converges to one. 

Last paragraph of 2G: ""Within every Monte Carlo run, we consider a success to be a case in which an algorithm is able to keep the total maximum heights from each cell in the field, per agent, under the value 1000."" Can the authors provide some physical intuition to this threshold. Is this the sum of the worst weed height being under 1000m/cm, something else? It would also be beneficial to understand how this threshold was/should be chosen.

Equations:

Equation (18) why is the minimum time to weed a row 2 minutes?

Equations (11) and (12) are identical to Equations (6) and (7)

Figures/Tables:

Fig. 4 - What is the density represented here?

Fig. 6/7 - The caption here is much to long, the conclusions drawn in the Results and Discussion section should not be present here.",Agreement accepted,"The paper is well written, easy to follow and ideas are conveyed with clarity. The paper tries to solve a relevant problem in agricultural robotics concerning weeding. The authors have a clear understanding of the problem and have done a thorough literature review on the topic. In addition, the paper contains extensive evaluations of the strategy. 

Comments:

1) How well does the strategy translate into real-world robotics applications?
2) In Section I.B, the first sentence should be ""Section II presents an overview of ...""
3) In Section 2.B, the authors say: ""We expect the algorithm to scale to larger fields in a straight forward manner"". The authors should justify why do they expect this or remove this sentence. 
4) The reviewer thinks the authors meant D_{KL}(Q || P) in Eq 24 in the exploration bonus term.
5) The authors have moved some of their experimental results to supplementary material which the reviewer assumes is due to space constraints. The reviewer wouldn't recommend doing that since it is a part of the main contribution to the paper. 
6) The clarity of the video should be improved either by adding a text description of what is happening or by adding a voice-over describing the process. 
",Agreement accepted,,,,07/15 15:00,07/15 17:00,[Not Answered],[Not Answered],,,62.0,2Aarco2YfE0
"We investigate the problem of using mobile robots equipped with 2D range sensors to optimally guard perimeters or regions, i.e., 1D or 2D sets. Given such a set of arbitrary shape to be guarded, and k mobile sensors where the i-th sensor can guard a circular region with a variable radius r_i, we seek the optimal strategy to deploy the k sensors to fully cover the set such that max r_i is minimized. On the side of computational complexity, we show that computing a 1.155-optimal solution for guarding a perimeter or a region is NP-hard, i.e., the problem is hard to approximate. The hardness result on perimeter guarding holds when each sensor may guard at most two disjoint perimeter segments. On the side of computational methods, for the guarding perimeters, we develop a fully polynomial time approximation scheme (FPTAS) for the special setting where each sensor may only guard a single continuous perimeter segment, suggesting that the aforementioned hard-to-approximate result on the two-disjoint-segment sensing model is tight. For the general problem, we first describe a polynomial-time (2+\epsilon)-approximation algorithm as an upper bound, applicable to both perimeter guarding and region guarding. This is followed by a high-performance integer linear programming (ILP) based method that computes near-optimal solutions. Thorough computational benchmarks as well as evaluation on potential application scenarios demonstrate the effectiveness of these algorithmic solutions. ",rss_form.pdf,"[Si Wei Feng](https://sites.google.com/view/swfeng/homepage), [Jingjin Yu](https://arc.cs.rutgers.edu)",Siwei Feng (Rutgers University)*; Jingjin Yu (Rutgers Univ.),rss_form.pdf (112388 bytes); _main.pdf (915781 bytes),_main.pdf,1251.0,63.0,,2.0,Optimally Guarding Perimeters and Regions with Mobile Range Sensors,,0.17942509623035896,"The problem is interesting, and likely to interest some in the community.  
The results are mostly presented clearly, with a few exceptions -- see below.
This sort of problem has many close cousins (art gallery, bin packing, etc.) but this particular flavor seems to be unique.  With some imagination (as expected for this sort of work), one can see some possibility for applications to specific ""real robot"" problems.  The level of detail in the proofs is just about right for this kind of venue.

The direct statement of the problem under consideration and the contributions against that problem is noted and appreciated.  However, for algorithm analysis purposes, saying that the input is ""a compact workspace"" is a bit vague, because the notion of input size is not clear.  (And thus, things like hardness results don't make much sense.)  Thus, perhaps Problem II.1 is the appropriate place to restrict attention to polygons?  This subtlety shows up again later in Theorem III.1 --- what is the input size for a 3-net?

Also, as a minor point, it is a little strange to see the assumption about lower and upper bounds on the sensing range *after* the formal statement in Problem II.1.  As the authors note later when this assumption is utilized, such an assumption is quite relevant to the analysis.  Better to state such assumptions clearly as part of the problem statement.

In a few places, the writing is not entirely clear about the difference between a special case of a problem and a variant.  The variant for which you provide a 1+\epsilon approximation seems not to be a proper special case of OSG_2D, --- instances of the former are not properly instances of the latter, since the restriction is not on allowable inputs, but on the notion of what constitutes an optimal solution --- but the writing seems to leave the distinction a bit fuzzy.

In the proof of Theorem III.1, this reviewer did not succeed in locating any direct statement of the problem from which the reduction proceeds.  It seems to be a special case of vertex cover?  Also in this proof, it would be helpful to distinguish carefully between a graph and an embedding of that graph into the plane.",,"This paper considers two related problems, Optimal Perimeter Guarding (OPG_2D) and Optimal Region Guarding (ORG_2D), with k robots each having a circular 2D range sensor. OPG_2D and ORG_2D together comprise the Optimal Set Guarding problem (OSG_2D). The goal is to use a set of mobile robots to monitor either the perimeter of a region (in OPG) or a region (ORG). The regions are assumed to have simple polygonal boundaries, with zero or more simple polygonal obstacles. The objective being minimized is the sensor radius, and the solution also computes the locations of the robots. The main theoretical result in the paper is an inapproximability result showing that finding solutions for OSG_2D with an approximation factor within 1.155 is NP-hard. This result builds on prior results [22, 23] on vertex cover for planar graphs of maximum degree 3, by designing a 3-net backbone structure. The main algorithmic results are three types of algorithms. The first is a (1+ epsilon) approximation algorithm that discretizes the perimeter (or region) using lengths of 2*epsilon (or squares of epsilon^2), where each sensor is responsible to guard only a single continuous perimeter segment. This algorithm, which involves a binary search on the decision version of the problem, is called AL_OPG_2D_CONT and is a fully polynomial time approximation scheme (FPTAS). The second class of algorithm gives a (2+ epsilon) approximation using results from the facility location problem (or equivalently, the k-center problem). The third class of algorithm uses a discretization of the perimeter (or region) and presents an integer linear program formulation. Finally, simulation results are presented on synthetically generated simple polygons and comparison of the various algorithms are given in terms of computational time and quality of the solutions generated. Practical examples on two different environments are presented as well.

The paper is technically strong. However minimizing the sensor radius to guard a perimeter or region does not seem well motivated. While the paper states that this decreased sensor footprint increases the resolution of the image data, this metric needs to be better justified. A more useful objective would be to minimize the number of robots needed to guard the perimeter/region. The paper also assumes that the robots are in fixed positions (given by the solution). This solution does not really use the mobility of the robots beyond getting them to the desired fixed locations. Wouldn't it be more effective to have robots patrolling at a sufficiently high frequency? If using drones for the two application senarios (Section V B), practical constraints like limited battery life would make it harder to use such solutions.

Section II: The definition of size(k, D) is a little hard to grasp. Are the center locations c1,..., ck defined over R^2? From the current definition it appears that the center locations are fixed, making one of the two minimizations redundant. Perhaps providing a geometric/physical interpretation for size(k, D) would help, particularly given its use throughout the paper. 

Section III A: The selection of m in the conversion of an edge uw to a path is not discussed. This is important since it appears that there is an implicit assumption that the curvature of each the paths is smooth, i.e., there are no abrupt changes in the angles between two consecutive unit segments of a path. If the curvature were not smooth then it may happen that a circle of radius alpha covers more than 4 points from the vertical bars. However, under the smooth curvature constraint, it is not clear if an appropriate 3-net can always be created.
 
Section III B was confusing. It was not clear why for small enough delta, P is a polygon with holes. Fig 8 also did not help me identify the holes in the polygon. Perhaps the holes can be identified in the figure. Isn't the polygon in Fig 8(a) a simple polygon (assuming its end caps are closed)?

Section III B: Is the definition of K correct? Should the 2 be in the denominator?

What does it mean to say ""each sensor can cover at most two disjoint perimeter segments""? That the lengths of the segments must be less than the diameter of the sensor circle? Is this the case in Fig 12?

Section IV A: M[] is used without being defined.

Theorem IV.1: Does ""continuous coverage"" refer to coverage of continuous perimeter segments, or coverage continuous in time? Both occur here.

Table III: The run times do not show a consistent decrease with increasing k, especially in the last two rows. So the scalability of the ILP is not clear.

Last sentence of Section V A is a bit puzzling. Unclear how AL_OPG_2D_ILP is ""getting very close to being 1-optimal"".

Section V does not state the values of the epsilon parameter. It is not clear how close to optimal the solutions are.

It appears it should be possible to extend your OPG solutions to the case when the perimeters of the interior holes should also be guarded.

Could a mixed ILP optimize the locations of the sensors on the continuous R^2 domain?

Have you considered the problem of reducing the number of robots for fixed sensor radius? What about optimizing both the number of robots and the sensor radius?


Presentation suggestions:

The introduction describes the sensors as 2D range sensors, whereas Section II describes a quadcopter with a vision sensor. The model seems to better match the latter -- why the use of the term ""range sensor""?

Since the regions in the environment are modeled as simple polygons, this should be made clear in the Abstract and/or Introduction.

There are some awkwardly worded/unclear sentences. For example, the first sentence of the last paragraph of Section II. Also, the first sentence of the proof of Theorem III.3.

Section II A: In the definition of L, the summation should be over uw \in E(G).

There are several grammatical errors throughout the paper. A careful proofreading should eliminate them. For example, the wording of Theorem III.1.

Section III A, last sentence of first paragraph should end with ""3 (left).""

Theorem III.3: Is d_u the degree of u? Please define.

Fig 13: What is the discretization value of epsilon here? The left and right figures appear to have different discretizations.

References: A little attention should be given to appropriate capitalization in the references. For example, Kepler, NP-complete, R^3. Titles of conferences, journals, and books should also be appropriately capitalized.",Agreement accepted,"This paper presents an analysis of the coverage planning problem for a computer science perspective. This contribution is interesting and work on this topic would be a welcome contribution to the field. My concerns with the paper surround the thoroughness and clarity of its technical results and the scientific rigour of its experiments.

---Technical Analysis---
The paper presents an analysis that seeks to show that the perimeter/area coverage problem cannot be solved in polynomial time to a better approximation than 2\sqrt{3}/3 for the area problem and 1.155 for the perimeter problem, unless P=NP. This analysis is made by proving the hardness of guarding a ""3-net"", then showing that a 3-net can approximate ""simple polygons"". In doing so, they also demonstrate that any polygon can be converted to a 3-net by replacing vertices with cycles.

I found this analysis difficult to follow, and can only provide comments at a low level with the aim of improving the communication of the results.

1. It is not stated clearly whether the results assume that the given k robots are capable of covering the target region and, other than in one algorithm, it is not discussed what would happen if they cannot. Relatedly, what assumptions are made about the range of the sensing radius, is it assumed to extend to infinity?

2. What is meant by ""continuous boundary segments""  in phrases such as ""each sensor may only guard a single continuous perimeter segment"" or ""no more than two continuous boundary segments"". My understanding is that a side of a polygon would be a single continuous perimeter segment, and a corner would be where two continuous perimeter segments meet. But are the statements meant to include/preclude two sides that are topologically distant but spatially close, i.e., parallel?

3. In the preliminaries section, the size of a 2D simple polygonal region is defined as:
    size(k, D) = min_{c1,...,ck} max_{p\inD} min_{1<=i<=k} || c_i - p ||
This equation is fundamental to the analysis and I believe the reader would benefit from more explanation and intuition. It is not readily apparent to me what effect the left-most minimum has if the first minimum is specifying the c_i. Is there some interaction between c_i and p?

4. In the proof of Theorem III.1 it is stated that: ""For a path u...w on T_G , since the path length is odd, [...]"". It is not obvious to me why every path in T_G must be of odd path length.

5. Theorem III.1 is stated to prove NP-hardness, yet the proof itself makes no mention of NP. If this result comes from proving that the theorem is equivalent to something that is previously known to be NP, then please make this fact explicit and include a citation to the proof of the connected result.

6. Possible typo on page 4 in the statement ""Let K = ((L - |E|)2 + k)"", should it be /2?

7. A number of results are presented about a graph with maximum degree 3, including extending these results to simple polygons, well before demonstrating that any graph can be converted to such a graph using the technique illustrated in Fig. 9. I think it would be helpful to readers to reconsider the order of this presentation such that the generality of the 3-net to higher-degree graphs was presented before the connections to simple polygons.

---Algorithms---
The paper presents algorithms with bounded optimality for the perimeter and area coverage problems. 

1. What is ""OPT"" in the statement ""OPT + \epsilon"" on page 6?
2. Do the algorithms search for r or r*? On pg. 6 you say ""given a candidate radius r"" but on pg. 7 you say ""we can do a binary search on r*"". My current understanding is that r* represents the true optimum and that r is the approximate radius found by the algorithm, is that correct?

---Experimental Results---
The paper presents multiple runs on simulated results and on two real-world motivated problems. The simulated results are run for 100 trials (perimeter) and 10 trials (ILP).

1. It is stated that the variances are small and therefore not necessary to report in detail; however, the provided ""normalized average standard deviations"" are given as 0.06 (Table 1) and 0.125 (Table 2) and 0.545 (Table 3). None of these can be taken as small given that they are each larger than a number of the results in their respective tables.

2. 10 runs is of questionable statistical significance.

---Minor Typos---
This is not a complete list of typos or other minor mistakes
pg. 4: ""On natural restriction""
pg. 5: ""we first describe a method that used for discretizing the problem""
pg. 7: ""then start to check the feasibility of following integer programming model""
",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,,,,63.0,1-PsAmQlVw8
"Reproducing the diverse and agile locomotion skills of animals has been a longstanding challenge in robotics. While manually-designed controllers have been able to emulate many complex behaviors, building such controllers involves a time-consuming and difficult development process, often requiring substantial  expertise of the nuances of each skill. Reinforcement learning provides an appealing alternative for automating the manual effort involved in the development of controllers. However, designing learning objectives that elicit the desired behaviors from an agent can also require a great deal of skill-specific expertise. In this work, we present an imitation learning system that enables legged robots to learn agile locomotion skills by imitating real-world animals. We show that by leveraging reference motion data, a single learning-based approach is able to automatically synthesize controllers for a diverse repertoire behaviors for legged robots. By incorporating sample efficient domain adaptation techniques into the training process, our system is able to learn adaptive policies in simulation that can then be quickly adapted for real-world deployment. To demonstrate the effectiveness of our system, we train an 18-DoF quadruped robot to perform a variety of agile behaviors ranging from different locomotion gaits to dynamic hops and turns.",RSS2020_Author_Agreement.pdf,"[Xue Bin Peng ](https://xbpeng.github.io/),  [Erwin Coumans ](https://twitter.com/erwincoumans),  [Tingnan Zhang  ](http://N/A),  [Tsang-Wei Lee ](http://N/A),  [Jie Tan ](http://www.jie-tan.net/),  [Sergey Levine ](https://people.eecs.berkeley.edu/~svlevine/)",Xue Bin Peng (UC Berkeley)*; Erwin Coumans (Google); Tingnan Zhang (Google); Tsang-Wei Lee (Google Brain); Jie Tan (Google); Sergey Levine (UC Berkeley),RSS2020_Author_Agreement.pdf (155885 bytes); Laikago_Imitation_Main.pdf (10327063 bytes),Laikago_Imitation_Main.pdf,21.0,64.0,,2.0,Learning Agile Robotic Locomotion Skills by Imitating Animals,https://xbpeng.github.io/projects/Robotic_Imitation/index.html,0.503813455552737,"The paper presents a comprehensive study on mapping mocap animal gaits onto the Laikago robot.  A technical contribution is a modification to [Yu-Liu-Turk-ICLR-2019], by adding an information bottleneck (IB), via a stochastic encoder. The aim of this is to prevent potential overfitting that could result in the learned policy being brittle in ways that are not necessarily observed during the adaptation process. The adaptation algorithm itself is also slightly different.

The paper represents a thorough study of the possibilities of leveraging imitation learning for quadruped robots. The results will be of broad interest to the community and will inspire future work.

Understanding when and why adaptation is necessary would be interesting to speculate on, e.g., [Hwangbo 2019] do not do adaptation, excluding the learned motor dynamics.

The only critique I have is that the ""overfitting"" problem that the information bottleneck aims to address could be better documented.  Currently it seems fairly minimal, e.g., in Fig 10, the beta=10^-3 curve on general really does almost as well  beta=10^-4 curve.  Consider adding a further large value of beta to these plots, to better document the problems of overfitting. 

Maybe I missed it, but it would be interesting to know how beneficial it would be to adapt across the ensemble of skills, as opposed to the individual skills.  Perhaps with the ensemble of skills, the information-bottleneck ""regularization"" may not be needed, given that it would be more difficult to overfit in some particularly way. 

I'm also curious as to how similar the final parameter estimates are for the different skills. I.e., how skill-specific are the adaptations?

suggestions

The paper is specific quadruped robots, so including ""quadruped"" in the title
would be more precise and help to make it findable for others working on quadrupeds.

Abstract:  could shorten this?
The first four sentences are all about the context; reduce?
Only in sentence 5 do we get to: ""In this work, we present ...""
which would be fine as the starting sentence for an abstract.

Excellent and comprehensive review of the related work.

use of a low-pass filter to smooth motions:
Suggest to provide the details and the time constant for the low-pass filter.
E.g., for basic smoothing, s_t = alpha*x_t + (1-alpha)*s_{t-1},
provide either alpha or tau, where tau = dT/alpha, where dT is the sampling rate.

Algo 1, Line 9:  as explained in the text, the argmax is not what is actually used,
so perhaps reflect this in the algo description.

Fig 5:  The fair presentation of the results is greatly appreciated.
Fig 10: This is an important ablation/set-of-tests to see

Fig 5, Fig 7:  I don't understand what the ""Adaptive (Before)"" results refer to.
Presumably it is with parameters corresponding to the middle of their ranges as given in Table 1?  Maybe I missed this in the text.

How difficult is it for Laikago to do a pace? It's wide body would seem to preclude 
",Agreement accepted,"This is a well-written paper, and it was a pleasure to read it. I think the value of this work is that it builds up an end-to-end framework and has demonstrated a number of very agile movements trained from animation data on a quadruped robot. The system as a whole is new, though some of the components are not very novel, such as that the motion retargeting is pretty standard, and that the training of control policies using RL largely repeats existing works.

The domain adaptation is based on the algorithm of [65,67], with a key change to enforce an information bottleneck during training. While the ablation study on the IB is convincing, I wonder how the IB compares to directly reducing the dimensionality of the latent space. In addition, the dimension of the latent space is not provided in the paper. Is it the same for all the motions?

The supplementary video includes a failure case which is not a real failure in my opinion, because the real robot can follow the simulation well. I actually curious about in what case the sim-to-real transfer will fail. It would be appreciated to have an example in the paper.
 
",Agreement accepted,"All the pieces fit together very well and the results are impressive. The paper is dense but well written, and it represents a large amount of work (involving many experiments). The main weakness is the statistical evaluation on the real robot (see below), but I know how hard it is to get statistics on a real, walking robot.

Originality: The overall pipeline is novel and gives results that are very promising. The most novel part is the domain adaptation technique (although, as the authors write it, similar ideas have already been explored) , which seems to be data-efficient enough to be easily used on real robots (5 episodes). The general idea is that during training (1) the policy takes additional inputs that encode the situation and (2) the situation is randomized. For transferring the behavior to the robot, only the values of these inputs are searched using advantage-weighted regression (AWR).

Quality & writing: This is a high-quality paper. The writing is very good and all pieces are well explained.  The main issue is that there is not much statistical analysis of the performance of the pipeline for the real robot. The PPO step and the domain adaptation steps are both stochastic, and here we have only 3 (?) replicates (Fig.5) and it is unclear if the 3 policies are using the same imitation stage (PPO) or not. However, (1) I know that experiments with real robots take a lot of time (and the authors did a lot of experiments in this paper), therefore I can hardly blame the authors for this (I would have preferred fewer 'skills' and more statistics, however), and (2) there are more statistics form simulation on Fig. 8.

Significance: Learning agile skills for legged robots is one of the main challenges for legged robotics. We have many papers that show how to learn forward gaits, but almost none of them can be easily used to create a full behavioral repertoire (jumping, turning, etc.), especially dynamic behaviors (planning & trajectory optimization could solve this problem online, but it is very hard to run full-body real-time MPC on legged robots for now). Using imitation (from real data or from an animation artist) is much more practical way of achieving this objective than writing reward functions. Ultimately, a 'behavior designer' could design behavior that make a legged robots run, jump, etc. without writing a single line of code. To summarize, I believe the approach described here is one of the best use of learning I know for locomotion in quadrupeds.

Minor remarks:
- If Fig.5 is using only 3 replicates for each treatments, then bars with error bars do not make any sense. The authors have obviously no way of knowing that the distribution of the results is Gaussian with only 3 data points (a mean and a standard deviation make sense *only* if the data is Gaussian!). It would be much more honest to show the 3 points (or show the bars and the 3 points, like: http://www.sthda.com/english/sthda-upload/figures/r-graphics-essentials/006-plot-grouped-data-r-graphics-cookbook-and-examples-for-great-data-visualization-error-bars-with-jitter-points-dot-plots-violin-plots-1.png  or http://www.sthda.com/english/sthda-upload/figures/r-graphics-essentials/006-plot-grouped-data-r-graphics-cookbook-and-examples-for-great-data-visualization-line-plot-with-error-bars-2.png 
- The caption of Fig.5 should be clearer. Does that mean that the pipeline (imitation + adaptation) was used 3 different times with different seeds? 
- Does Fig.7 correspond to the same number of replicates?
- What do the colors on Fig.9 mean? (I guess these are different environments?)
- The authors should release the source code: the paper is giving a lot of details, but there are many complex components that are combined here. I am sure some details are forgotten (this is always the case in these complex papers) and I think most students would have a very hard time to reproduce the results described here.
- The caption of FIg. 8 and Fig.10 should say the number of replicates and what is displayed (the median? the mean? the IQR? the standard error?).



",,,,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/google-research/motion_imitation,https://www.youtube.com/watch?v=lKYh6uuCwRY,,,64.0,RMqxSRKRFH0
"In this paper we tackle the problem of deformable object manipulation through model-free visual reinforcement learning (RL). In order to circumvent the sample inefficiency of RL, we propose two key ideas that accelerate learning. First, we propose an iterative pick-place action space that encodes the conditional relationship between picking and placing on deformable objects. The explicit structural encoding enables faster learning under complex object dynamics. Second, instead of jointly learning both the pick and the place locations, we only explicitly learn the placing policy conditioned on random pick points. Then, by selecting the pick point that has Maximal Value under Placing (MVP), we obtain our picking policy. Using this learning framework, we obtain an order of magnitude faster learning compared to independent action-spaces on our suite of deformable object manipulation tasks. Finally, using domain randomization, we transfer our policies to a real PR2 robot for challenging cloth and rope manipulation. ",RSS2020_Agreement.pdf,"[Yilin Wu](http://yilinwu.net/), [Wilson Yan](https://wilson1yan.github.io/), [Thanard Kurutach](http://people.eecs.berkeley.edu/~thanard.kurutach/), [Lerrel Pinto](https://cs.nyu.edu/~lp91/), [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/)",Yilin Wu (UC Berkeley); Wilson Yan (UC Berkeley)*; Thanard Kurutach (UC Berkeley); Lerrel Pinto (); Pieter Abbeel (UC Berkeley),RSS2020_Agreement.pdf (109202 bytes); RSS2020_Camera_Ready.pdf (8299647 bytes),RSS2020_Camera_Ready.pdf,29.0,65.0,,2.0,Learning to Manipulate Deformable Objects without Demonstrations,https://sites.google.com/view/alternating-pick-and-place/home,0.39706827994974503,"The paper developed a learning system for creating controllers that can manipulate cloth or rope through a series of pick and place actions. The key challenge is that jointly learning the pick and place locations is difficult, due to the larger search space, and the dependency between the actions. The paper proposes to learn a conditional policy for the placing action alone, and have it observe different picking actions during training. During deployment, the picking action is then selected as the one that achieves the highest value in the value function. This approach seems effective in the presented results and outperformed several baseline methods.

The paper is in general well written, and the results look interesting. There are, however, a few issues that merits further clarification and potential experiments, as listed below.

First, the experiments presented in this work are largely flattening/extending a deformable object. For these tasks, the exact picking location may not be as important, as suggested by the close gap between uniform pick and the proposed method that optimizes the pick location. It would be interesting to see results for the inverse process, i.e. folding a piece of cloth, or manipulating the rope into certain configurations, where finding the right picking location is more essential and the planning is more difficult.

Second, previous work ([67] Wang et al) demonstrated manipulation of rope using a self-supervised learning approach. The tasks seem more complex than the one in this paper for the rope manipulation domain and the approach is more flexible: after one training session it can in theory work for many tasks. I think it would be helpful to illustrate scenarios where the proposed method works better than [67].
Finally, in the video the robot seems to take many unnecessary manipulation steps, before it is able to achieve the task. It’s not clear if it’s due to the difference in the simulation and real-world, or if it is the behavior in simulation as well, which would be a bit concerning.
",Agreement accepted,"The paper presented a model free RL technique to solve deformable body manipulation problem. Deformable body manipulation is challenging in robotics, I think the proposed strategy of learning placing only and optimizing picking based on placing value approximator can inspire more works. The paper is well written and easy to understand. The main concern I have is whether the approach can generalize to more complex tasks, and the lack of comparison with the existing techniques (see my detailed comments below).

From the experiment result, learned picking strategies (including independent and conditional) perform much worse than uniform picking, when the picking up location is constrained to be four corners. But this behavior doesn't appear in the full cloth environment. Why is that? I would imagine the opposite, as learning to pick becomes harder when the action space is larger in full cloth environment. 

The conclusion that conditional learning speeds up learning in Section V.D. is somewhat inconsistent with the result in Fig.4, as conditional policy actually suffers from mode collapse.

The study with the independent and conditional strategy baselines is great, but there are lack of comparison with other existing deformable object manipulation techniques, such as those with demonstration.

The task the paper demonstrated on is limited. It is a spreading task and the picking place may not be very important. It would be interesting to see how well this technique can be applied to other more complex tasks such as folding, or knotting.

It is good to see sim-to-real works, and simply with domain randomization. From Fig.7, looks like physics randomization has no or even negative impact. Can you explain that?",Agreement accepted,"The paper is clearly written and easy to understand. 
The is focused on a specific aspect and experiments show that given the assumptions made for the method, the presented approach does help.

The main observation in this paper is that by treating the joint pick and place action as conditionals and optimizing the pick location away based on the conditional value function of the placing location. This is possible in this scenario given the very constraint action space which allows optimizing this in a brute force manner. However, it is not clear how this would scale to a more complex scenarios.
Further, it would not be clear how well this training procedure works if the initial sampling for learning the placing would neither have a dense reward nor a good initial uniform picking policy which actually picks the object itself. To be fair the experiments mention that learning the picking policy was also biased towards the object. From the experiments it seems that the essentially the image domain seems to hinder the joint policy learning given that the state based policies seem to perform almost equally or sometimes better. 
It is nice to see the sim to real experiments but given the constraints that the visual domain is required to be fairly similar to the simulation domain it would be interesting to explain why the approach taken in simulation for the state does not apply. The approach itself requires an additional calibration step which hints that a comparison to the state based problem should be considered also in real.
Although it is mentioned in the paper that no demonstrations are required, the presented approach essentially requires several problem/domain specific elements, a very specific action representation and additional problem specific sampling which boils down to having an essential infinite amount of demonstrations. It would be nice to have a comparison against a demonstration based approach with a low number of demonstration compared against the 10**5 samples required for the presented approach.

Additional literature to consider discussing for the paper:
Sim-to-Real Reinforcement Learning for Deformable Object Manipulation
Deep reinforcement learning with smooth policy update: Application to robotic cloth manipulation",,,,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/wilson1yan/rlpyt,,,,65.0,7kxkJlPuLz4
"We present an end-to-end algorithm for training deep neural networks to grasp novel objects. Our algorithm builds all the essential components of a grasping system using a forward-backward automatic differentiation approach, including the forward kinematics of the gripper, the collision between the gripper and the target object, and the metric for grasp poses. In particular, we show that a generalized Q1 grasp metric is defined and differentiable for inexact grasps generated by a neural network, and the derivatives of our generalized Q1 metric can be computed from a sensitivity analysis of the induced optimization problem. We show that the derivatives of the (self-)collision terms can be efficiently computed from a watertight triangle mesh of low-quality. Altogether, our algorithm allows for the computation of grasp poses for high-DOF grippers in an unsupervised mode with no ground truth data, or it improves the results in a supervised mode using a small dataset. Our new learning algorithm significantly simplifies the data preparation for learning-based grasping systems and leads to higher qualities of learned grasps on common 3D shape datasets [7, 49, 26, 25], achieving a 22% higher success rate on physical hardware and a 0.12 higher value on the Q1 grasp quality metric.",RSS2020_Author_Agreement.pdf,Min Liu (National University of Defense Technology)*; Zherong Pan (University of North Carolina at Chapel Hill); Kai Xu (National University of Defense Technology); Kanishka Ganguly (University of Maryland at College Park); Dinesh Manocha (University of North Carolina at Chapel Hill),Min Liu (National University of Defense Technology)*; Zherong Pan (University of North Carolina at Chapel Hill); Kai Xu (National University of Defense Technology); Kanishka Ganguly (University of Maryland at College Park); Dinesh Manocha (University of North Carolina at Chapel Hill),Differentiable_Grasp_Final.pdf (6868817 bytes); RSS2020_Author_Agreement.pdf (430616 bytes),Differentiable_Grasp_Final.pdf,62.0,66.0,,2.0,Deep Differentiable Grasp Planner for High-DOF Grippers,https://gamma.umd.edu/researchdirections/grasping/differentiable_grasp_planner,0.06582222002987399,"
Strengths:
1) Because it is hard for grasp learning to predict grasping points that are in exact contacts with the target object, the paper proposes a generalized grasping metric with inexact contacts based on the Q1 metric from citation [15]. 
2) It is interesting to combine analytical grasp planning and deep learning for multi-fingered grasp planning. This makes it possible for the grasping method to be used for both local optimal grasp planning and grasp deep learning. 
3) This paper shows the proposed grasping metric is locally differentiable. It derives the sub-gradient of the generalized Q1 grasping metric in two different ways based on previous work (citation [41] and [13]). 
4) It is nice to consider collision avoidance and forward kinematics for the grasp planning, in addition to the grasping metric.
5) The paper performs physical experiments for 50 YCB objects to show the proposed method outperforms an existing grasp planning work (citation [26]) in terms of grasp success rate. 
6) The paper writing is clear. 
7) The supplementary video shows cool real-robot grasping demos, though objects are always placed in roughly the same location on the table. 

Weaknesses: 
1) This paper misses important related work [1] [2] [3] [4] listed below. Multi-fingered grasp planning is formulated as a continuous optimization problem over the learned grasping success function in [1][2]. In particular [1] predates the work cited in this paper claiming to introduce the optimization-based learning approacht o grasping. [3] presents a multi-fingered grasping optimization approach leveraging the learned grasping function and the analytical constraints such as the reconstructed object sign distance field. [4] proposes a grasp learning approach for objects in clutter for parallel jaw grippers. 
[1]  Qingkai Lu, Mark Van der Merwe, Balakumar Sundaralingam and Tucker Hermans. Multi-Fingered Grasp Planning via Inference in Deep Neural Networks. IEEE Robotics & Automation Magazine (RAM) Special Issue: Deep Learning and Machine Learning in Robotics.
[2] Qingkai Lu, Kautilya Chenna, Balakumar Sundaralingam, and Tucker Hermans. Planning Multi-Fingered Grasps as Probabilistic Inference in a Learned Deep Network. International Symposium on Robotics Research (ISRR), 2017.
[3] Mark Van der Merwe, Qingkai Lu,  Balakumar Sundaralingam, Martin Matak and Tucker Hermans. Learning Continuous 3D Reconstructions for Geometrically Aware Grasping. IEEE International Conference on Robotics and Automation (ICRA) 2020.
[4]  Gualtieri, Marcus, Andreas Ten Pas, Kate Saenko, and Robert Platt. High precision grasp pose detection in dense clutter. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 598-605. IEEE, 2016.

2) There are at least five hyper parameters in the optimization objective function. 
3) Qualitatively, the grasping results are on par with those of GraspIt! (citation [30]), instead of outperforming GraspIt!. 
4) Each YCB object is only tested for one trial at roughly the same location for the physical experiments. It would make the results and conclusion more convincing by testing each object for multiple trials at different object poses. 
5) The paper does not mention what cameras and how many cameras are used for physical experiments. 
6) In the experimental setup section, it says there is an offline testing set. But the paper did not report the offline testing performance (e.g. the loss values) on the testing set. 
7) The proposed grasp learning method is limited to predict a single successful grasp for the same object, which might fail due to environment or task constraints. 

Questions:
1) How do you choose the object pose for the physical experiment?
2) Are all tested YCB objects unseen from training? 
3) The last sentence of Experimental Results section is unclear: “our neural network failed on the 5 objects due to slippage”. Does “neural network” refer to the grasp optimization with learning? Which 5 objects? 
4) Is the grasping neural network with ResNet-50 structure trained from scratch? Or is it fine-tuned from certain computer vision models? ",Agreement accepted,"The paper ‘Deep Differentiable Grasp Planner for High DoF Grippers’ proposes a network architectures that takes as input a set of depth images of a single object from multiple viewpoints and outputs a grasp pose and finger configuration for the Shadow hand. The main contribution of the paper is in the derivation of the loss function that is a differentiable version of the standard Ferrari/Canny metric Q. Specifically, the authors allow gradients event to flow if there is not yet contact between hand and object. They also handle self-collisions.

The authors show that their algorithm can be used as a local grasp optimiser when differentiating the novel loss with respect to the grasp pose and joint configuration. They also show that their algorithm can directly regress to a grasp pose and configuration given depth images. Additionally, the authors demonstrate their approach on a real platform and compare to a prior method.

Strength/Contribution:
- derivation of fully differentiable Q metric which can be used for 
— (i) optimising full hand configuration from an initial hand pose as thereby serve as a local grasp planner
— (ii) output a grasp pose and joint configuration directly 

Weaknesses: 
- Related work: 
— The reviewed related work is mostly sufficient and mentions important pros and cons of prior approaches. However, the authors completely ignore the fact that many of the reviewed works actually solve much more complex grasping tasks than shown in this paper, especially grasping in clutter with more than just one object on a clean table. Furthermore, the authors also ignore that these works do not require multiple depth images but just one. While it is hard to directly compare these very different approaches, the authors should at least comment on these shortcomings of their approach relative to related work. 
— Minor:
	— missing related work for high-DoF hands grasping single objects [a]
	— missing related work that are not sampling-based but use a fully-convolutional architecture to make pixel-wise predictions for grasping in clutter [b] -> the author do not seem to be aware of this kind of work that also seeks alternatives to sampling-based approaches

- Experiments:
— because of the aforementioned two assumptions (single object, depth images from multiple viewpoints), it is very hard to see how this work can have impact in comparison to work that is grasping complex objects in complex, cluttered scenes when only provided a depth or RGB image from a single viewpoint. It would have been helpful for comparison to prior work if the authors would have focussed less on the dexterous hand and rather on the differentiation of the Q-metric and how this newly gained loss helps to train superior learned grasp planners. For example, could the authors train a model that outputs a pose for a parallel-yaw gripper in a cluttered scene be better trained with this loss and achieve a higher success rate than a sampling approach like DexNet? As network architecture, the authors could assume something similar to [b] which predicts grasp quality and optimal pose per pixel in a depth image and works in realtime. These would be experiments that would let the reader really access the impact of the work in relation to prior work. Once this is shown, the author could also show the benefits of the approach for a dexterous hand. It is clear that because it is a more brittle hand, it is difficult to control it for grasping in clutter. 
— the authors make some vague claims about a comparison to a sampling-based method in Section VI.C. It is unclear if the authors use the EigenGrasp planner from [30] or just sample from the mesh surface a set of approach normals and close the fingers. Furthermore, Fig 7 only shows a few solutions of the proposed approach but no comparison to [30]. Therefore the statement that the results are on par is vague. It would have been more useful to compare to other sampling-based approach that also use depth images as input. Also the EigenGrasp Planner (if used) uses a lower-dimensional space for the high DoF. Therefore, a sampling-based approach should not suffer to much in the 2D space of grasp synergies. 

Other comments:
- cite [a,b] appropriately especially in relation to paragraph in related work, page 3, clmn 1
- the authors criticise sampling-based approaches for inferring a good grasp, yet they suffer from the problem that an object may be grasped equally well from multiple grasps and they ask their network to only output one. The authors solve this by pre-computing 100 grasps per training object and then use a chamfer-based loss to find representative grasps. They may want to emphasise more in the related work. Furthermore, the way the training data is generated based on this data loss is unclear in the first pert of Section V (specifically, “…. pick which grasp pose is most representable” is unclear.)
- in general , the paper is somewhat vague about when depth images are used in the experiments.
- Why does fine-tuning the network take so long and so many more episodes than pre-training?
- Success Plan seems meaningless as you didn’t take the geometry of the environment into account during training. How would any difference be a meaningful indicator of grasp quality for the two compared approaches? 

References:
[a] Q. Lu, K. Chenna, B. Sundaralingam, and T. Hermans, “Planning Multi-Fingered Grasps as Probabilistic Inference in a Learned Deep Network,”in Int. Symp. on Robotics Research, 2017.

[b] Douglas Morrison, Juxi Leitner, Peter Corke. “Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach”. RSS 2018.
",Agreement accepted,"The authors present a grasp metric definition that is fully differentiable, includes forward kinematics and terms to avoid undesired collisions. The function can be applied in the context of an optimization framework as a grasp planner for known geometries or in a learning framework. The experiments show improved performance in simulation and on a robotic platform compared to a prior version [26].

Contribution:
The contributions are stated  in the introduction. The paper presents a differentiable grasp metric that includes collision terms. It shows how to calculate its derivatives and a method to compute the collision terms without using signed distance fields but only having access to a watertight triangle mesh of the target object.

The paper is well written and contributions are clearly stated. The method and results are clearly of interest for the wider community. A few recommended improvements follow:

Related Work:
The following statement is incorrect: “All the existing grasp quality metrics have discontinuities”. A lot of prior work has used optimization to find grasps using differentiable metrics, e.g.:
[30]
“Grasping Unknown Objects by Exploiting Complementarity with Robot Hand Geometry” by Kiatos and Malassiotis
""Robotic grasping of unmodeled objects using time-of-flight range data and finger torque information” by Maldonado et al.
These are just a few examples from the top of my head. I’m convinced a thorough literature research will reveal significantly more examples.
“A grasp quality metric is only defined when the gripper and the target object have exact contact” Again, an incorrect statement (see metrics above).
Another relevant line of work are those that use learned, e.g.:
“Multi-Fingered Grasp Planning via Inference in Deep Neural Networks” by Lu et al. 
“This is in contrast with prior works [...]” (Sec. III) I think it makes sense to distinguish your approach from other works. But I think it would also help to show works that use the same/similar formulation as you do. Otherwise the reader might be left with the impression that your approach is unique in that regard.

Method:
Is $m$ (the scalar in matrix $M$, equation (1)) the constant that scales the importance of torques w.r.t. forces? If so, how do you choose it? (often it’s inverse proportional to the extents of the object) And maybe add sentence of what it means (right now you only write “user-provided metric tensor”).

Experiments:
The grasps all seem to be dominated by fingertip contacts (== precision grasps), without any palm contacts (at least none are visible in the figures or video). I assume this is a side effect of the optimization scheme / a local minima. Is there any idea on how power grasps could be favored?
I can’t find any comparable numbers from [30]. Instead you write: “We have compared the (standard) Q_1 metric of our method and that generated using sampling-based grasp planner [30] in Figure 7. The results show that the qualities of our grasp poses are on par with those of [30].”  How is this shown? The Fig. 7 shows a few examples and they have nothing to do with [30]. Would it be possible to add the results of [30] to Table II?
“Our depth cameras are calibrated beforehand to make the camera pose exactly same as the poses used for training.” This sounds non-trivial. How can this exactly be replicated?
Please add percentages for successes in Table II.

Video:
How is success measured in the real-world robot trials? I’m asking because in the video the banana is clearly slipping out of the hand but the video cuts to the next object (i assume this was counted as a success?).


Language / Typos
The paper is well written and easy to follow, two small typos:
P. 7: “our planner provides locally optimal and”
P. 7: “that generated using sampling-based grasp planner”
",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,https://youtu.be/j6QHj3-ClGE,,,66.0,90R9srzX_-I
This paper presents a formulation for swarm control and high-level task planning that is dynamically responsive to user commands and adaptable to environmental changes. We design an end-to-end pipeline from a tactile tablet interface for user commands to onboard control of robotic agents based on decentralized ergodic coverage. Our approach demonstrates reliable and dynamic control of a swarm collective through the use of ergodic specifications for planning and executing agent trajectories as well as responding to user and external inputs. We validate our approach in a virtual reality simulation environment and in real-world experiments at the DARPA OFFSET Urban Swarm Challenge FX3 field tests with a robotic swarm where user-based control of the swarm and mission-based tasks require a dynamic and flexible response to changing conditions and objectives in real-time.,RSS2020_Author_Agreement_Signed.pdf,"[Ahalya Prabhakar](https://apr600.github.io/) [Ian Abraham](https://i-abr.github.io/)Annalisa Taylor, Millicent Schlafly, Katarina Popovic, Giovani Diniz, Brendan Teich, Borislava Simidchieva, Shane Clark,  [Todd Murphey](https://murpheylab.github.io/)",Ahalya Prabhakar (Northwestern University)*; Ian Abraham (Northwestern University); Annalisa Taylor (Northwestern University); Millicent Schlafly (Northwestern University); Katarina Popovic (Northwestern University); Giovani Diniz (Raytheon); Brendan Teich (Raytheon); Borislava Simidchieva (Raytheon); Shane Clark (Raytheon); Todd Murphey (Northwestern Univ.),RSS2020_ErgodicSwarm.pdf (4646142 bytes); RSS2020_Author_Agreement_Signed.pdf (58398 bytes),RSS2020_ErgodicSwarm.pdf,1176.0,67.0,,2.0,Ergodic Specifications for Flexible Swarm Control: From User Commands to Persistent Adaptation,https://sites.google.com/view/ergodic-flexible-swarm-control,0.8524987612458871,"Overall comment: I am not entirely convinced this should be referred to as a swarm controller. It appears, to be effective, a lot of infomation must be passed between agents which gives the intuition it would not scale well. It may be more appropriate calling this a multi-agent control strategy.


Introduction:

	-The first paragraph could be make the problem space more clear.
		-E.G. ""One of the biggest problems in multi-agent control of robotic systems is the management and individualized control of the swarm of robots."" The way this could read is the problem is; to enable an individual human operator single out and control individual agents in the swarm or design individual agent control laws that coalesce into cooperative behaviors or design interfaces enabling collective management and control of a swarm by a single human.
		-E.G.""However, it is still necessary to develop a method that integrates both a framework to incorporate a user command into the supervision of a swarm and individual robot-level planning algorithms."" Like the previous example this is also unclear. An example may clear this up.

	-""Our approach motivates the use of flexible density descriptions where each agent is responsible for coverage of the full area, but can communicate its past and intended trajectory to the other agents. This allows for each agent to prioritize local exploration while ensuring coverage specifications are robust to network dynamics."" There is a lot of information presented here that make the approach that will be presented unclear and how it is different than [3]. E.g. What is a flexible density description, how the robots are covering the full area (patrolling?) while locally exloring, coverage specifications robust to network dynamics (do robots need to stay in proximity for communication)?

Algorithm:

	-Eq (1) there is nothing wrong with this but the standard variables used for control affine systems are typically f(x) + g(x)u. Having the natural dynamics as g may go against the intuition of many readers.

	-Eq (2) what is s?

	-What is a sine arbitrary spatial distribution? I'm assuming this is a Fourier series for a spatial density function but it should be made clear in the text.

	-Eq (5) u should be defined differently as the ensemble control.

	-The agent dynamics being independent is a fine assumption but this reviewer believes this statement is not entirely true due to the obstacle avoidance and potentially the RRT planner that is dependent on the other agents (negligable at low density but probably not at high density).

	-Second to last paragraph of left column of page 4: ""Minimizing the ergodic metric thus avoids issues often faced with multimodal optimizations as the robot will allocate proportional amounts of its time within some allotted time depending on the measure of importance specified by all the elements that are desirable (e.g., easter eggs and overriding user commands)."" I think this should be allocated proportional amounts of time within some allotted space?

Results:

	-The communication topology of the swarm is incredibly important to these results but is not really investigated or explained. Is everything presented here a star topology through a central computer, a fully connected graph, is there a communication range?

	-An interesting experiment would be to see how the cost function behaves with respect to some appropriate network topology metric. 

Figures:

Figure 1 is never referred to in the text.
There is no figure 2.
Figure 3 is not referred to in the text.",Agreement accepted,"Strengths: 
The video was helpful in understanding how the spatial allocation of robots behaved in practice. This was a nice complement to the heat maps used in Figures 9 and 10 and helped improve the clarity of the paper.

The work appears to be original and has the potential for being significant. I'd personally like to read more about the approach.

The derivation of the ergodic control law appeared to be solid and well thought through. The math made sense, though there were some assumptions that had to be made (e.g., I had to assume that saying that v<n in the paragraph between equation (2) and equation (3) meant that the distribution could apply to 2D even though robot dynamics were in 6D (3 spatial dimensions plus roll, tilt, and yaw)).

Areas for Improvement:
There was a mismatch between some of the claims of the paper and the evidence provided to support those claims. Mismatches include:
* Footnote 2 and the introductory paragraph if Section III assert that the work applies to heterogeneous agents, but the derivation and demonstration were only for homogeneous agents. Without the details, the claim about heterogeneity is not supported.
* The last paragraph under figure 9 claimed that the swarm uniformly covered the workspace, but there was no quantitative data to support this so it is difficult to have confidence in the claim (or even fully understand what ""uniformly covered"" means).
* The discussion claimed that the formulation could minimize human operator workload, but no data was gathered to support this assertion.  An expert in human factors might point out that understanding why the agents were doing what they were doing might require more workload than giving inputs to another algorithm like a sheepdog steering algorithm.
* The discussion claimed that as the number of agents was reduced the algorithm adapted, but no data was used to quantify what this meant. Similarly, claims were made about scalability and the effect of various communication topologies that were not quantified,
* A claim was made in the introduction that adjusting swarm behavior by influencing individual agents becomes less effective as the number of agents grows. This depends on the way individual agents are selected, how the individual agents affect the other agents, and so on. Approaches such as those from PB Sujit's lab and MA Goodrich's lab show the ability to influence a lot of agents using very few individuals, so the claim needs to be better scoped and explained.

There were some details that were missing that made it difficult to evaluate how generalizable the results were. Missing details include:
* How did the RRT* planning algorithms ensure that agents would not collide? It didn't appear that the ergodic part of the algorithm addressed collisions, so the collision-avoidance must have been done in the RRT* algorithm.
* The ergodic specification was decentralized, but it depended on all agents knowing the c_k parameters. A claim was made that the result was robust to varying ways of communicating C_k, but details were not included to explain how this would work.",Agreement accepted,"The paper is overall well written and is well structured. The formulation in section III was described in detail. Sufficient description was provided for each figure.

The idea of combining the ergodic planner’s tendency to explore recently unvisited locations and user specified locations seems to be beneficial. This allows the user to concentrate on finding interesting locations, rather than spending time managing the swarm to spread across the area.

The authors mention that they intend “to test human cognitive load” using the developed system in the future. Personally, I am interested in whether users would find certain aspects of the developed system easy or difficult to use and what might be the reason for those outcomes. For example, the usefulness of the tactile tablet seems trivial without a user study showcasing its advantages. For this paper’s purpose, perhaps an ordinary touchscreen tablet could have been sufficient to control the swarm?

It is not clearly explained how a user explores within the virtual reality environment. The paper suggests that the user is able to navigate within the simulated environment. However, there is no description of how this is done. The ability to move around to change the viewpoint is a distinct type of control that adds further workload to the user while monitoring the swarm. 

The presentation quality of the work requires input  from a senior author. Examples below.

The meaning of the term ergodic should be introduced to help the reader.

The first figure should not be placed before the abstract. Also on the first page, a figure should not span both columns. 

The abbreviation VR does not need to be introduced in the abstract, as it is not used there. It needs to be introduced in Section II, when first used.

Figures 4, 5 and 8 are not well formatted.

There are several minor typos/errors in the paper:
In page 1, “Our approach attempts to mitigate these issues through a decentralized strategy which is independent of an central control hub ...” This should be “a” instead of “an”.
In page 1, “[3] presents a decentralized, density-based coverage approach which influences multiples robots in a swarm from user commands.” This should be “multiple” instead of “multiples”.
In page 1, “Other planners that attempt to replan based on updates swarm often rely on” - this is not clear.
In page 5, “The parameter Σ is the width of the region of attraction (or repulsion) that can be tuned basd on the size of the task space and the desired granularity.” This should be “based” instead of “basd”.
In page 7, “The ergodic specification enables each agent to constantly generates actions ... ” This should be “generate” instead of “generates”.

Some references are incomplete, for example, [13]. Some titles in references do not use correct lower/upper casing, for example, [12].",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/apr600/ErgodicHumanSwarmCollab,,,,67.0,JO-KGEZ-X_c
"We consider the problem of dynamically allocating tasks to multiple agents under time window constraints and task completion uncertainty. Our objective is to minimize the number of unsuccessful tasks at the end of the operation horizon.We present a multi-robot allocation algorithm that decouples the key computational challenges of sequential decision-making under uncertainty and multi-agent coordination and addresses them in a hierarchical manner.The lower layer computes policies for individual agents using dynamic programming with tree search, and the upper layer resolves conflicts in individual plans to obtain a valid multi-agent allocation. Our algorithm, Stochastic Conflict-Based Allocation (SCoBA), is optimal in expectation and complete under some reasonable assumptions. In practice, SCoBA is computationally efficient enough to interleave planning and execution online. On the metric of successful task completion, SCoBA consistently outperforms a number of baseline methods and shows strong competitive performance against an oracle with complete lookahead. It also scales well with the number of tasks and agents. We validate our results over a wide range of simulations on two distinct domains: multi-arm conveyor belt pick-and-place and multi-drone delivery dispatch in a city.",RSS2020_Author_Agreement.pdf,Shushman Choudhury (Stanford University)*; Jayesh Gupta (Stanford University); Mykel Kochenderfer (Stanford University); Dorsa Sadigh (Stanford); Jeannette Bohg (Stanford),Shushman Choudhury (Stanford University)*; Jayesh Gupta (Stanford University); Mykel Kochenderfer (Stanford University); Dorsa Sadigh (Stanford); Jeannette Bohg (Stanford),RSS2020_Author_Agreement.pdf (145526 bytes); rss_camready.pdf (3194054 bytes),rss_camready.pdf,119.0,68.0,,2.0,Dynamic Multi-Robot Task Allocation under Uncertainty and Temporal Constraints,https://arxiv.org/abs/2005.13109,0.952308317781644,"The main idea of the algorithm is to use two levels, one for individual robots and one for coordination.  The idea of two levels has been around for a long time in motion planning for multiple robots to avoid collisions.  The existence of time windows considered in the paper helps, because robots tasks become unavailable and so robots do not need to try to avoid running into each other for a long time and give up the task to another robot (like it is shown in the video for the robot manipulators).
The optimality and completeness properties of the algorithm depend on not having new tasks appear online, which limits the value of the approach, since often in real situations tasks appear on line.  Yet, having those properties even under restricted conditions, is valuable. 
Interleaving planning with execution adds value to the method.
The experimental results include two very different domains, where the different algorithms tested perform differently, so adding confidence to the value of the approach proposed. ",Agreement accepted,"Overall, I think the idea is a reasonable one. I like the proof of optimality in the supplement, but am annoyed that it is not in the main manuscript. I would highly recommend cutting down on the other description and trying to get the proof into the main manuscript without making the reader read additional documents. 

The biggest complaint I have is wrt comparisons to other works. Given the abundance of work in this area, EDD and Hungarian are not reasonable baselines in my opinion. Through Section III (C), the authors list several other works ([5][20][49]) that this work is inspired by. Using some of those as a baseline would have been interesting. 

Table II only shows how quickly SCoBA runs. A key claim was that SCoBA scales well. However, there is no comparison with the baselines wrt time complexity of execution. I think this is a major flaw and needs to be fixed. 

Table I is very dependent on the setup. If there are four arms instead of three, or if the particular constants of pick up speed are changed, these numbers change. I would have liked some comparison that is generic, and compares it across generic MRTA values. 

",Agreement accepted,"In my opinion, this approach may has a significant impact to the robotic community but not only. Planning and Scheduling and sequential-decision making under uncertainty (dynamic programming, MDP, POMDPs) communities may also be interested. This approach merges two ""worlds"" reaching an important public.

Globally, the paper is well-written and easy to follow.
The proofs of optimality are given in the appendix (and are correct as far as I could check). I would advise authors to insert, at least, a proof-sketch in the paper corpus.

Meanwhile I have a couple of questions.
- Stochastic Conflict-Based Allocation:
*During the child generation (alg. 1), a new policy tree is computed considering the k task to be excluded. But, nothing guarantee that new conflicts will appear given remaining tasks and the new policy tree. Is it right? And, what happens if no solution is found? You need to replan considering a different number of robots? how do you solve this issue?
*At SCoBA level, what about the stochasticity? why is not the probability of a given conflict that is considered? For instance, there is a chance that an arm does not grasp an object, and if it is the case, and if I well understood, conflicts may arrive during execution if only the branch with correct grasp was considered for conflict resolution. How could you ensure to correctly handle such a small probability event?
*Could you provide an theoretical analysis in how but the complexity of the planning problem can be broken by making use of you hierarchical planner compared to the state-of-the art approaches that solve the entire problem. Computation time gives some idea of the efficiency. But a theoretical analysis would enrich the discussion.

Anyway the paper is original, with a strong empirical simulated evaluation.

Minor comments:
- the caption of table 1 and figure 5 relates to ""boxes"", while in the main text you talk about objects. Maybe use also ""objects"" in those table and figure.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,https://github.com/sisl/SCoBA.jl,,,,68.0,ouricUBKrVY
"Autonomous agents are limited in their ability to observe the world state. Partially observable Markov decision processes (POMDPs) model planning under world state uncertainty, but POMDPs with multimodal beliefs, continuous actions, and nonlinear dynamics suitable for robotics applications are challenging to solve. We present a dynamic programming algorithm for planning in the belief space over discrete latent states in POMDPs with continuous states, actions, observations, and nonlinear dynamics. Unlike prior belief space motion planning approaches which assume unimodal Gaussian uncertainty, our approach constructs a novel tree-structured representation of possible observations and multimodal belief space trajectories, and optimizes a contingency plan over this structure. We apply our method to problems with uncertainty over the reward or cost function (e.g., the configuration of goals or obstacles), uncertainty over the dynamics, and uncertainty about interactions, where other agents' behavior is conditioned on latent intentions. Three experiments show that our algorithm outperforms strong baselines for planning under uncertainty, and results from an autonomous lane changing task demonstrate that our algorithm can synthesize robust interactive trajectories.",RSS2020_Author_Agreement.pdf,"[Dicong Qiu](https://www.isee.ai), [Yibiao Zhao](https://www.isee.ai), [Chris Baker](https://www.isee.ai)",Dicong Qiu (iSee); Yibiao Zhao (iSee); Chris Baker (iSee)*,RSS2020_Author_Agreement.pdf (104233 bytes); Partially_Observable_DDP_final.pdf (1825510 bytes),Partially_Observable_DDP_final.pdf,1279.0,69.0,,2.0,"Latent Belief Space Motion Planning under Cost, Dynamics, and Intent Uncertainty",https://davidqiu1993.github.io/poddp-paper,0.48459836962673797,"This paper presents a variant of differential dynamic programming that allows for optimization with latent discrete dynamics parameters. A POMDP is defined over observable continuous state and action spaces with non-linear dynamics. The dynamics are further parameterized by latent discrete variables. An algorithm is derived that performs DDP over a horizon with all possible assignments of the latent parameters. The method is evaluated in 3 toy tasks and compared to other variants of DDP.

The method is described well and the algorithm is stated clearly. The experiments provide a nice illustrative example of the algorithm execution which helps understanding the method. Further, the addressed POMDP is sufficient to model an interesting range of tasks.

I see 3 main points of potential improvement that the paper could benefit from.
The latent discrete variables as defined in Fig. 1 have the limitation that they are not controllable, i.e. they are not functions of the actions. If this restriction was removed, the method could be applied to an even broader range of dynamical systems (for example, manipulation / locomotion with discrete contact variables, etc.).
One drawback of the algorithm is the exponential complexity w.r.t horizon length and dimension of the discrete parameter Z. I like the suggestion in Section IV C) for a naive way of addressing the issue. It would be interesting to see if there are other ways to exploit structure in typical use-cases of Z that could help make the method computationally tractable.
The experiments are focused on cases where Z is a binary variable. It would be interesting to showcase PODDP on more complex problems. Further, I am missing a comparison of computational efficiency with the baseline algorithms.
",Agreement accepted,"The topic of the paper is relevant and the authors do a good job motivating and framing it within the state of the art. The approach is reasonably well explained and the paper is easy to read. However, the approach seems to take a strong assumption and a direct adaptation of known algorithms; doesn't compare itself against interesting baselines; and is not proven to scale to any kind of realistic system with multimodal beliefs. More specifically, I'd like to see the following issues addressed in the rebuttal phase:


1. The algorithm is dependent on the fact that one can build a discrete tree by assuming a discrete latent variable and MLO observations. The latter assumption seems rather strong, but there is no discussion on it in the paper and the empirical evaluation doesn't study its impact. Furthermore, it seems to me that after one makes this strong assumption, the resulting algorithm doesn't really bring significant novelty to the state of the art.

2. Given the assumptions made that allow for a discrete tree representation of the trajectories, couldn't one use something like POMCP directly, even if with a coarse discretization of actions? This would be a nicer baseline to compare against.

3. The experiments only deal with binary latent variables, which kind of defeats the purpose of developing an approach for multimodal beliefs. Furthermore, no results on scalability are provided. The approach is only evaluated on really small examples, making me doubt its feasibility for realistic problems.",Agreement accepted,"The paper is technically sound and all necessary details are given. 
The approach is well evaluated in different experiments.
Nevertheless, the paper can be improved in some areas.
Although the evaluation is well done, the presentation of the results can be improved.
For instance, the authors should show an overview of the considered scenario in part A of the evaluation. 
The graphs in Fig. 3 are hard to read and interpret.
Considering Fig. 3b, the authors should briefly explain why two trajectories are not reaching the goal for completeness. 
The readability of the Fig. 4 and 5 can also be improved. 
In particular, Fig. 5 misses labels to indicate important details. 
For reproducibility, the authors should summarize necessary parameters and details of each scenario.
The evaluation can further be improved by adding computation times and showing scenarios in which the other two approaches (MLDDP and PWDDP) may perform better.

Some other minor issues are:
* Usually, autonomous vehicles have no uncertainty in ""the mode of the vehicle and its components"" (see introduction). The authors should provide references here. 
* For completeness, the authors should show additional steps between Eq. 4 and 5.
* Since POMDP solvers significantly differ in the performance, the authors should provide more details on the performance of their proposed solution, e.g., computation time, complexities, depth of the tree and branching factor. 
* On p. 5, the authors should give more explanations (and references) to the claim ""because perturbations can push the belief off of the |Z|-1-dimensional simplex"". Also, provide the necessary computation steps afterwards. 
* What variable corresponds to the ""uncertainty level"" (see results sections)? The authors should refer to this variable. 
* The trajectory partition needs to be explained in more detail.
* Can the authors comment on improving the performance for T>=5?
* For easier reference, can the authors mention the meaning of each variable in Alg. 1 again?

The paper is well written and the authors guide the reader through each section. 
Yet, minor issues are:
* The BibTex files needs revision, e.g., capitalization of ""Markov"" in [1], ""CHOMP"" in [2], conference name missing in [5], full author list missing in [12], pages missing in [15], [22], [25], [26], [27]. Please check each reference. 
* The colon before Eq. 3 is misplaced.
* What do the authors want to say when using \doteq in Eq. 1?
* Why are the authors using \cdot in Eq. 1?
* Please use big brackets if possible to improve readability, e.g., in Eq. 5.
* A bracket is misplaced in ""a(n"" on p. 4.
* The authors should use proper set notations for ""1:|Z|"" in caption of Fig. 2.
* The authors should use \eqref when referencing equations (instead of writing ""Equation 1"" or Eq. 7"").
* Please reference algorithms using \ref in the text. 
* In Eq. 8, the variable \tilde{Q} is dangling and should be placed on the page before instead. 
* The authors should balance the columns on the last page.

The proposed approach is interesting and addresses an important problem. 
The results are promising and well presented. 
The authors are encouraged to revise the paper for improved readability.",Agreement accepted,,,Agreement accepted,07/15 15:00,07/15 17:00,,https://davidqiu1993.github.io/poddp-paper,,,69.0,zVNIqI9f30g
"Untethered small-scale soft robots have promising applications in minimally invasive surgery, targeted drug delivery, and bioengineering applications as they can access confined spaces in the human body. However, due to highly nonlinear soft continuum deformation kinematics, inherent variability during fabrication on the miniature scale, and lack of accurate models, the conventional control methods cannot be easily applied. Adaptivity of the robot control is additionally crucial for medical operations, as operation environments show large variability and robot materials may degrade or change over time, which would have deteriorating factors on the robot motion and task performance. In this work, we propose using a probabilistic learning approach for millimeter-scale magnetic walking soft robots using Bayesian optimization (BO) and Gaussian processes (GPs). Our approach provides a data-efficient learning scheme to find controller parameters while optimizing the stride length performance of the walking soft millirobot. We demonstrate adaptation to fabrication variabilities and different walking surfaces by adopting our controller learning system to three robots within a small number of physical experiments. ",RSS2020_Author_Agreement_1301.pdf,"[Utku Culha](https://utkuculha.com/), [Sinan Ozgun Demir](https://pi.is.mpg.de/person/sinandemir), [Sebastian Trimpe](https://ics.is.mpg.de/person/strimpe), [Metin Sitti](https://pi.is.mpg.de/person/sitti)",Utku Culha (Max-Planck Institute for Intelligent Systems); Sinan Ozgun Demir (Max Planck Institute for Intelligent Systems); Sebastian Trimpe (Max Planck Institute for Intelligent Systems); Metin Sitti (Carnegie Mellon University)*,RSS2020_1301_revised.pdf (2468195 bytes); RSS2020_Author_Agreement_1301.pdf (53749 bytes),RSS2020_1301_revised.pdf,1301.0,70.0,,3.0,Learning of Sub-optimal Gait Controllers for Magnetic Walking Soft Millirobots,,0.34827072599347497,"I think this is an interesting paper, and the videos also show interesting demonstrations. However, in my opinion, it has a critical flaw in its current form: all results presented are anecdotal, in that there is no statistical analysis of any of the results. This is a shame, because the authors have collected a lot of data (maybe not enough?) that could have been easily analyzed for statistical significance. As a result, it is unclear if any of the results in this paper could be reproduced.

Here are a number of smaller comments:

In the caption of Figure 2, where you say ""x-axis"", you should say ""abscissa"" instead. You are using y- and z-axis in that same caption with a very different meaning.

In that same caption, for improved clarity, I suggest moving ""(middle)"" directly after ""magnitude"", and I suggest putting a comma after ""(3)"".

In that caption, and more generally in the paper, can you please clarify that (which I'm assuming) you only instantaneously pass through flat-out state.

In the first paragraph of Section II, where you say ""same material properties"" I suggest saying something like ""same nominal material properties"" to better highlight this idea that each robot is really different from the others.

In the second paragraph of Section II, you never explicitly say that the field is uniform (you only get to that later in the experimental portion of the paper).

In the second paragraph of Section III, there is kind of a strange mix of range and interval notation (e.g., [10 - 50]).

There are plus/minus values reported throughout this paper, but it is never made clear what convention they are (e.g., standard deviation, standard error, etc.). Similarly, the whiskers in Figures 6 and 7 are not defined.

In the asterisk comment under Table I, you say ""variance of 0.40 mm"". I don't think this can be correct. The units of variance would be mm^2, no?

In Figure 4, you stack images vertically, but in Figure 5 you stack them horizontally. I suggest you do the same thing in both images to improve clarity.

I'm confused by the values reported in the last paragraph of Section IV-B. First, they don't seem to match what is being shown in Figure 4, where average stride length seems to be something like 1.5 mm, nor do they match what is shown in Table 1. Second, why is it that performance with prior information decreases (i.e., smaller step size)? Isn't that opposite of what you would expect?

In the first sentence of Section IV-D, you state a ""significant impact"". You shouldn't use the term ""significant"", since you haven't provided any statistical analysis.",,"The work has an excellent motivation, meaningful problem statement and goal. The scope of the work is clear, and it is refreshing to read a paper which so clearly articulates what exactly is being studied (and what aspects of the work are not being claimed as novel). 
There is a clear need for this type of result in the micro-robotics community. This data-driven approach nicely complements the recent state of the art advances in microrobotic fabrication and control, which are primarily guided by physics-based models. This work studies whether a data-driven approach could yield a better controller for one particular gait of walking for a flexible magnetic sheet. The authors wisely choose to study the exact device already published in several works, which allows them to make fair comparisons in a meaningful and helpful way to the community. 
I think this type of approach should be adopted as an additional tool for the micro-robotics community, which (unlike most sub-fields in robotics) thus far mostly avoided the use of ML techniques for design and control. The authors have accurately captured the micro-robotics specific data collection challenge here, and so I see this work as being very valuable for the community. While a physics-based approach will likely continue to play a dominant role in micro-robotics research approach, publications such as this one will help the community evaluate the value of data-driven approaches.

The primary thesis of this work is that due to a large search space (four controller parameters over a continuum of values), it is not feasible to experimentally evaluate the entire search space to find the optimal control inputs. Logically, this motivates the use of Bayesian optimization. However, the paper does not explicitly test the thesis, which would require the authors to test a ""brute force"" search over the search space in a random or systematic manner (with the same 20x3=60 experiments). I presume that doing so would result in a poorer stride length than the BO method, but it would allow you to more directly claim the success of the method.

Comments on the paper:
A reader of this paper likely needs to be familiar with Bayesian optimization and Guassian process method to understand the paper fully because the methods are only explained briefly. Given that the most valuable target audience of this work may be the micro-roboticists who thus far have shied away from data-driven methods, those people may not have the background needed to read the paper. My suggestion for this paper to have maximum impact would be to include some additional basic descriptions of the algorithms used to ""hold the hand"" of the reader in section III. 
The primary result of the paper is not stated in the abstract.
The authors interchangeably use the terms millirobot and miniature robot. It might be clearer to choose one.
On page 3, it is stated that there are 203,520 possible parameter sets. This is an arbitrary number dependent on the step sizes, which are not stated. Can you make a more disciplined argument for a particular step size based on the expected sensitivity to each parameter in the observed data?
The results of Table 1 show that robot 2 gets a better stride length without the prior! This is counter to the claims made throughout the discussion section which claim that the prior always helps.
Figure 7: is the vertical axis the stride length, or the improvement in stride length?
The Conclusions section of the paper is summarizing and re-stating claims that are already made elsewhere. It does not add value to the paper and should be removed.
",Agreement accepted,"## 1. General feel:
The authors adopt a method for manufacturing millimiter-scale ""millirobots"" that are activated using an external field, and then apply Bayesian Optimization to determine a suitable parameter set from a search space containing four free variables.

The motivation seems reasonable, namely to improve the locomotive efficiency of such a robot.

However, I question the approach and the results. Major comments:
1. Why Bayesian Optimization? There are numerous other possible algorithms for optimizing robot gaits, including ""Intelligent Trial-and-Error"" [1], genetic algorithms [2, 3], policy gradients [4], and many others throughout various areas of robotics.
2. Why is your data so noisy? The paper you are using as your benchmark (reference [27 of submission 1301], specifically the most-relevant section on walking P 49-54 of the supplementary information, but also for other gait classes) has much lower noise than e.g. your Fig. 4, 5, 7. 
- Related, you report many values as ""X +/- Y"" without stating the width of this confidence interval. Is that one SD? 95% CI? etc.
- Related, did you report the benchmarks' actual mean and variance, or are the reported ""benchmarks"" the values obtained on your hardware with their parameters (by assumption, you are referring to the parameters in Fig. S13 on Page 49 of the supplementary information of [27 of submission 1301])?
3. The data noise puts into further question the utility of the reported results.

Combining the points in item 2 and 3 above, it is unclear what p-values you use for drawing conclusions. Thus, even though you might actually have presented significant improvements and simply  have issues with your specific hardware manufacturing setup, it is unclear how others can benefit from and build upon these reported results.



## 2. Technical merit, etc.:
I'll reference the text as p[page in manuscript = PDF page -1].[lines]

p1 The claim ""safe human, which are hard to achieve using conventional rigid materials"" is misleading. Collision avoidance is a well-developed field, including quite impressive results even dating back to, for one of many examples, 2012 [5]
p1 ""However, these controllers typically depend on the continuous sensing of symmetric body deformations and computationally heavy model solutions"" ... what does this mean?
p1 The tasks mentioned in the end of the second paragraph are not unique to medical robotics.
""Soft mobile robots targeting medical applications have further constraints such as the dynamic task environment, complex deformation kinematics, fabrication-dependent performance variations, and actuation/sensing limitations, which require adaptive and data-efficient control methods [14].""
p2 How much does restricting alpha affect the results? This could be an interesting study: finding the marginal contribution of each parameter, and which ranges are useful.
p3 ""In addition to the virtual infinite degrees of freedom inherited by the soft materials, the controller parameters existed in a continuous space, making an exhaustive manual search using physical experiments impractical."" This is nonsense. Doing grid search or some guided binary search should reduce the number of experiments to a manageable number. This should be compared to the proposed GP method. Furthermore, design of experiments is a well-developed field, and might provide further insight. Not to mention other methods like PSO, gradient descent, etc., that could be adapted, in addition to the algorithms mentioned in my comments above (references [1,2,3,4]).
p3 ""The magnetic soft millirobots in our paper do not have an inverse kinematic (see Fig. 2b) or dynamics model that would allow us to run a systematic numerical analysis to find the control parameters for the optimum stride length performance."" While this is partially true, it is severely downplaying the extensive characterization, theoretical analysis (definitely incomplete, but still more than the authors of submission 1301 would make it seem) and data available in [27 of submission 1301].
p3 ""Therefore, following the arguments in [27], α1 and α2 are limited to [10 - 50]° and [40 - 80]° respectively."" What arguments? State briefly.
p3-4 Please explain what the primes ' and asterisks * mean when applied to your variables. I interpret ' as transpose, but subtracting \theta^' from \theta results in a dimension mismatch for a vector \theta and previously you say it is describing a 1-D case.
p4 ""The center of the uniform magnetic field coincides with the center of the test environment and has a size of 40 mm3"" Is it 40 mm edge length, or 40 mm3 total volume?
p4 If you colored the x'es with the same color scheme (maybe with a black outline), it might make it easier to interpret the error. Currently it is unclear how effective the regression is in this parameter plane.
p5 The experimental design seems unjustified/random. WHy 18 trials, then 38, then 17, then 50 for each treatment?
p6 Table 1 Are these variances plus minus 40, or variances *of the mean*? 0.4 seems lower than what your data suggest (e.g. Fig. 4,5)
p7 ""Also, the kinematic models of the small-scale robots can be improved by utilizing the constant curvature (CC) approximations [10] and finite element analysis (FEA) methods [11]."" What about the analytic solutions presented in [27 of submission 1301]?


[1] A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret, “Robots that can adapt like animals,” Nature, vol. 521, no. 7553, pp. 503–507, May 2015, doi: 10.1038/nature14422.
[2]S. Kriegman, S. Walker, D. Shah, M. Levin, R. Kramer-Bottiglio, and J. Bongard, “Automated shapeshifting for function recovery in damaged robots,” in Robotics: Science and Systems, Freiburg im Breisgau, Germany, 2019.
[3]C. Paul, F. J. Valero-Cuevas, and H. Lipson, “Design and control of tensegrity robots for locomotion,” IEEE Transactions on Robotics, vol. 22, no. 5, pp. 944–957, Oct. 2006, doi: 10.1109/TRO.2006.878980.
[4]F. Sehnke, C. Osendorfer, T. Rückstieß, A. Graves, J. Peters, and J. Schmidhuber, “Parameter-exploring policy gradients,” Neural Networks, vol. 23, no. 4, pp. 551–559, May 2010, doi: 10.1016/j.neunet.2009.12.004.
[5]F. Flacco, T. Kröger, A. D. Luca, and O. Khatib, “A depth space approach to human-robot collision avoidance,” in 2012 IEEE International Conference on Robotics and Automation, 2012, pp. 338–345, doi: 10.1109/ICRA.2012.6225245.


## 3. Comments on Multimedia (Videos, etc.)
+ scale bar
+ consistent sizing and viewing angle
- lighting (video 1 is appropriate, Video 2 is darker, video 3 is very dark)
- None of the video speeds make sense, which diminishes the value of the multimedia.
    Video 1 has different speeds, and it is unclear why each speed was chosen. For example, why speed up the top row? It already appears jumpy, and speeding it up just makes it look more jumpy. For clarity, I recommend putting all gaits on the same 1x speed, since none of the speed adjustments appear justified.
    The difference in speed between video 1 and 2 is confusing to compare ""optimal"" with the others
    Video 2 should have all robots at the same speed in order for the comparison to be more meaningful.

    Same comment for Video 3. Why did you slow down the parameters which were optimized for the smooth surface? It seems if you're trying to compare the parameters that were optimized for smooth vs. rough, on the same rough surface... that you'd put both at the same speed.",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,,"https://utkuculha.com/wp-content/uploads/2020/06/RSS_Supplementary_Video1.mp4,https://utkuculha.com/wp-content/uploads/2020/06/RSS_Supplementary_Video2.mp4,https://utkuculha.com/wp-content/uploads/2020/06/RSS_Supplementary_Video3.mp4",,,70.0,eZbyL_jNjn4
"In this work, we present a semi-supervised learning method to transfer human motion data to humanoid robots with varying kinematic configurations while avoiding self-collisions.To this end, we propose a data-driven motion retargeting  named locally weighted latent learning which possesses the benefits of both nonparametric regression and deep latent variable modeling.The method can leverage both paired and domain-specific datasets and can maintain robot motion feasibility owing to the nonparametric regression and graph-based heuristics it uses. The proposed method is evaluated using two different humanoid robots,the Robotis ThorMang and COMAN, in simulation environments with diverse motion capture datasets. Furthermore, online puppeteering of a real humanoid robot is implemented.",20030R.Robotics-Foundation-DRS-Author-Agreement.pdf,Sungjoon Choi (Disney Research)*; Matthew Pan (Disney Research); Joohyung Kim (University of Illinois Urbana-Champaign),Sungjoon Choi (Disney Research)*; Matthew Pan (Disney Research); Joohyung Kim (University of Illinois Urbana-Champaign),manuscript.pdf (7881981 bytes); 20030R.Robotics-Foundation-DRS-Author-Agreement.pdf (400912 bytes),manuscript.pdf,84.0,71.0,,3.0,Nonparametric Motion Retargeting for Humanoid Robots on Shared Latent Space,[Not Answered],0.407394788737669,"There are a few things I do not quite understand for this paper.  Once a shared latent space is created, why is it necessary to do a nearest neighbour search?  You can simply use the decoder to compute a corresponding pose of the robot - am I missing something?  The subsampling method sounds good - although the distance metric sounds quite naive.  It is only for the poses of the two arms.  Then, how is it going to be managed when the legs of the robot are also involved?   The result video appears very noisy and discontinuous.   I think a method based on spatial relations will produce far smoother motions compared to what I see here - maybe it should be compared with those.   Some motions like dual arm rotations look very dissimilar to the motion of the human. 

Molla, Eray, Henrique Galvan Debarba, and Ronan Boulic. ""Egocentric mapping of body surface constraints."" IEEE transactions on visualization and computer graphics 24.7 (2017): 2089-2102.

Jin, Taeil, Meekyoung Kim, and Sung‐Hee Lee. ""Aura mesh: Motion retargeting to preserve the spatial relationships between skinned characters."" Computer Graphics Forum. Vol. 37. No. 2. 2018.

Overall, I think the method sounds fine - the LPP module sounds very useful for producing a good mapping from imbalanced training data.   On the other hand, the other parts sounds a bit unclear - such as the nearest neighbour search, etc. The method sounds like a hybrid approach of deep learning approaches and classic approaches, but the justification of the entire pipeline is not satisfactory. I think there could have been some other approaches say, based on cycle-GAN to produce a better mapping between the two. 

”Once an encoder/decoder pair is constructed for each domain, we deploy locally weighted regression on the latent space to find a mapping from one domain to the other.""  -  I do not understand this part too.  If the pose is in the shared space, why is it necessary to do a locally weighted regression?   A regression from which domain to which domain?   

minor typos:
page 4, right column:  that if when we apply
Fig 4, caption:  Uniform Samplpling
Tab 2. Sef collision
page 7:  better retargeting results *than* the baseline



",Agreement accepted,"The paper is well written and structured. The techniques of choice and assumptions are justified clearly and the overall approach is sound. The novelty stands from combining Wasserstein auto encoders with locally weighted regression on the embedded space, and the incorporation of collision handling and sub sampling for the retargeting task. The approach is, however, not a simple concatenation of previously presented techniques. The entire pipeline requires the definition of several quantities such as divergence and distance function and losses for the WAE, local parameter k for the local regression, DPP as a subset sampling mechanism for more accurate latent space learning. The authors excelled in making sure all the components are connected and justified. 

My main criticism is the experiments and the comparisons provided. The paper only presents comparisons to one other method [3] and no ablation studies are reported. The paper would benefit from a more detailed evaluation on the various choices. For example, it would be interesting to see the performance of the method with another regression technique instead of LWR, for example Gaussian processes, that can learn the parameters of the kernel directly. With today's ML tools and variational inference, GPs are fast and can scale to very large datasets. How sensitive is the method to different values of k? How does the performance improves with data augmentation of different sizes? And finally, how does it compare to a simple behaviour cloning strategy constrained by collisions? These comparisons and discussions would make the paper significantly more impactful. 

Overall, I believe there are sufficient novel ideas and the quality of presentation is excellent making the paper a solid contribution to the conference.   


",Agreement accepted,"This paper presents a framework for mapping motions from a robot to another robot.
The proposed framework learns the latent space shared by motion domains of two different robots.
For learning the shared latent space, Wasserstein autoencoder is adapted in this study.

The contribution of the paper is 1) to propose the framework for learning the latent space shared by two different robot pose domains, 2) the heuristic to check the feasibility of transitions, and 3) a trick for training neural networks using imbalanced data sets.

Regarding the first contribution, the objective function in Eq.(4) and (5) seem similar to style transfer GAN, although the paper is not cited.
""Image Style Transfer Using Convolutional Neural Networks"" Gatys  et al., CVPR 2016.
I recommend the authors to cite the style transfer GAN paper and discuss the relation.

I summarize the strong and weak points of the paper:

Strong points:
- The entire algorithm seems work well as verified in the experiments. The proposed method reduces the self collision while keeping the tracking performance comparable to the baseline.
- The heuristic for checking the feasibility of transitions looks practical
- LA-DPP look also practical and I can see from equations that LA-DPP should be more computationally efficient than the original DPP. 


Weak points:
- The paper requires some revisions to improve the presentation. Especially, the way of using the locally weighted regression is not clear.
  Please refer to the following comments. I suggest to put a pseudo code in the method section.

- Regarding the second contribution, the benefit of the feasibility check of the transitions are not explicitly evaluated in the experiment section.

- Regarding the third contribution, the computational efficiency fo proposed LA-DPP over the original DPP is not quantitatively evaluated in the experiment.

Detailed comments on presentation:

- I do not clearly understand how the locally weighted regression is used on the latent space.

  From the term ""locally weighted regression"", I think of something presented in this webpage.
  https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/cohn96a-html/node7.html
  ""k"" can be any positive real number in this case.

  However, the authors described, ""setting k = 1, as the proposed LWL2 becomes a table look-up method.""
  I do not understand this sentence. It is necessary to clarify how the locally weighted regression is used in the proposed framework.

  In addition, I do not clearly understand why we need the locally weight regression and why we cannot directly reconstruct the motion using the decoder P(z).

- I do not understand the third paragraph of Section IV.D. Specifically, I do not understand the black squares in Fig.2.

- In Eq.(4) and (5), $x^l_i$ is used, but its definition seems missing, although $x_i$ is defined.
I understand that $x^l_i$ is the $i$th robot pose data point in the domain l, but it should be explicitly described in the text.

- In the third paragraph of Section III, there are some equations using R(:,3). 
This programming-language-like expression should be avoided and please use mathematically correct equations.
In addition, it seems that ""R"" is a rotation matrix, although it is defined as simply ""orientation"" in the text. 
If necessary, the reason why the use of the capsule representation is computationally efficient can be described in the supplementary material.

Minor comment:
- I suggest authors to have a look at ""AUC optimization"", which address the class imbalance in the context of classification problems. It maybe useful for future work.
",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,[Not Answered],[Not Answered],,,71.0,99r7-Pwf_f4
"Shared autonomy provides an effective framework for human-robot collaboration that takes advantage of the complementary strengths of humans and robots to achieve common goals. Many existing approaches to shared autonomy make restrictive assumptions that the goal space, environment dynamics, or human policy are known a priori, or are limited to discrete action spaces, preventing those methods from scaling to complicated real world environments. We propose a model-free, residual policy learning algorithm for shared autonomy that alleviates the need for these assumptions. Our agents are trained to minimally adjust the human’s actions such that a set of goal-agnostic constraints are satisfied. We test our method in two continuous control environments: Lunar Lander, a 2D flight control domain, and a 6-DOF quadrotor reaching task. In experiments with human and surrogate pilots, our method significantly improves task performance without any knowledge of the human’s goal beyond the constraints. These results highlight the ability of model-free deep reinforcement learning to realize assistive agents suited to continuous control settings with little knowledge of user intent.",RSS2020_Author_Agreement copy.pdf,"[Charles Schaff](https://ttic.uchicago.edu/~cbschaff/), [Matthew R. Walter](https://ttic.uchicago.edu/~mwalter/)",Charles Schaff (Toyota Technological Institute at Chicago)*; Matthew Walter (Toyota Technological Institute at Chicago),rss2020.pdf (555197 bytes); RSS2020_Author_Agreement copy.pdf (101391 bytes),rss2020.pdf,133.0,72.0,,3.0,Residual Policy Learning for Shared Autonomy,https://ttic.uchicago.edu/~cbschaff/rsa/,0.802763553661241,"Summary:
This paper develops a method for shared autonomy between a human and a robot. Prior work generally assumes that the agent has access to information about the set of goals for the task or the environment dynamics. The authors’ approach does not rely on this information and instead aims to learn a policy that augments the human while satisfying some constraints. They add the agent’s action as a residual correction to the human’s actions and use a formulation based on constrained MDPs to ensure that the agent follows the constraints set by a reward function R_{general}. The method is evaluated on multiple domains with both simulated and real human users, highlighting that having an agent copilot a human leads to significantly better performance than having a human act alone.

Originality:
The work has some original components, such as taking existing methods (e.g., residual policy learning) and applying it to a shared autonomy scenario. However, there are a lot of works in the space of human-agent joint RL so the authors should spend more time talking about how this work differs from these, not just how the work differs from the most related works (e.g., Reddy et al. and Broad et al.) 

Clarity:
The paper is well-written and clear. The figures are well-done (e.g., Figure 2).

Quality:
The paper is of high quality. The experiments show that the agent copilot helps the human in both simulated and real human settings. There could have been more baselines to show that the agent is really helping at the right points (more below in other points), like comparing with randomly-assisting agents or agents that provide bad assistance. 

One point to fix in the paper though is that the introduction of the paper and other places throughout the text slightly oversell the work in that it claims that this work doesn’t require any information about goals, environment dynamics, etc, but it sounds like the main goal of the paper is to support the human in the task. In this setting, it’s not that the agent is learning the goals autonomously. It is instead just deflecting to the human, which is not what is expected when reading through the abstract and introduction. 

Significance:
The work is significant for the community as the method can support effective shared autonomy between humans and robots. I appreciate that the authors included both simulated human and real human experiments, as that leads to a stronger evaluation and better generalization to the community. 

Other comments:
- Related to the point in the quality section, there is a statement about prior work often “requir[ing] access to demonstrations or the user’s policy for achieving each goal,” but this work also requires similar data from humans. I would recommend in this place and throughout the paper, to be careful about overselling. The paper seems to be about developing an agent that can assist humans rather than an agent that can act under unknown goals and dynamics.
- In Results, this point comes again, where the authors state that the agent generalizes, despite having no explicit or implicit knowledge of the task. But, this is debatable as the agent does have R_{general} and the rest of the task-specific components are taken care of by a human. This statement should be revised to be more clear about this.
- For experiments, the authors compared a human acting alone vs. a human acting with the copilot. It would have been nice to include baselines such as random assistance (randomly providing assistance to the human at about the same frequency as the assisted condition) and bad assistance (assisting at exactly the moments the assisted condition would not).
- Before the conclusion, the authors say that the agent responded slowly to user commands. Why was this? Was it because the algorithm ran slowly or that the authors set up a time delay in the experiments?",Agreement accepted,"Regarding originality, this paper provides a novel perspective on shared autonomy, in that it focuses on training a task-agnostic agent that can effectively help human users without having to know the user's intent. This can be applied to any domain that has constraints (e.g., avoid collisions, stay upright) that are independent of what the goal is. The algorithm itself is not especially novel -- it combines residual policy learning with constrained policy optimization, both of which have been proposed by previous work. (On that note, it's worth briefly citing related work on constrained policy optimization in the paper, e.g. [1] and [2].) In addition, the contribution of the paper is somewhat overstated. Even though the agent itself is model-free and thus does not learn a model of the environment dynamics, the simulator used for training implicitly contains knowledge of environment dynamics. In particular, there are no guarantees that the agent will work if the dynamics at test time (e.g., in the real world), are different from those in the simulator. This limits the ability to use this approach for real-world applications.

Regarding quality and clarity, this paper is well-motivated, written very well, and easy to understand. Both the approach and the experimental design are explained clearly, and the experiments are well-designed. The results show that agents trained with this approach indeed help humans perform better, both in the task that the agent was trained on (i.e., Lunar Lander and Drone Reacher), and in a new task (Lunar Reacher). The latter shows that the agent is indeed task-agnostic, since the goals in Lunar Reacher are quite different than those in Lunar Lander.

There are a few details missing that are necessary for reproducibility; these could be included in an Appendix, if there is not enough space in the main paper. Specifically, what was the weighting on the different task-agnostic reward components? Is the learning sensitive to how these weights are chosen? And how were the shaping terms used in Lunar Lander and Drone Reacher computed?

I also have a few concerns regarding experiment design. First, the baseline is weak. The only comparison is to an agent that does not assist the human at all. It would be beneficial to compare against agents trained with different human models (e.g., laggy or noisy). Even better, it would be great to compare against Reddy et al. (citation [41] in the paper) in the Lunar Lander environment, as a best-case scenario of how helpful a non-task-agnostic agent can be, and perhaps even take an agent trained with that approach in Lunar Lander, and see how it performs in assisting people in Lunar Reacher.

Second, why wasn't a single agent trained on all models of human behavior together (i.e., noisy, laggy, and all imitation policies)? It seems that this would result in an agent that is more robust than the agents considered in the paper, which are trained on only one of those three models of human behavior. Finally, a drawback is that the user sample is very gender-biased (all male), and possibly age biased (what was the standard deviation in age? please include this in the paper as well).

Minor comments / questions:
- The direction of the inequality sign for the constraint is greater-than in equations 6 and 7, but less-than-or-equal-to in equation 3, which is inconsistent.
- Why did users assisted by the trained agent experience more timeouts in Drone Reacher? Participants said the agent ""responded slowly to user commands,"" but why was this the case in Drone Reacher but not in Lunar Lander or Lunar Reacher?
- Figure 4 plots the success vs. crash vs. timeout proportions; I would also like to see a plot of rewards, and an analysis of whether those differences are statistically significant.

[1] Tessler et al. Reward constrained policy optimization. ICLR 2019.
[2] Bohez et al. Value constrained model-free continuous control. https://arxiv.org/abs/1902.04623",Agreement accepted,"The paper is interesting and reads well, level of English is high, literature review is substantial.
The results are thorough, I like the paper and the results. In the end, though, it is more or less (as I said in the description of the work) a neural network that maps between the outputs of a controller and the required correction to get the correct controller.
The results are not surprising and actually match well with this paper: 
Ganesh, G., Takagi, A., Osu, R. et al. Two is better than one: Physical interactions improve motor performance in humans. Sci Rep 4, 3824 (2015).
Which says, that if you have something that helps you in the correct direction, you will get better and the other one will not get worse. Basically we see the same results. The paper goes through great lengths to provide the theoretical backgrounds and all methods, which is nice (but a bit much). Also, there is a lot of reading on the set-up of the experiments and specifically the surrogate pilots. It is understandable that you need to set them up, but the surrogates are not the main contribution of the paper, more of a tool. Yet, they take a huge chunk of the paper and make it less focused.
I wonder, though, why you did not compare the “behavior of human pilots” in connection with human co-pilots (basically 2 pilots at once), I wonder what the results would be there – perhaps in the direction of results reported in the above cited paper. It would be interesting to see, perhaps in another paper?
A simpler description (such as the one I provided) is somewhat missing from this paper, also a more re-implementation friendly description of the methodology.
TD3 is not explained
One or 2 typos: th instead of the (I did not write them down as I went along, so I forgot... really not many)
",,,,Agreement accepted,07/16 15:00,07/16 17:00,https://github.com/cbschaff/rsa,https://ttic.uchicago.edu/~cbschaff/rsa/,,,72.0,HJ-NoHyoHBo
"State-of-the-art dense mapping approaches cannot be deployed on Size, Weight, and Power (SWaP) constrained platforms because of their large memory and compute requirements. In this paper, we present an accurate, and efficient approach to dense multi-fidelity 3D mapping using Gaussian distributions asvolumetric primitives. The proposed mapping approach supports both high fidelity dense surface reconstruction and lower fidelity volumetric environment representation for fundamental robotics applications. We exploit the inherent working characteristics of an off-the-shelf depth sensor and approximate the distribution of approximately planar points using Gaussian distributions. Explicit modeling of the sensor noise characteristics enable us to incrementally update the map representation in real-time with high accuracy. We present the advantages of our proposed map representation over other well known state-of-the-art representations by highlighting its superior performance in terms of reconstruction accuracy, completeness and map compression properties via quantitative and qualitative metrics.",rss_author_agreement_signed.pdf,"[Aditya Dhawale ](https://adityadhawale.github.io/),  [ Nathan Michael ](https://www.rislab.org/nathan-michael)",Aditya Dhawale (Carnegie Mellon University)*; Nathan Michael (Carnegie Mellon University),rss_author_agreement_signed.pdf (270426 bytes); camera_ready_final.pdf (1713708 bytes),camera_ready_final.pdf,1236.0,73.0,,3.0,Efficient Parametric Multi-Fidelity Surface Mapping,https://adityadhawale.github.io/research/gfusion,0.5511552573330091,"# Efficient Parametric Multi-Fidelity Surface Mapping

The paper presents a mapping approach for RGBD sensors which achieves good quality reconstruction while being computationally efficient. The map is represented as a Gaussian mixture model (GMM) which is updated incrementally based on new depth images within a hierarchical scheme. This allows the approach to avoid heavy computations and provides frame-rate performance on a laptop CPU. The approach performs mapping assuming the poses of the sensor to be known. 

The interesting aspect of this work is that it provides a mapping method with a couple of user-defined parameters such that it can be adapted for different applications based on the computational resources and the memory available. For example, it would allow for planning/navigation purposes at lower quality maps or more precise reconstruction of the scene at the cost slightly higher computations. The authors also demonstrate this capability for room-sized maps using both simulated and real-world data. 

Here are some comments about the overall work and the different sections of the paper:

- The main contribution of the paper is that it combines ideas from different works such using the gaussian distributions for map representation as in NDT maps [1,17], projective association similar to KinectFusion/ElasticFusion [9,25] and a hierarchical scheme [21] to reduce computations. 
These ideas have been put together with the goal of achieving a comparable/better reconstruction quality for a smaller memory footprint as compared to other state-of-the-art mapping approaches.

- In addition to building upon these works, the new idea in this paper is to exploit the neighborhood information in the depth image to fit the Gaussian mixture model and avoid computationally expensive optimization procedures used in the previous works. 

- The paper seems to provide sufficient theoretical and implementation details to reproduce the work. In my opinion, the impact of the work can be enhanced by providing an implementation of the system so that it can be used both to compare against other mapping approaches, as well as build other applications on top. 

- Overall, the paper is well structured and clearly written. I appreciate the introduction section where the present work has been presented in the context of previous works and how they are related.

- The assumptions in terms of input data, modeling and performance are clearly spelled out in most sections of the paper. I would recommend adding a short table with the values of all the fixed parameters such as alpha_n, alpha_e, alpha_conf, Sigma_unc (min and max) and other parameters used in the approach. 

- The derivations, as well as the notation in the paper, seems consistent. Please add the relevant reference for equation 2 as this may not be trivial. Also add the reference for equation 3 (or the link to (eq 1?)) to explain how it can be derived.   

- The experiments back up the claims made in the paper in terms of accuracy and memory footprint as performance as compared to other state-of-the-art approaches. The accuracy measures look impressive particularly given the size of the maps. The approach shows better accuracy and a lower memory footprint as compared to Occupancy based Maps as well as NDT maps. It would be interesting to compare the accuracy against TSDF based methods (such as KinectFusion/ElasticFusion) as the GMM maps at least look visually messy than typical TSDF maps. This would also fill in a gap in terms of the types of maps used for comparison. You may make this additional experiment based on your space constraints. 

- In addition to the run-time analysis performed in Sec.V E, it would help to report the timing of different methods (yours, NDT, Occupancy base mapping) for the datasets at comparable resolution.

- Although it is not the main contribution, I appreciate the experiment showing the reconstructed allow for frame-to-model estimation.

Here are some minor comments/corrections:
- Sec II.B (end of page 3), it should be bxb pixel pathces instead of pxp.
- Fig 2: The correspondence between the explanation in the caption and the figure above is a bit confusing. It may help to explicitly label each part with a number.
- Sec II.D Line 13: \theta^{K} --> K should be k?
- Sec III.E Please provide a reference to the Bhattacharya measure for similarity.
- Fig 8 is too small to see the numbers.
- Sec. IV A: (Lines 3-5) The sentence construction is confusing. It would help to rephrase it.
- Table 2, 3: Please provide the thresholds used for computing precision and recall values for map accuracy.
- Sec IV.F: The accuracy for D1 is reported ad 0.0004m whereas in Fig 8. it is reported as 0.004 m. Please make them consistent.
- Just a comment on aesthetics, the text looks super cramped on page 1 and 2. Maybe go easy on the vspaces ;).
",Agreement accepted,"- This is just a minor point: I was wondering why related work is not presented in an own Related Work section.

- ""A novel map representation has been proposed by Saarinen et al. [17], Srivastava and Michael [21], Eckart et al. [5]"" => None of the mentioned approaches is compared to the surface mapping approach proposed by the authors. Why is that so? Further, the proposed surface mapping algorithm seems to be compared to the 3D-NDT algorithm introduced by [Magnusson et al., 2007]. It is unclear why the authors refer to the paper of Saarinen et al. [17] instead that introduces NDT-OM, a hybrid representation that combines elements from occupancy and NDT mapping.
[Magnusson et al., 2007]
Scan Registration for Autonomous Mining Vehicles Using 3D-NDT
Martin Magnusson, Tom Duckett and Achim J. Lilienthal
Journal of Field Robotics, 24:10, 24 Oct 2007, pp. 803-827.

- ""We exploit the understanding that the ordered data obtained from such sensors
resides along the surfaces in the scene within the ambient space R3."" => I'm not fully sure that I understand this sentence (and similar ones that follow later on in the paper). I used the working hypothesis that here the authors motivate that the Gaussian primitives are only representing planar surface patches.

- In the description of the proposed method it remained unclear why the weights w_k are introduced and how they are precisely computed. The text just says that the ""weight [...] is obtained by linearly scaling the determinant [...] between the minimum and the maximum observable uncertainty"" but does not say how the level of scaling is determined.

- The proposed approach requires to set a number of parameters (an example is the threshold below which distributions are discarded as not having sufficient evidence). It is an important weakness of the paper, that authors do not list all such parameters and that they do not explain how they set them or how they should be set given a specific environment.

- The authors should mention exactly which implementation of the baseline algorithms (""Octomap [8] and NDT Maps [1]"") they used and how they set the respective parameters. For NDT it is known, that it can facilitate highly accurate localization even with very large cells (up to a cell size of 1 m) and that it can be even a problem if the cells are selected too small (see, e.g., [Saarinen et al, 2014]). Thus, choosing a cell size of 0.05 m may give an unfair advantage to the proposed method.
[Saarinen et al, 2014]
Normal Distributions Transform Monte-Carlo Localization (NDT-MCL)	
Jari Saarinen, Henrik Andreasson, Todor Stoyanov and Achim J. Lilienthal	Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Tokyo, Japan, November 3 - 8, 2013, pp. 382-389.


- Please also note that [1] refers to a 2D implementation.

- A key issue I have with the paper is that it is not very clear how the evaluation was carried out. As mentioned above the baseline algorithms and the parameter selection of the proposed approach remain unclear. In addition, the used datasets are not sufficiently characterized and the presented performance measures are not well described. An example in Table 1: How was the ""error"" computed? How precision and recall? It is also not always explained in the figure and table captions what data set was used in order to get the respective results.",,"The work seems to be an original contribution. The paper is technically sound (except for some minor issues).
Perhaps the paper could be more clear; some parts of the paper, like Section I, are a bit cryptic.
The paper compares the proposed approach with other methods in the state of the art and outperforms them in the provided scenarios and metrics. However, the evaluation suffers a bit from being limited to four datasets and lacking some plots comparing different metrics. An evaluation with more samples and variety of scenarios would be desirable.

Some more specific comments:
- In the listed contributions at the end of Section I. There seem to be some redundancy. It would be nice if the paper could condense them and/or point to which section corresponds to each listed contribution (likely, they are II.C.1, II.C.2 and II.E), to make them clear.
- In general, it is not clear what the text means by ""fidelity"" since it is never explicitly stated. And it is not accuracy, since often these two terms appear but not as synonyms. It appears that the fidelity is a synonym for ""model complexity"" and equivalent to the ""level of detail"" or ""resolution"" in computer graphics; however, this definition or clarification should be given by the authors.
- The introduction could be split... It is a rather long introduction that goes over many different types of representations used for 3D scene modeling. I think this part could be carved out in its own section on ""Related work"" or ""Representations of 3D scenes"".

Understanding the methodology:
- In general, the approach of using probabilistic filters to represent and refine depth in 3D reconstruction is similar to the idea of Bayesian ""depth filters"" in the ICRA 2014 papers by Pizzoli et al., SVO (Fast Semi-Direct Monocular Visual Odometry), and REMODE (Probabilistic, Monocular Dense Reconstruction in Real Time). However, the paper does not mention these works, and therefore does not compare to REMODE. I understand that the resulting systems are quite different, but the ideas for depth fusion may not be so much and worth a short discussion.
- Fig. 2 should be more explicit about what is theta_0, theta_t, in the four different plots. Why is there a sudden jump of color in the floor of the room in Fig. 2, from the left-most image to the one immediately on its right? It would also be better to set the same size for the chair, to better see how the 3D reconstruction groups in extension.
- What do the three colors used for the blocks in Fig. 3 represent? Knowing this would help better understand the system and its input/output.
- Because many figures are not referenced in the text and their captions contain explanations, some things are not clear just following the main text. For example, it is not clear that the poses are given.
- The image in the eye-catcher is never referenced; the right plot with the sample plots is not repeated in the experiments. So it makes one wonder how necessary it is.
- In Eq. (3) how is the condition of Sigma being positive semi-definite enforced, so that it is a valid covariance matrix?

Evaluation / Experiments:
- Evaluation metrics, such as precision and recall should be defined, as well, for completeness.
- What is the trade-off in accuracy vs. speed? Could the authors provide a plot for it?
- What is the trade-off in accuracy vs. completeness? This is a standard plot to asses 3D reconstruction algorithms. See, for example, Vogiatzis et al. Video-based, real-time multi-view stereo, CVIU 2011 or Pizzoli et al. REMODE ICRA 2014.
- It would be nice to have a trade-off plot that illustrates the multi-fidelity aspect of the proposed method. One axis is fidelity; the other axis could be amount of texture detail, accuracy, memory, etc.
- How many distributions are used in Fig. 10? I assume that the execution time is highly influenced by this parameter. What are the units of the vertical axis of this plot (Fig. 10)?
- How much is ""sensor rates"" at the end of Section IV.E? 50 Hz? 100Hz?
- Please include units as much as possible: alpha_e sometimes has units, sometimes not.  Same for alpha_len.  ""b, the patch size is set to 8."" [pixels?]

Other comments:
- Adopting units of cm to measure absolute error, rather than meters, seems more appropriate. It would also be useful to provide a relative measure, such as the error normalized by the scene size or depth with respect to the camera.
- Figures do not seem to be in the order that they are mentioned in the text.
- The orange shading and the small size of most plots do not help much visualize the reconstruction or the error (at least on printed paper). Insets with a zoom, as in Fig. 9 are somehow helpful. Other papers use color-coded error maps (i.e., plotting the differences with respect to the most accurate model).
- Section II: The first time that it is used: ""Given an image I_0 of size..."" should be ""Given a *depth* image I_0 ...""  otherwise the type of information contained in I_0 is confusing.
- Section II: (the back-projection) can be viewed as a linear transformation --> I would rather say that it can be *approximated* by such a transformation.
- Section II: ""at time t = t ..."" looks like a tautology.
- Section II: redundunt --> redundant
- Is there a reference for the use of the acronym SWaP? I think it is not standard in computer vision; it may be more known in aerospace and military contexts.
- Some sentences are very long (multiple verbs) and therefore difficult to follow. Example: last sentence of Section IV.A.
- Some references do not have publication venue, e.g., [22]. Also in [22] gmm -> GMM. Check for other lowercase acronyms: slam, rgb-d.

Possible typos:
- The indices (subscripts, superscripts) in the variables of Sections II.B - II.C are not always consistent, which makes it a bit difficult to understand the details of the update rules of the GMM parameters. I suggest to review such indices.
- I think there is a missing term (x-mu)^T in the update of Sigma in Eq. (1); otherwise Sigma is just a vector, not a covariance matrix.
- Section II.C.1: I could not find an intuitive interpretation for the 99.97% confidence. Was it intended to be instead the usual 3-sigma rule: 99.7%? Note that the confidence rule of 99.7% probability corresponding to +-3 standard deviations is for a 1D Gaussian distribution. For higher dimensional distributions, such as the 2D projection (ellipse), the confidence rule changes (see ""On the Mahalanobis distance classification criterion for multidimensional normal distributions"", IEEE Trans. Signal Processing, 2013), and so 99.7% confidence corresponds to a Mahalanobis distance of 3.44 rather than 3. This number grows with the dimension.
",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,,,,,73.0,UDmg9VT6lXo
"In this work, we present a spiking neural network (SNN) based PID controller on a neuromorphic chip. On-chip SNNs are currently being explored in low-power AI applications. Due to potentially ultra-low power consumption, low latency, and high processing speed, on-chip SNNs are a promising tool for control of power-constrained platforms, such as Unmanned Aerial Vehicles (UAV). To obtain highly efficient and fast end-to-end neuromorphic controllers, the SNN-based AI architectures must be seamlessly integrated with motor control. Towards this goal, we present here the first implementation of a fully neuromorphic PID controller. We interfaced Intel's neuromorphic research chip Loihi to a UAV, constrained to a single degree of freedom. We developed an SNN control architecture using populations of spiking neurons, in which each spike carries information about the measured, control, or error value, defined by the identity of the spiking neuron. Using this sparse code, we realize a precise PID controller. The P, I, and D gains of the controller are implemented as synaptic weights that can adapt according to an on-chip plasticity rule. In future work, these plastic synapses can be used to tune and adapt the controller autonomously. ",RSS2020_Author_Agreement.pdf,"[Rasmus K. Stagsted](https://portal.findresearcher.sdu.dk/en/persons/rk), [Antonio Vitale](https://www.linkedin.com/in/antonio-vitale-808338157/?originalSubdomain=ch), [Jonas Binz](http://), [Alpha Renner](https://services.ini.uzh.ch/admin/modules/uzh/person.php?id=44962&back=../uzh/people), [Leon Bonde Larsen](https://portal.findresearcher.sdu.dk/en/persons/lelar), [Yulia Sandamirskaya](http://www.sandamirskaya.eu)","Rasmus Stagsted (University of Southern Denmark); Antonio Vitale (ETH Zurich); Jonas Binz (ETH Zurich); Alpha Renner (Institute of Neuroinformatics, University of Zurich and ETH Zurich); Leon Bonde Larsen (University of Southern Denmark); Yulia Sandamirskaya (Institute of Neuroinformatics, University of Zurich and ETH Zurich, Switzerland)*",RSS2020_Author_Agreement.pdf (79188 bytes); SNN_Drone_Controller (5).pdf (5603735 bytes),SNN_Drone_Controller (5).pdf,1178.0,74.0,,3.0,Towards neuromorphic control: A spiking neural network based PID controller for UAV,,0.668895709776688,"The research is clearly presented and original due to the novelty of the chip involved and utilzing neural networks to achieve the most widely-used control loop in real world systems. The presented results are transparently benched marked against a traditional system. 

The biggest improvement I can see is to go about more methodically tuning each PID system (CPU & Loihi) to ensure a fair and thorough comparison between the two. Though performance is not rigorously compared, the contribution of the paper is in the feasibility and method to achieve PID control. The scope of the paper is appropriate and detailed enough to be readily understood and replicated. ",,"The authors present a SNN architecture for direct control of two robotic systems, one of which requires nested PID loops, and evaluate control performance relative to a cpu representation as a step response.

The paper is well written and explained. From my perspective I think the main thing that can be improved is the interrogation of the control performance, and a broader set of tests and results, the presented data are quite limited. It would be helpful to see the performance of the controller on the same system with different control contexts, e.g. replacing imu sensing with external markers for higher sensor fidelity, increasing decreasing timestep sizes, or changing the size of the neuron arrays, so that the effects of the implementation on performance can be more clearly seen/investigated.

Some other comments:

'Mapping between motor velocity and torque' - this should be detailed.

The PID gains of both systems were tuned by hand - I think a little more detail is needed to have a stronger basis for comparison, and to know to what extent the chosen gains favour one controller's performance over another. To me the CPU PID seems oddly poor performing.

What sensitivity is there to the quality of the sensor data? How does the system function with ground truth input, or with simulated input?

Pushbot - 'directly translated into motor commands' A little more detail would be better.

It would be helpful to have overshoot stated as a % of the step.

I would also show a recommend a showing a single step response which allows a longer settling time, so we can see what a little better what happens with constant input? The exemplary graphs change their input a little to quickly to see the full response.

The paper reads well, but there are a couple typos:
Raspberri - typo
programm - typo

I enjoyed the reading the paper, and the topic is a fascinating frontier in robotics. I look forward to seeing more of the author's work.",Agreement accepted,"This paper presents an attempt to implement a  PID controller fully in a Spiking Neural Network (SNN) on Intel’s neuromorphic research chip Loihi. The SNN was tested on two robotic platforms; on a mobile robot platform and a 1-DOF UAV platform. Idea of implementing SNN on neuromorphic chip is certainly interesting and it is an important research question to attempt. Design and realization of the SNN on the nueromohlic chip has been presented well in detail and certainly has some merits. However, publication of the present experiments and results seems premature. 

Major comments:

1. I understand the importance of on-chip SNN approaches for applications such as UAVs due to potentially ultra low power consumption, low latency, and high processing speed. However, conventional PID based approaches are also effective, easy to implement and widely use in such applications. What are the specific merits of this on-chip SNN approach against the conventional PID on a chip? I think such comparison has not been clearly presented in the manuscript. 

2. Based on the comparison results as summarized in Table III and the results from Fig. 7 and 8, it is obvious that the conventional PID approach is giving better results (eg. based on overshoot, oscillations, steady-state-error) than the proposed method. 

3. In my opinion, the implementation of the proposed method on a ""Pushbot"" seems to be not matching relevant to title of the paper. The authors could have found a way to test a single PID on the UAV platform rather than on a Pushbot. 

4. In the case of Pushbot, the authors claimed that ""both controllers work well despite the delay in the system"". I do not think this claim is justifiable based on the results presented in Table I and II.

5. I do not clearly understand how did the authors train the SNNs during experiments. No information has been given in the manuscript on training and cross-validation of SNNs.   

6. The authors have mentioned about two nested PID controllers (inner and outer PIDs) related to the 1-DOF UAV platform. The inner PID computes the torque. However, no results have been presented to show the behavior of the torque output or how it affects the overall system.    

7. I wonder what information we can acquire as readers from the Fig.1. Even though, a 3D model of the set-up has been presented (top panel), it has not been referred anywhere in the manuscript. Just presenting a photo of the system/experiment set-up without details (eg. naming the important components, connections etc) is not adequate. 

8. No evidence for generalization is offered. Multiple trials/experiments could have done with different motion profiles (i.e: instead of only using step inputs as in Fig. 7 and 8) to generalize the results and conclusions. 

Comments on the video submission:

Though the video submission is clear, it is not that informative. Information such as PID gains, other set-up variables, assumptions in this perticular trial should be presented. ",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,,https://www.youtube.com/watch?v=lnGQqz7MM8w,,,74.0,sdnEosW5NGQ
"The distributional perspective on reinforcement learning (RL) has given rise to a series of successful Q-learning algorithms, resulting in state-of-the-art performance in arcade game environments. However, it has not yet been analyzed how these findings from a discrete setting translate to complex practical applications characterized by noisy, high dimensional and continuous state-action spaces. In this work, we propose Quantile QT-Opt (Q2-Opt), a distributional variant of the recently introduced distributed Q-learning algorithm for continuous domains, and examine its behaviour in a series of simulated and real vision-based robotic grasping tasks. The absence of an actor in Q2-Opt allows us to directly draw a parallel to the previous discrete experiments in the literature without the additional complexities induced by an actor-critic architecture. We demonstrate that Q2-Opt achieves a superior vision-based object grasping success rate, while also being more sample efficient. The distributional formulation also allows us to experiment with various risk distortion metrics that give us an indication of how robots can concretely manage risk in practice using a Deep RL control policy. As an additional contribution, we perform batch RL experiments in our virtual environment and compare them with the latest findings from discrete settings. Surprisingly, we find that the previous batch RL findings from the literature obtained on arcade game environments do not generalise to our setup.",RSS2020_Author_Agreement_Signed.pdf,"[Cristian Bodnar](https://crisbodnar.github.io/), [Adrian Li](https://scholar.google.com/citations?user=ncJWfs0AAAAJ&hl=en), [Karol Hausman](https://karolhausman.github.io/), [Peter Pastor](https://scholar.google.com/citations?user=_ws9LLgAAAAJ&hl=en), [Mrinal Kalakrishnan](https://scholar.google.com/citations?user=DMTuJzAAAAAJ&hl=en)",Cristian Bodnar (University of Cambridge)*; Adrian  Li (X); Karol Hausman (Google Brain); Peter Pastor (X); Mrinal Kalakrishnan (X),RSS2020_Author_Agreement_Signed.pdf (82048 bytes); Q2_Opt_RSS.pdf (6535099 bytes),Q2_Opt_RSS.pdf,125.0,75.0,,3.0,Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping,https://q2-opt.github.io/,0.979584244577814,"Originality

The authors build upon QT-Opt, which performs Q-learning in continuous action spaces by using the Cross Entropy Method (CEM) for selecting maximum value actions, and recent advances in distributional reinforcement learning by modeling the distribution of Q-values with quantiles. The originality of the algorithm itself is minimal - it is basically a previous method, QT-Opt, combined with prior distributional RL methods (such as Implicit Quantile Networks). Many parts of the method have already been used in prior work - for example, risk distortion metrics have already been used in the Implicit Quantile Networks paper. However, the study conducted by the authors on the efficacy of using distributional RL in a robotic grasping setting is novel and useful. The study itself is also quite thorough - several risk metrics are compared in both simulation and the real world.

Quality

As mentioned before, while there is little to no novelty in the method, there is merit in the evaluation of distributional RL and risk metrics on simulated and real robotic grasping. The experiments in the paper are well-motivated and the results are interesting and useful.

Clarity

The paper is clear and well-written. The authors cover the relevant background work and explicitly state the modifications they make to form their algorithm.

Significance

The results that the authors present are interesting. In simulation, the authors demonstrate that while their method does not lead to significant asymptotic improvement (around 2%), their method is more sample efficient (Figures 3 and 4). Table 3 is also a useful comparison of the effect of different risk metrics and how it impacts final performance. 

The authors also evaluate their algorithm in a real world grasping setup. Table 4 demonstrates significant improvement over the QT-Opt baseline. Figure 6 is greatly appreciated - showing how the number of broken gripper fingers roughly corresponds to risk-averse, risk-neutral, and risk-seeking policies is interesting. The qualitative behaviors from risk-averse policies in the supplementary video is also useful to visualize, as are the live plots of the q-value distributions. 

Finally, the results in the batch reinforcement learning setting are interesting in light of recent work in this setting. They suggest that continuous control domains and Atari are not equivalent in terms of learning from batch datasets and that diversity in batch dataset is critical to achieve good performance. ",Agreement accepted,"Originality:
The paper presents an original algorithm that extends [12] to distributional value estimation. This is a considerable step forward in knowledge and understanding.

Quality:
The paper is very well written, with few exceptions. Proper experiments and comparisons were performed, and proper analysis is provided.
However, no ablation study was performed, and this is particularly missing with respect to the quantile embedding.
Is it truly useful to pass the entire quantile vector tau jointly through the network, as opposed to each quantile tau_i separately?
There's also confusion in the equation in IV.D due to i being used to index both quantiles and basis functions.

Clarity:
The paper is very clear. Two issues:
It'd be valuable to have some details on the ""multitude of control policies"" used to generate the real-world grasping dataset. What were they?
Sorting Tables III and IV would help compare them.
",Agreement accepted,"This paper presents a study on distributional RL with application to grasping tasks 

The system is built on QT-Opt with some incremental improvements: 
1) replacing Q-learning with distributional Q-learning
2) maximizing the risk-sensitive score function instead of the reward function

The main contribution of this paper is empirical study of distributional Q-learning in the context of the grasping task.
This study empirically investigates the risk-sensitive score functions, which were previously studied by Dabney et al. [2018].

The results show that the proposed methods with the distributional Q-learning outperform QT-Opt.
The experimental results also show that the use of the risk-sensitive cost function can improve the safety during the training phase.
In addition, the empirical study presented in Section V.E. presents interesting insights on batch RL with offline training.

I summarize the strong and weak points of the paper:

Strong points:
- The experiments contain interesting insights on distributional Q-learning and batch RL
- Performing an empirical study with this scale is challenging and the it worth sharing the results with the research community.

Weak points:
- Some details of the experiments seem missing. Please refer to the following comments.

Suggestion for improvement:  
- Although this study is focused on variants of Q-learning, the motivation of this design choice is not clear to me.
What would be the difficulty when we apply actor-critic methods to grasping tasks? 
Why should we use the variant of Q-learning, which requires running CEM for selecting actions? 
It would be benefitial for readers if it is discussed in the related work section.

- For reproducibility, it would be better to provide some more information of the implementation and experiments.

-- In Q2R-Opt, N quantile midpoints of the value distribution. What is the value of N in the experiment? How many midpoints are learned?
-- In CEM, how many iterations of sampling were performed and how many samples were generated in each iteration? How much time is required to select the action with CEM?
-- When training a neural network, is any pre-training used?
-- Some more information of traning the neural network: batch size, learning rate. 
-- it is reported that 500,000 episodes are used. How many time steps does each episode contain? How many is the total time steps?
-- In page 5, ""500,000 episodes, collected over many months using a multitude of control policies."" This is not academic. How many months did it take to collect the data?
 
",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,,,,,75.0,5xkBUsm1nt0
"By harnessing a growing dataset of robot experience, we learn control policies for a diverse and increasing set of related manipulation tasks. To make this possible, we introduce reward sketching: an effective way of eliciting human preferences to learn the reward function for a new task. This reward function is then used to retrospectively annotate all historical data, collected for different tasks, with predicted rewards for the new task. The resulting massive annotated dataset can then be used to learn manipulation policies with batch reinforcement learning (RL) from visual input in a completely off-line way, i.e., without interactions with the real robot. This approach makes it possible to scale up RL in robotics, as we no longer need to run the robot for each step of learning. We show that the trained batch RL agents, when deployed in real robots, can perform a variety of challenging tasks involving multiple interactions among rigid or deformable objects. Moreover, they display a significant degree of robustness and generalization. In some cases, they even outperform human teleoperators.",RSS2020_Author_Agreement.pdf,"Serkan Cabi (DeepMind)*; Sergio Gómez Colmenarejo (DeepMind); Alexander Novikov (DeepMind); Ksenia Konyushova (DeepMind); Scott Reed (DeepMind); Rae Jeong (DeepMind); Konrad Zolna (DeepMind); Yusuf Aytar (DeepMind); David Budden (DeepMind); Mel Vecerik (Deepmind); Oleg Sushkov (DeepMind); David Barker (DeepMind); Jonathan Scholz (DeepMind); Misha Denil (DeepMind); Nando de Freitas (DeepMind); Ziyu Wang (Google Research, Brain Team)","Serkan Cabi (DeepMind)*; Sergio Gómez Colmenarejo (DeepMind); Alexander Novikov (DeepMind); Ksenia Konyushova (DeepMind); Scott Reed (DeepMind); Rae Jeong (DeepMind); Konrad Zolna (DeepMind); Yusuf Aytar (DeepMind); David Budden (DeepMind); Mel Vecerik (Deepmind); Oleg Sushkov (DeepMind); David Barker (DeepMind); Jonathan Scholz (DeepMind); Misha Denil (DeepMind); Nando de Freitas (DeepMind); Ziyu Wang (Google Research, Brain Team)",RSS2020_Author_Agreement.pdf (124631 bytes); Scaling data driven robotics.pdf (8244961 bytes),Scaling data driven robotics.pdf,126.0,76.0,,3.0,Scaling data-driven robotics with reward sketching and batch reinforcement learning,https://sites.google.com/corp/view/data-driven-robotics/,0.8678902822504929,"
The dataset collected here has no description of data quality. 
The authors claim that there is a large number of mixed quality examples, however the details are a bit vague. It would help to understand, how much of data is teleoperated, how much is random, how much is interactive (within interaction, what fraction of time interaction happens). 

The authors also claim that multiple task labels can be provided to the same daat examples. both qualitative examples and quantitative assessment of performance of policies on off-task relabelled data would useful to understand the benefits. Also policy learning with only relabeled data should be compared to on-task data and both. 

Reward sketching and ranking based reward regression is indeed an insightful idea from this paper, at least novel in this context if not broadly.
In the loss computation the reward model, currently there appear to be too many parameters -- 6! perhaps ranking loss should be pairwise ordering rather than a scalar difference?

During the iterative data collection and retraining -- it is unclear what and how often the human annotators needs to intervene. ""to fix a reward delusion...."" -- what is the setup, and a supervision or all the outlier cases seems rather inefficient and ad hoc!

Experiments:
The tasks of choice are simpler than other similar scaled robotics datasets MIME and Roboturk. How would RL compare on these tasks?

Training Detail: 
""We train multiple RL agent..."" how many? how many roll-outs. 
The experimental evaluation suggests thats Non-distributional RL is a key component along with adding off-task data to the training set. 
Details of the non-distributional rl algorithmic setup (either with explicit deatils of the models and insights, or with code) will make this paper much more impactful. 

Other similar papers in the area of RL, such SAC have claimed similar successes -- ""real robot rl in matter of hours"". A quantitative comparison or atleast a discussion on the topic would help the reader place this work in the broadere context. 


Big picture concerns: 
The execution and evaulation step is -on-task+on-robot.
how is the dataset useful if the evaluation setup (the robot setup) is not available? How does only the dataset, if at all, help the community to work on Batch RL, unless the bigger framework is released for use by independent researchers in their setups. 


Citations:
Datasets:
Multiple Interactions Made Easy (MIME) CoRL 2019, (real) -- Many tasks
Roboturk, CoRL 2018 (simulated) and IROS 2019 (real) -- Stacking, cloth, search in cluttered bin -- See table in IROS paper for other datatsets
RoboNet, CoRL 2019 (real) -- Pushing

Batch RL:
Off-policy policy gradient with state distribution correction.(Liu et al. 1904.08473) 
Empirical Study of Off policy Pollicy eval. (Voloshin,1911.06854)
IRIS (Mandlekar, 1911.05321)


Quality: Well done paper, with intruguing and interesting results. However the takeaway for the reader is uncertain, other than the fact that batch rl can be made to work! However the data itself is not useful per se, it would be the system architechture and implemetation details to reproduce such a system that would be really helpful.

Clarity:Good and well written with well made figures, video, and descriptive language

",Agreement accepted,"This work proposes a framework that enables continuous data collection through demonstrations, reward sketching and experience evaluation. It tries to tackle an important problem in robot learning: Large amounts of data are not available and very costly to get. Reward sketching is a very neat idea and it can be useful in many different applications. However, I have had a hard time understanding the contributions of the paper. This might be because the related work is given at the very end. Below are my comments to the authors:

- A list of contribution in the Introduction would be really helpful. Reward sketching is definitely one (and a very good one), but it seems the batch RL method used in this paper is from existing literature. In that sense, the paper might be overselling itself, and it could be better to focus on the advantages of using reward sketching (possibly over other alternatives).
- I appreciate the effort put in the experiments, they are really good. It is very interesting to see that data from other tasks and random trajectories significantly help learn a task.
- How is the NES system innovative? As someone who is not familiar with the research on database systems, I don't see how the used NES is different from existing databases that allow SQL queries. Moreover, the description of NES in Section II is a little distractive for the readers as it has only loose connections with the real purpose of the paper, so I would suggest completely removing Section 2B.
- Reward sketching may not always be possible, because humans are not always good at assigning reward values. The authors have already discussed this, which I appreciate. It would be also good to discuss alternative ways of how to achieve reward sketching. For example, would it be possible to do something similar to preference-based learning where the reward values are obtained through comparisons between the states at different time steps?
- Is ""annotating data from prior tasks for a new task"" (Section 2C) always possible? When the two tasks are very different, one may be completely unrelated to the other, and the humans may not be able to sketch rewards.
- In the Experiments, it was not clear to me whether the human sketches reward for all the trajectories from random_watcher. If not, how are the data that will be sketched selected?

Below are my minor comments:
- The second paragraph of Section 2 uses enumeration from A to G, whereas Fig. 5 uses 1-7 for the corresponding list. It would be better to use the same format.
- In Section 2A, should ""optionally"" be replaced with ""occasionally""? Otherwise it may mean that human has the option to always provide unsuccessful examples.
- Typo in Section 2A: interprete --> interpret
- Ref 18: It seems there is a syntax error in the list of authors. The first two authors are combined and misordered.",Agreement accepted,"This paper does not have many truly original ideas, but has very good quality, clarity, and significance. 

The system described is quite sensible and its implementation is clear. I do not have many details comments, except to say that the experiments demonstrating the effectiveness of this system are quite well done and make this a likely significant work as one of the first to demonstrate such impressive results with large scale demonstration collection and Batch RL on real robots.

The main drawback I see in this paper is a poor discussion of the human elements of the systems (teleoperation and reward sketching). It is never discussed how much prior practice or overall expertise the humans involved in the experiments had, and in general no discussion of interface considerations are present -- from a user study perspective, this is quite poor compared to prior works in Learning from Demonstrations literature. Another significant flaw is that the related work section is  a bit too heavy on recent RL and does not give enough credit to related work in Learning from Demonstrations or data collection via teleoperation; in particular it is surprising that RoboTurk (https://cvgl.stanford.edu/projects/roboturk/ccr-web/) is not cited or compared to, and more generally many other approaches to using annotations and demonstrations from humans are not cited (https://ai.stanford.edu/~cdarpino/CLEARN/index.html). ",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,https://github.com/deepmind/deepmind-research/tree/master/sketchy,https://www.youtube.com/watch?v=3FfDRXrlWVs,,,76.0,HCcoi9EofoA
"This work introduces the so-called Modular Passive Tracking Controller (MPTC), a generic passivity-based controller, which aims at independently fulfilling several subtask objectives. These are combined in a stack of tasks (SoT) that serves as a basis for the synthesis of an overall system controller. The corresponding analysis and controller design are based on Lyapunov theory. An important contribution of this work is the design of a specific optimization weighting matrix that ensures passivity of an overdetermined and thus conflicting task setup. The proposed framework is validated through simulations and experiments for both fixed-base and free-floating robots. ",letter_of_agreement_Englsberger.pdf,"Johannes Englsberger (German Aerospace Center (DLR))*; Alexander Dietrich (DLR); George Mesesan (German Aerospace Center (DLR)); Gianluca Garofalo (German Aerospace Center (DLR)); Christian Ott (DLR); Alin  Albu-Schaeffer (Robotics and Mechatronics Center (RMC), German Aerospace Center (DLR))","Johannes Englsberger (German Aerospace Center (DLR))*; Alexander Dietrich (DLR); George Mesesan (German Aerospace Center (DLR)); Gianluca Garofalo (German Aerospace Center (DLR)); Christian Ott (DLR); Alin  Albu-Schaeffer (Robotics and Mechatronics Center (RMC), German Aerospace Center (DLR))",letter_of_agreement_Englsberger.pdf (268675 bytes); RSS_2020_JEnglsberger_FinalSubmission_corrected.pdf (1365181 bytes),RSS_2020_JEnglsberger_FinalSubmission_corrected.pdf,1308.0,77.0,,3.0,MPTC – Modular Passive Tracking Controller for stack of tasks based control frameworks,,0.331326610019265,"In the first section the authors do a quick overview of Inverse Dynamics scheme based on a Stack of Tasks paradigm. The citations are good and cover the essential work in the field.

The second section derive some results to the control of robot using its dynamics. The authors first give an analytical solution to find joint acceleration for a given set of torques and forces applied to the system. In a second time, an acceleration error in the task space is provided as well as the weighted pseudoinverse using the mass matrix. The third part studies the behavior of the Lyapunov function related to a desired generalized task.  The part  of the Lyapunov function which may lead to a none dissipative behavior is provided as well as the task dynamics.
The particularity of this task is that it does not cancel the Coriolis term.

The fourth section consider a set of controllers following the form presented previously. A Lyapunov function considering the overall behavior of the system is therefore proposed. It is shown that the system is converging with the proposed controller and when the tasks are compatible. This is shown by considering an appropriate cost function which is minimized by the controllers. If they are not compatible a modification of the cost function is proposed to maintain convergence of the algorithm. This is specified by Eq.41. This is the most important result of the paper. It is shown later that this cannot be extended for trajectory tracking.

The fourth section shows how the structure of the control is allowing to start from a full inverse dynamics formulation to go up to a full PD+ controller. The interpretation of this architecture is not completely clear. But it is easier to tune each controller individually.

The simulations are demonstrating the capabilities of the controller. And it is interesting to see that the overall stability is guarantee even so if individually a controller can be diverging. What matters is the global behavior of the Lyapunov function which is converging.

The only weakness of this paper (apart some shortcuts in some derivations) is the lack of experiments on a real robot.
But this is a very nice paper. ",,"Overall, this reviewer enjoys reading the paper. It is well-written and also strives to cover a number of whole-body control problems including over-determined problem and under-actuation. Nevertheless, there are suggestions to improve the paper for publication. 
1.	The main work of this paper is to allow soft-hierarchy in multiple tasks while satisfying passivity condition, where the PWOW (41) might be the key idea. However, this motivation should be strengthened. Mainly, the question would be “why is the soft-hierarchy important? (against strict hierarchical task approach)
2.	If this reviewer correctly understands, the proposed controller, in principle, gives semi-decoupling solution between fully-decoupled and -coupled one. In this sense, the name “DPTC” is rather confusing since it gives impression biased to “fully” decoupled control. 
3.	This reviewer cannot be sure that the proposed control can guarantee dynamic consistency for the under-actuation case. Please show further explanation or verification for this. 
4.	Again, for the under-actuation case, the optimized control solution with the PWOW cannot guarantee passivity. Then, one can still argue that the proposed method is a “passivity”-based controller for the under-actuation case?
5.	In light of comments 3 and 4, this reviewer thinks that such generalization needs further validation. Besides, it is difficult to convince it through Simulation V.B (floating-based humanoid) since more description and discussion of the result should have been provided to clearly understand. For these reasons, this reviewer would like to suggest the authors focus on the full actuation case. 

Some minor comments are as follows:
1.	Both matrices S and U are named actuation mapping. 
2.	What was the weighting values (\psi) used in simulation V.A?
3.	Reference [6] needs to be updated. 
4.	There are some grammatical errors, e.g., “two of the works, that inspired this work, …”. Please carefully find and correct them. 
",,"The paper describes a control approach for fixed and floating base articulated robots
building on a hierarchical control structure composed of multiple tasks.
It reuses concepts from passivity based task space control formulations together with a
deliberately chosen weighing matrix to maintain passivity of the complete hierarchical
control framework. The paper addresses different use cases of fully actuated fixed based
robots, underactuated robots and robots with constraint actuation. 

II.C.:
""For such cases, further analysis may become necessary.""
It is not entirely clear what this sentence should tell the reader.
Is the stability in the regulation case (unexpected perturbations) completely out
of the scope of this paper? Then this should be clearly stated.

III.B.:
Eqn 26: Why are there weights for the different V_k or how are these weights defined?
        They seem to be the same as the task weights, is there a reason for this?

III.D.:
Eqn 39: If the conflict resolution is time variant, does this still work? 

One stylistic remark: use less emph or italic text, this does not increase readability.
",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,,https://youtu.be/WdF9UQK8aIo,,,77.0,xgCvjZUYRCU
"We propose NH-TTC, a general method for fast, anticipatory collision avoidance for autonomous robots with arbitrary equations of motions. Our approach exploits implicit differentiation and subgradient descent to locally  optimize  the  non-convex and non-smooth cost  functions  that  arise  from  planning  over  the anticipated  future positions of nearby obstacles. The result is a flexible framework capable of supporting high-quality, collision-free navigation with a wide variety of robot motion models in various challenging scenarios. We show results for different navigating tasks, with various numbers of agents (with and without reciprocity), on both physical differential drive robots, and simulated robots with different motion models and kinematic and dynamic constraints, including acceleration-controlled agents, differential-drive agents, and smooth car-like agents. The resulting paths are high quality and collision-free, while needing only a few milliseconds of computation as part of an integrated sense-plan-act navigation loop. For a video of further results and reference code, please see the corresponding webpage: http://motion.cs.umn.edu/r/NH-TTC/",RSS2020_Author_Agreement_signed.pdf,Bobby Davis (University of Minnesota Twin Cities)*; Ioannis Karamouzas (Clemson University); Stephen Guy (University of Minnesota Twin Cities),Bobby Davis (University of Minnesota Twin Cities)*; Ioannis Karamouzas (Clemson University); Stephen Guy (University of Minnesota Twin Cities),NH_TTC_V3.pdf (844067 bytes); RSS2020_Author_Agreement_signed.pdf (229948 bytes),NH_TTC_V3.pdf,104.0,78.0,,3.0,NH-TTC: A gradient-based framework for generalized anticipatory collision avoidance,http://motion.cs.umn.edu/r/NH-TTC/,0.960240965457004,"Summary
This paper presents a motion planner for collision avoidance in multi-agent navigation tasks. The planner is based on a trajectory optimization framework employing goal attraction and collision avoidance costs and state constraints. Simulated and real-world experiments demonstrate the robustness of the framework in handling collision avoidance with moving obstacles.

Strengths
-The paper is mostly well-written and easy to follow. There is a good amount of related work, and the problem is of interest to the RSS community with good future impact on robotics applications.
-The use of the subgradient descent methodology appears to be well-motivated as a workaround to conventional gradient descent which could not work here due to the discontinuities observed in the cost functions employed.
-The included results and videos show that the framework is capable of generating smooth collision-avoidance maneuvers.
-I find interesting that by closing the loop super quickly as shown, the resulted trajectories appear to be smooth despite the consideration of only straight trajectories of fixed velocity. It would be interesting to quantify how smooth the trajectories actually are.
-As highlighted above, the paper includes a good amount of simulations and real-world experiments.

Weaknesses
-It was not clear what motion model the obstacles are running. My understanding is that they move straight with fixed speed and they are not reacting to others. But this should be made clear to avoid confusion.
-While the approach and the results presented make an interesting and novel contribution, I find that the paper is missing a compelling motivation. What is the scope of the framework and what is the problem that this approach is trying to solve? 
-The problem with the current lack of motivation is that it is unclear whether some of the assumptions made are valid. For instance, the core of the evaluation assumes that the moving obstacles are non-reactive and always moving straight. While this creates a challenging experimental regime, it is not straightforward to imagine what could be a real-world instance for it. Further, the behaviors generated by the framework include backtracking –in many real-world applications, this would not be desired. Finally, depending on the application, different evaluation metrics would be necessary. The computation time and the ration of collision-free execution are well-motivated but there are other qualities such as trajectory smoothness, social criteria and more, that could be applicable depending on the task at hand.
-While including multiple moving obstacles makes for a challenging scenario, the straight-moving assumption relaxes the prediction problem. To me, it would appear more interesting and meaningful to see an in-depth evaluation of how the framework performs when obstacles are reactive, following a policy that is not exactly known. A good start would be to investigate a heterogeneous case, similar to the example included. This would definitely have some more straightforward transfer to a wide variety of real-world problems and convince about the value of the proposed framework for robotics applications.
-Within the included related work, there are a few approaches considering multi-agent collision avoidance for differentially constrained robots. The claimed improvement of this paper is that it avoids the approximation of robot motion models. It would be good to expand further and show that this approximation indeed results in poorer performance. Comparing with some baselines from the literature (e.g., Mora et al. TRO ’18) or at least explain why such a comparison is not applicable would be important. Overall, since the main contribution of the paper appears to be handling multi-agent collision avoidance in environments with nonholonomic agents, it would be crucial to contrast the approach directly with approaches of similar objectives.

Minor comments
-The notation could be further compacted. I do not understand eq. 3 and eq. 4. What is the difference between x and x bar? O and o bar? Why is p and q necessary? The “trajectory” is defined twice, as x(t) and then as \cal{T} = \{x(t), \forall t\gec 0\}. The cost C is first defined generally and then it is stated that “this creates a new cost function comprised of two terms” –this is not well-defined. Also, C is first defined as C(T) and then as C(u) –while I get the point (trajectories are now parametrized wrt fixed u), mathematically this is not correct. What is C a function of?
-I am not sure if the standard deviation makes sense for percentages. A Bernoulli error bar is a more principled error for such experiments.
-I do not see how the proposed approach could scale to humanoids or quadrotors. The proposed framework and the underlying assumptions made throughout the paper would not directly scale to higher dimensions.

There is some work in collision avoidance among potentially heterogeneous agents that should be referenced, e.g.:
Godoy, J., Karamouzas, I., Guy, S. J., & Gini, M. L. (2016). Moving in a crowd: Safe and efficient navigation among heterogeneous agents. IJCAI International Joint Conference on Artificial Intelligence, 2016-January, 294-300. 
Mavrogiannis C., Knepper R. A., ""Multi-Agent Trajectory Prediction and Generation with Topological Invariants Enforced by Hamiltonian Dynamics"", Proceedings of the 2018 International Workshop on the Algorithmic Foundations of Robotics (WAFR '18), 2018.

Recommendation
This paper presents a novel and interesting framework for planning collision-free trajectories in multi-agent domains for robots with nonholonomic dynamics. It is mostly well-written, and the technical approach is well-motivated and described. It presents a good amount of experimental evaluations and the submission includes some nice videos and demonstrations. The main issue with the paper appears to be its lack of concrete motivation which raises questions about the experimental setup considered, the baselines, metrics and actual experiments employed. Despite the real-world demonstrations, I am not convinced that the problem considered is well-motivated and set up (at least from a robotics perspective).",,"Collision avoidance for mobile robots has been one traditional topic of research in real time navigation and this work is important in this context. The proposed approach plans over the full control space of the robots using exact kinematics and dynamics constraints of the motion models and this allows the approach to work for both holonomic and non-holonomic robots. 

The proposed approach works by exploiting implicit differentiation and sub gradient descent techniques to locally optimize two non-convex and non-smooth cost functions by penalizing true collisions and promoting goal-oriented controls. The penalized cost functions are (a) Collision cost: a cost function to evaluate how well the trajectory provides collision free motion, and, (b) Goal cost:  a cost function to evaluate how quickly the robot reaches its goal position while following the trajectory. In order to make the algorithm efficient, the approach only considers single control for optimization.

The author further provides several scenarios and comparisons with well-known techniques such as ORCA and TTC. The authors also present comparisons of optimization time, performance and convergence for different motion models which makes their work credible. To summarize, this paper proposes a new fast anticipatory collision avoidance framework for both holonomic and non-holonomic robot motion models. While related work in collision avoidance techniques has already been published not many have taken into account the motion model as a central criterion. The results presented in the video demonstrate good results with several examples.

A few suggestions for improvements are given below:

- On page number 2, 3rd paragraph:
“Our proposed navigation approach supports fast, real time collision avoidance across the same wide range of motion models as GRVO or CCA, but removes the need to approximate robot motion models or collision computations”
I am not sure what the authors mean with “removes the need to approximate robot motion models or collision computations”. A clarification would be useful.

- In section III, Notation and Assumptions section, first line of second column:
 “state such that x(u,t) in X for all t. This is needed to specify state...” 
I believe x(t,u) should be written instead.

- In section III, Notation and Assumptions section, second paragraph
“These disks are defined by projecting both the robot and obstacle states into a common Euclidean workspace, typically 2d or 3d,……”
When discussing 3D, it would be clearer to use word spheres instead of disks.


",,"This paper presents a collision-avoidance algorithm using gradients to locally optimize robot control for TTC-based objectives. The core of the work is to compute the gradient using numerical trajectory integration and various analytical manipulations to handle discontinuity and implicit functions. The method is claimed to be generic under the assumption that gradients of the robots' dynamics are given in analytical forms. The method has been compared with TTC and ORCA to show its improvements in collision avoidance.

The model assumes a single control to be consistently executed in the future, thus it is implicitly constraint to local collision avoidance and can only be applied to low-speed scenarios. 
Nevertheless, as a local model,  the proposed method offers a new idea for solving such problems. Compared to ORCA-based models, a clear benefit of the gradient-based method is that the solution improves over time thus can work in an any-time fashion. 

However, the reviewer has the following concerns:

* The method can be stuck in bad local optimal solutions due to bad initial solutions, especially when interacting with multiple obstacles. In contract, ORCA-like methods use the global optimal in the constrained velocity-space. Experiments should be conducted to examine such limitations.

* The evaluation is not comprehensive and baselines are weak. There exist many models explicitly addressing non-holonomic kinematics and dynamics of agents. The paper should probably compare with non-holonomic variants of ORCA, e.g., NH-ORCA [1] and GAMMA [2], and control-space variants of ORCA like AVO [3].

Overall, this work uses sophisticated mathematical manipulations to derive an efficient collision avoidance algorithm that directly optimizes controls. However, the method has not been compared with state-of-the-art models. The reviewer suggests to conduct more comprehensive experiments on multi-obstacle scenarios and compare with stronger baselines. 

References:
[1] Alonso-Mora, Javier, Andreas Breitenmoser, Martin Rufli, Paul Beardsley, and Roland Siegwart. ""Optimal reciprocal collision avoidance for multiple non-holonomic robots."" In Distributed autonomous robotic systems, pp. 203-216. Springer, Berlin, Heidelberg, 2013.
[2] Luo, Yuanfu, and Panpan Cai. ""GAMMA: A General Agent Motion Prediction Model for Autonomous Driving."" arXiv preprint arXiv:1906.01566 (2019).
[3] Van Den Berg, Jur, Jamie Snape, Stephen J. Guy, and Dinesh Manocha. ""Reciprocal collision avoidance with acceleration-velocity obstacles."" In 2011 IEEE International Conference on Robotics and Automation, pp. 3475-3482. IEEE, 2011.",Agreement accepted,,,,07/16 15:00,07/16 17:00,,,,,78.0,B6bepiJAtQQ
"We present a unified representation for actionable spatial perception: 3D Dynamic Scene Graphs. Scene graphs are directed graphs where nodes represent entities in the scene (e.g., objects, walls, rooms), and edges represent relations (e.g., inclusion, adjacency) among nodes. Dynamic scene graphs (DSGs) extend this notion to represent dynamic scenes with moving agents (e.g., humans, robots), and to include actionable information to support planning and decision-making (e.g., spatio-temporal relations, topology at different levels of abstraction). Our second contribution is to provide the first end-to-end fully automatic Spatial PerceptIon eNgine (SPIN) to build a DSG from visual-inertial data. We integrate state-of-the-art techniques for object and human detection and pose estimation, and we describe how to robustly infer object, robot, and human nodes in crowded scenes. To the best of our knowledge, this is the first paper that reconciles visual-inertial SLAM and dense human mesh tracking. Moreover, we provide algorithms to obtain hierarchical representations of indoor environments (e.g., places, structures, rooms) and their relations. Our third contribution is to demonstrate the proposed spatial perception engine in a photo-realistic Unity-based simulator, where we assess its robustness and expressiveness. Finally, we discuss the implications of our proposal on modern robotics applications. We believe 3D Dynamic Scene Graphs can have a profound impact on planning and decision-making, human-robot interaction, long-term autonomy, and scene prediction.",RSS2020_Author_Agreement (2).pdf,"[Antoni Rosinol](https://www.mit.edu/~arosinol/), [Arjun Gupta](http://), [Marcus Abate](http://), [Jingnan Shi](http://), [Luca Carlone](https://lucacarlone.mit.edu/)",Antoni Rosinol (MIT)*; Arjun Gupta (MIT); Marcus Abate (MIT); Jingnan Shi (MIT); Luca Carlone (Massachusetts Institute of Technology),main.pdf (7969214 bytes); RSS2020_Author_Agreement (2).pdf (357046 bytes),main.pdf,100.0,79.0,,3.0,"3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans",http://web.mit.edu/sparklab/datasets/uHumans/,0.7689988791717159,"In my opinion, the weakest part of the proposal is the graph of places in Layer 3. A place is simply a 3D position in free space with a bounding box, but what size of bounding box? Looking at the large number of places in figure 2 it seems that the system is assuming a very small flying robot, while a bigger land robot would need a totally different places graph for path-planning. So, the claim that this places graph allows for path planning should be moderated.  Also, looking in detail at figure 2, we can observe that the system has found many paths between rooms that traverse crystal walls! Probably, good semantic mapping of this kind of environments would require detecting and explicitly representing doors.",Agreement accepted,"Positive:
Making data structure that contains many entities and dynamic objects like humans is very important and the right direction to pursue. I imagine that this requires a large amount of efforts and time. 

Weakness:
Many details are provided in the paper and it is difficult to follow. Some details should be moved to appendix.  

As 3D scene graphs are already proposed previously and adding humans only limits its novelty slightly. The data structure is only established on a simulation environment.

I am not convinced by the human tracking part. Since it uses simulation, there is no need to use human tracking on it. Temporal correspondences are already established. I expect that the authors are assuming to use the method on real images in future but considering the current level of 3D human pose detection, it is a bit early for applying them to this kind of crowded environment and get accurate results espetially absolute depths of the root. Rather, it would be more valuable to use and provide a wide variety of simulated human agents (cloths, body styles etc,) in the scenes. Why not use real scenes as in 3D scene graphs and more realistc human agents from commecial software.

In the result section, several applications of dynamic 3D scene graphs are discussed. What is said here seems useful. However, the concrete results are not provided. I would appreciate at least one concrete results of applying existing or non-existing techniques on the presented 3D scene graphs.  

Some references from computer graphics and human interaction are lacking such as [1], [2] and the ones in [3]. 
Reference:
[1]	Sören Pirk, Vojtech Krs, Kaimo Hu, Suren Deepak Rajasekaran, Hao Kang, Yusuke Yoshiyasu, Bedrich Benes, and Leonidas J. Guibas, Understanding and Exploiting Object Interaction Landscapes, ACM Trans. Graph., 36, 2017.
[2]	Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin Yumer, and Niloy J Mitra. imapper: interaction-guided scene mapping from monocular videos. ACM Transactions on Graphics (TOG), 38(4):92, 2019
[3]	R. Hu, M. Savva, O. van Kaick, Functionality Representations and Applications for Shape Analysis, Volume 37, 2, 2018.
",,"The paper presents a novel 3D dynamic scene graph that describes the actionable spatial perception of the environment. It is of great importance to design scene representations that take planning and decision-making aspects of the robot into account. For robots to operate freely in dynamic and unstructured human environments, different modules in robots require different levels of abstraction of the environment. The proposed unified representation connects spatial and actionable concepts to afford planning queries and is compositional. State-of-the-art neural network models are employed for inferring different concepts. Post processing methods are associating these outputs to generate a 3D dynamic scene graph. The system is inherently high cohesion/ low coupling between modules. However not principled or not end-to-end,  the proposed system is still of great practical use. Experiments are performed in three simulated scenarios including different numbers of humans.  The insight of the proposed approach is novel and valuable. 

The literature survey provides a clear introduction to scene representations in robotics, scene reconstruction in different abstractions and human pose estimation. It precisely distinguishes the proposed representation and existing works.  


However the reviewer has some concerns and questions about the work and hope these can be addressed or clarified by the authors. 

1.  A number of neural networks and geometric methods are used to estimate different entities in the layered representation. What are the resources needed to generate the 3D scene graph? What is the running time for the entire estimation and for a single frame? It would be great if they can be clarified and analyzed.

2. For actionable information in the 3D dynamic scene graph, they are mainly used for planning and obstacle avoidance. It would be interesting if the affordance of each object is also included in the actionable information so that they become manipulable by robots with a manipulator. 

3. The representation of the objects now only includes furniture like chairs and sofas. It would be great if the authors can discuss and analyze how the inclusion of household objects like cups and bottles may affect the performance of the estimation process.

4. Table II shows the RMSE mesh error for methods with and without dynamic masking. When taking VIO poses into account, the performance of methods with and without dynamic masking is very similar. However, the method with dynamic masking performs much better with ground poses. It would be better if it can be explained. 

5. For tracking dense mesh models of deformable objects (humans), how long does the system maintain the mesh model and pose at each timestamp of humans? There is a trade-off between the length of history and the memory efficiency.  

6. It would be great if the system can be demonstrated in a real world scenario and would certainly have a greater impact. 

Overall, the paper is easy to follow and very well written. It is presented with clear motivation towards the unified representation for actionable spatial perception.",,"Originality: The proposed 3D DSG includes multiple semantic layers mainly targeted for urban, indoor environments. Similar hierarchical representations have been used in indoor navigation and more sophisticated layers on traversability and terrain type analyses are used for outdoor navigation. Having both static and dynamically moving objects such as humans seems to be interesting. How scalable is the proposed approach with respect to the number of static and dynamic objects?
Quality: The basic idea of combining static environmental contexts and dynamically moving objects is great. One of the difficulties in doing so is to recognize whether an obstacle is a moving/movable one or not. It seems that the proposed approach resolves this issue using prior knowledge, e.g., humans are moving objects, but what if there is conflicting information from the common object detector and person detector? This question leads to a more general question on how the proposed approach handles uncertainty. Because the evaluation on this paper was done in a simulation setting, we could assume high accuracy in visual perception, however, in robotics, a key challenge is dealing with high-level of uncertainty due to sensor noise and environmental variables. I think that the paper can be strengthened by explaining how the perception uncertainty is represented and gets updated as an environment changes dynamically. 
Clarity: The paper is written clearly and organized following a common format of the technical paper that includes a survey of related work, the description of the proposed approach, and evaluation results and discussions. 
Significance: This paper proposes a representation of semantic information and the automatic building of it from sensor data. Here are some suggestions on how this paper can be improved for more significant contributions: 1) describe how the uncertainty can be included, 2) describe how the representation can be extended/modified to accommodate different needs of specific applications, e.g., adding topological and/or metric maps that can interface with path planning algorithms, 3)  evaluation of SPIN on other datasets, e.g., Matterport datasets.
",Agreement accepted,Agreement accepted,07/16 15:00,07/16 17:00,,https://www.youtube.com/watch?v=SWbofjhyPzI,,,79.0,nDmkjt6aU2Y
"Natural language object retrieval is a highly useful yet challenging task for robots in human-centric environments. Previous work has primarily focused on commands specifying the desired object's type such as ""scissors"" and/or visual attributes such as ""red,"" thus limiting the robot to only known object classes. We develop a model to retrieve objects based on descriptions of their usage. The model takes in a language command containing a verb, for example ""Hand me something to cut,"" and RGB images of candidate objects and selects the object that best satisfies the task specified by the verb. Our model directly predicts an object's appearance from the object's use specified by a verb phrase. We do not need to explicitly specify an object's class label. Our approach allows us to predict high level concepts like an object's utility based on the language query. Based on contextual information present in the language commands, our model can generalize to unseen object classes and unknown nouns in the commands. Our model correctly selects objects out of sets of five candidates to fulfill natural language commands, and achieves an average accuracy of 62.3% on a held-out test set of unseen ImageNet object classes and 53.0% on unseen object classes and unknown nouns. Our model also achieves an average accuracy of 54.7% on unseen YCB object classes, which have a different image distribution from ImageNet objects. We demonstrate our model on a KUKA LBR iiwa robot arm, enabling the robot to retrieve objects based on natural language descriptions of their usage. We also present a new dataset of 655 verb-object pairs denoting object usage over 50 verbs and 216 object classes.",RSS2020_Author_Agreement.pdf,"Thao Nguyen, Nakul Gopalan, Roma Patel, Matthew Corsaro, Ellie Pavlick, [Stefanie Tellex](http://h2r.cs.brown.edu)",Thao Nguyen (Brown University)*; Nakul Gopalan (Georgia Tech); Roma Patel (Brown University); Matthew Corsaro (Brown University); Ellie Pavlick (Brown University); Stefanie Tellex (Brown University),RSS2020_Author_Agreement.pdf (150415 bytes); RSS_2020.pdf (1969229 bytes),RSS_2020.pdf,1333.0,80.0,,3.0,Robot Object Retrieval with Contextual Natural Language Queries,,0.7695015409504041,"Overall, I think that the idea is fairly novel (though look at Sarah Aboutalib's thesis on multi-cue recognition ""a chair is an object you sit on""). However, if there aren't large databases of the verb data, I'm not sure that I can see a huge benefit from it without a little more insight from the authors about why it is useful and important. With people, is this really something they would do - use a verb? Can you cite some papers showing that people would actually use this as a tool?

My biggest concern is that the sections about data collection and training seem very redundant at times and very hard to follow at others. I am having trouble tracking the different sizes of things - 655 object actions pairs in one section and 535 pairs in another, was 80% taken from the pairs, or from the 216 objects? Can some object have many verbs? What's an example?

Sometimes there's no object in the question ""pass me something to play"" vs ""pass me a puck to play"". Is there a reason that they are different? 

Additionally, I'm very unclear why only 5 objects were tested per category if many more were available. 5 seems very very low. The accuracy of the models doesn't seem to increase with training data which is questionable - is 5000 just not enough at the NN's need a million? Is 1000 the most needed? That seems odd. Are there p-values that can be tested on more data.

Finally, the baselines weren’t super useful. Can you vary your algorithm and test on a part of the whole algorithm as a better comparison? Perhaps “pick up a hammer to swing” vs ""pickup an object to swing"" so it can use the word hammer for the image recognition? This also goes along with my question about why sometimes there's a noun and sometimes there isn't.

Overall, I think the idea is interesting, but the results aren't complete enough for me.",,"I have some questions/concerns about this paper. 

First, the image encoder. I understand the motivation for using a pre-trained model to represent objects as a vector. It's not clear why the authors chose ResNet101 when other models have performed better in the ImageNet task (some listed in [1]), including the most recent efficientnet [2]. Moreover, it's not clear why the authors chose the layer that the chose other than the vector size matched their custom embeddings for cosine distance comparisons (though the two vectors are in different subspaces). It's also not clear why the authors chose the loss they did; additional hidden layers are often used in cases like this for semantic/language grounding tasks, for example for MSCOCO which the authors cite. 

What the authors are trying to accomplish is indeed a grounding task, but instead of grounding words to visual features, they are attempting to ground affordance symbols (derived from words in commands). I can appreciate their approach and what they are trying to do and I definitely think this area needs attention. However, the novelty of this approach needs to be made clearer: the authors only cite and compare their work to a few others who attempt to learn affordances, but that is largely what this paper is trying to accomplish by other words. There seems to be a few points of wording in this paper that would better align it with the larger research field. I would suggest couching this work more in the affordance literature, and to be consistent with what the users are saying (queries? commands?). 

Another concern are some of the results. Figure 3 shows a comparison of training size vs. accuracies, but I'm not sure what this is showing, and why it's relevant to their claims. It's well known that more data improves results, and that's expected here. Moreover, the metrics could be more informative: accuracy (i.e., top object matches the true object) makes sense, but I would suggest f-score for a topN (e.g., top 5) and mean reciprocal rank which would capture the overall quality of the model for all of the ranked results (i.e., where in the ranked list of potential objects does the model place the true object?). 

I would also argue that the claim that these are ""natural language"" commands needs to be softened a bit because the model essentially only cares about verbs (and objects) and the utterances are constrained to have a specific syntactic pattern. 

The approach the authors take could benefit from comparison to [3] which mapped natural language descriptions to objects at the word level. That is, instead of a joint training regime like done here, the authors in [3] condition the word on the visual features. 

[1] https://keras.io/applications/

[2] Tan, M., & Le, Q. V. (2019). EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ArXiv. Retrieved from http://arxiv.org/abs/1905.11946

[3] Schlangen, D., Zarriess, S., & Kennington, C. (2016). Resolving References to Objects in Photographs using the Words-As-Classifiers Model. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 1213–1223. Retrieved from http://arxiv.org/abs/1510.02125",,"The paper proposes an object retrieval method based on descriptions of the object's usage and visual inputs to generalize to unseen object classes with/without unknown nouns. To achieve this, the method generates embeddings for the natural language command and images, calculates the cosine similarities between embeddings, and selects the most similar image to the input command.

General comments:

- The novelty of this works is focusing on objects that can satisfy the task specified by the verb. This work could be especially useful when a robot cannot locate the specified object by a user but could find another one that can satisfy the given task. Further, the results show that the method is applicable to unseen object classes and unknown nouns in the commands. This generalization capacity is significantly important because the robot can still complete the task even if there are unknown objects/object classes in the user's commands. 
- The paper is well-structured. It motivates the problem (i.e., the reason of using the functionality of the objects on a user commend), proposes a method to resolve it (i.e., retrieving the target object when a user command contains the object's usage) and evaluates it on different datasets (both containing unseen object classes with/without unknown nouns).
- The authors can go through the literature on referring expression comprehension for additional references (e.g., Hatori et al. [1]).  
- The authors collected a dataset containing 655 verb-object pairs. Making this dataset publicly available can be a valuable contribution to further search. Moreover, extending this dataset could increase network accuracies. As the authors stated, the presented accuracies are far from perfect but it could still be useful to reduce the robot's search space from all the candidate objects.
- The training accuracies of the network aren't provided in the paper. This information could be helpful in interpreting the results better.

[1] J. Hatori, Y. Kikuchi, S. Kobayashi, K. Takahashi, Y. Tsuboi, Y. Unno, W. Ko, and J. Tan, ""Interactively picking real-world objects with unconstrained spoken language instructions,"" in ICRA. IEEE, 2018.",,,,Agreement accepted,07/16 15:00,07/16 17:00,https://github.com/Thaonguyen3095/affordance-language,https://youtu.be/WMAdGhMmXEQ,,,80.0,HOYvL5AwX38
"This paper presents a novel system for autonomous, vision-based drone racing combining learned data abstraction, nonlinear filtering, and time-optimal trajectory planning.The system has successfully been deployed at the first autonomous drone racing world championship: the 2019 AlphaPilot Challenge.Contrary to traditional drone racing systems, which only detect the next gate, our approach makes use of any visible gate  and takes advantage of multiple, simultaneous gate detections to compensate for drift in the state estimate and build a global map of the gates.The global map and drift-compensated state estimate allow the drone to navigate through the race course even when the gates are not immediately visible and further enable to plan a near time-optimal path through the race course in real time based on approximate drone dynamics.The proposed system has been demonstrated to successfully guide the drone through tight race courses reaching speeds up to 8m/s and ranked second at the 2019 AlphaPilot Challenge.",RSS2020_Author_Agreement_signed.pdf,"Philipp Foehn (ETH / University of Zurich)*; Dario Brescianini (University of Zurich); Elia Kaufmann (ETH / University of Zurich); Titus Cieslewski (University of Zurich & ETH Zurich); Mathias Gehrig (University of Zurich); Manasi Muglikar (University of Zurich); Davide Scaramuzza (University of Zurich & ETH Zurich, Switzerland)","Philipp Foehn (ETH / University of Zurich)*; Dario Brescianini (University of Zurich); Elia Kaufmann (ETH / University of Zurich); Titus Cieslewski (University of Zurich & ETH Zurich); Mathias Gehrig (University of Zurich); Manasi Muglikar (University of Zurich); Davide Scaramuzza (University of Zurich & ETH Zurich, Switzerland)",RSS_Alphapilot_final.pdf (3731335 bytes); RSS2020_Author_Agreement_signed.pdf (118008 bytes),RSS_Alphapilot_final.pdf,1137.0,81.0,,3.0,AlphaPilot: Autonomous Drone Racing,,0.776153449512203,"There are many components integrated into a system, which shows a very good autonomous drone racing performance. In this regard, what's lacking a little bit in this paper  is that there are not much comparative studies and statistical analysis. For instance, how much benefit (statistical values not just visual graphs shown in Fig. 6) you can get by detecting the multiple gates (from one gates to 2, 3, 4, ...)? Or what is the impact of using time-optimal motion primitives and sampling-based RH planner compared with other existing approaches? It would become a much more appealing paper if more comparison study results are included.",,"
In this work, the authors nicely present a complete estimation, planning, and control framework for high-speed navigation of an aerial vehicle through known gates. The framework has been applied in the context of the AlphaPilot drone race. The paper is overall well organized and pleasant to read. The proposed framework is well explained and motivated. 


The detection of the gaits corners is done with a convolutional neural network approach that extracts the so-called Part Affinity Fields as well. The two pieces of information are used to associate the detected corners to gaits. Although the explanation of the method is quite easy to follow, some important details are missing like:
* dimension of some variables like the ground-truth corner and PAF maps, i.e., C_j(s) and E_(k,l)(s). Although those are mentioned in Fig.4, a more detailed explanation would easy the reader understanding
* the reason why the detection of the four corners for several gates is not enough to associate corners to gates.
* the distance d mentioned at the beginning of page 5 is it in image space? How is it computed?
* where the gate frame is placed? I guess at the center of the frame. A clear statement should be added.
Lastly, at the beginning of page 6, the authors say: ""with p_C and R_IC being the constant transformation between the inertial frame I and camera frame C"". Since the camera is attached to the robot, which is moving, it is not clear to me how the transformation between the inertial frame I and camera frame C can be constant. 

The planning of the trajectory passing through the gaits is done using a sampling-based method to sample states (mostly velocities) at the gaits center. The sampled states are then connected with a time-optimal steering method that, assuming the system as a linear second-order system, can compute the optimal trajectory in closed-form. In view of the efficiency of the method, the planning, limited to the next three gaits, is done in real-time at each control step (50Hz). The authors also highlight the fact that the considered model is only an approximation of the robot dynamics. In view of the differential flatness of the system, a slight improvement could be to model the system with a fourth-order dynamics. Still, this will not encapsulate the real motor limits.

Finally, the trajectory is tracked with a classical cascaded control loop composed of an inner attitude control loop and an outer position control loop. In the description of the position controller, in particular in equation (18) the symbol R_IB_z,z is not defined. Furthermore, I assume that ddp_B is the reference acceleration not the actual. I would then change the symbol. 

The overall framework has been tested with real experiments obtaining impressive results: 5 m/s with a success rate of 100% and at 8 m/s with a success rate of 60% (the number of trials should be mentioned). Regarding the shown results, since the trajectory is re-planned at every control step, it is not clear what Fig.6 on the left shows. Is this the offline computation of the entire path?
Anyway, in view of the obtained nice results, I believe that the proposed framework is a valid starting point to improve the system autonomy and robustness. The authors pointed out very interesting future works like the inclusion of a perception-aware motion planner.",,"Overall it is a good paper. A bit dense and quite a few details are left out but the page limit is understandable. 

It was unclear how multiple gates are identified for the proper sequence of motion. Are there special stamps on the gates to indicate the order they need to be crossed? 


Abstract
Score does not show how good the robot did. It should be something that is explainable without extra information (e.g., knowing the rules of the competition)

Video
The video does show the drone successfully navigating the gates but it is unclear if it is making any other point on the correction of drift, planning,etc. Subtitles could help. 

Motivation
“The goal of the challenge is to develop a fully autonomous drone that navigates through a race course using only machine vision “ THe word “only” is not right. It is obvious that the drone is using onboard sensors such as inertial measurement unit and others to fly. 

Related work
What is “optic flow”? 

Contribution
Please be specific on contribution, which sampling-based path planner? 

II 
“The race course layout and gate positions are provided ahead of each race up to approximately ±3 m “

Is this the side-to-side variability or front-to-back or both? 

Table 1
What is LRF?

In equation 1 what is f?

In equation 2 what is omega?

III System interface
“The sensor and drone interface are provided by the race organizers and are thus omitted.”  Please give a high level description for completeness;  readers may not know the drone competition in detail or know where to find this.

III B “In order to compensate for a drifting VIO estimate, the output of the gate detection and VIO ” VIO is mentioned two times and incorrectly.
",Agreement accepted,"This is a well-written system paper about a high-performing system with some technical innovation. The biggest innovation appears to be in the vision front end, with the use of neural nets to detect corners of gates and Part Affinity Field methods plus graph algorithms to associate corners with known gates. The path planner has some innovation to plan through multiple gates at once. The state estimator can do updates with multiple gate measurements, though not necessarily in the highest performance manner possible. ",,,07/16 15:00,07/16 17:00,,https://youtu.be/DGjwm5PZQT8,,,81.0,k6vGEj1ZZWc
"We aim to endow a robot with the ability to learn manipulation concepts that link natural language instructions to motor skills. Our goal is to learn a single multi-task policy that takes as input a natural language instruction and an image of the initial scene and outputs a robot motion trajectory to achieve the specified task. This policy has to generalize over different instructions and environments. Our insight is that we can approach this problem through Learning from Demonstration by leveraging large-scale video datasets of humans performing manipulation actions. Thereby, we avoid more time-consuming processes such as teleoperation or kinesthetic teaching. We also avoid having to manually design task-specific rewards. We propose a two-stage learning process where we first learn single-task policies through reinforcement learning. The reward is provided by scoring how well the robot visually appears to perform the task. This score is given by a video-based action classifier trained on a large-scale human activity dataset. In the second stage, we train a multi-task policy through imitation learning to imitate all the single-task policies. In extensive simulation experiments, we show that the multi-task policy learns to perform a large percentage of the 78 different manipulation tasks on which it was trained. The tasks are of greater variety and complexity than previously considered robot manipulation tasks. We show that the policy generalizes over variations of the environment. We also show examples of successful generalization over novel but similar instructions.",Author Agreement Form.pdf,Lin Shao (Stanford University)*; Toki Migimatsu (Stanford University); Qiang Zhang (Shanghai Jiao Tong University); Kaiyuan Yang (Stanford University); Jeannette Bohg (Stanford),Lin Shao (Stanford University)*; Toki Migimatsu (Stanford University); Qiang Zhang (Shanghai Jiao Tong University); Kaiyuan Yang (Stanford University); Jeannette Bohg (Stanford),Author Agreement Form.pdf (57512 bytes); ConceptLearning (61).pdf (4754575 bytes),ConceptLearning (61).pdf,1247.0,82.0,,3.0,Concept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations,https://sites.google.com/view/concept2robot,0.12471635558280197,"I like the paper and it is obvious a whole lot of work was put into the paper. So much actually, that it is perhaps a bit much for a single paper. Maybe it would have been better to split in several papers. For example, the complete algorithm is (I think) like this: 1) initial image and natural command are the input, from which a concatenated feature vector (from BERT and ResNet18) is made. 2) MLP is used to upconvolute the1X7 and TX7 vectors. These make a trajectory. 3) Trajectories are executed and evaluated, which is used for reinforcement learning of the trajectories. 4) Multi-task is learned using these learned trajectories. The paper explains briefly over each of the steps very systematically, however, given the amount of content, it cannot go into very specific details of any of them. If I wanted to re-do the experiments, I would be extremely hard pressed, for example, even to do the BERT or ResNet-18, because all you say is that you feed it through a MLP and start and end dimensions. It is even more difficult with reinforcement learning (3.E), where everything is described on a high level, but not in real details. If the content were in more papers, more details could be provided. Given the amount of content and state-of-the-art approaches, it is a bit strange that only 2 equations are provided.
It is a bit surprising that eq(1) was used instead of actual DMPs, where the equation is the same, only the forces are encoded with a smaller set of variables, distributed over the phase (and not time). I am not sure how given that the dimension f is much higher then with a DMP fits into all the networks, for example the up-convolution could be 1X7 and (for example) 25X7 (or even less, where the gripper could be described with fewer).
All in all, the paper shows an extraordinary amount of very interesting work. Showing something in the real world would make it even better.
Please cite statements in the introduction, starting with the first one: Humans are remarkably intelligent creatures.
N rollouts of state-action tuples (s^i, a^I, y^i) j=1:N -> what is j?
",,"This paper tackles a variety of problems and does extensive experiments.
It is great to see the analysis on the different aspects of the paper.
Yet, there are several critical design decisions which are not clear and references are missing.
Learning Feedback Terms for Reactive Planning and Control, would be an important reference which actually learns forcing and coupling terms via MLP for DMP formulations.

Where is the diminishing term in front of the forcing terms which ensures convergence?
It's surprising that a fixed/discrete representation is used rather than the classical DMP formulation with a canonical system. 
This formulation ends up being a different parameterization of an open loop trajectory with no clear benefit of choosing this representation.

Why is the MLP from embedding to forcing terms referred to as 1D upconvolution when there is no data to convolve over? The presented approach predicts not a policy but rather a trajectory for a given initial state which is executed in open loop with a discrete number of samples.

Isn't the goal representation 1x7 but referred in the text with 7x1.

Given the trajectory/action representation which is very high dimensional it would be worth discussing CEM. It is not clear what T is from the description of the paper. 

Why is the single task approach referred to as RL when it is really a contextual bandit. The optimization problem does not consider closed loop state action pairs but a context, action/trajectory representation which boils down to a supervised learning problem in this case. The whole trajectory gets optimized by the reward and not the individual state/action pairs.

Although it is great to see that the paper tackles many different aspects, it would be greatly beneficial if the paper would focus on a more core message. For example, learning single tasks policies vs trajectories using a learned reward functions. Then the individual contributions could be analyzed in more detail, for example using a policy vs trajectory and different representations.
It is nevertheless interesting to see that essentially the trajectory representation given context lends itself for multi task behavior cloning which seems to work well given the chosen embedding space. This is a good result.",,"The paper contributes to an important area of multi-modal, multi-task learning problem. The exposition is clear and well positioned. The method is novel and technically sound. The evaluations are nicely constructed to evaluate each component of the method. A more recent baseline would have been better. Parametrization of the trajectories is a clever way to ensure dynamic feasibility.

Questions:
- It is interesting that the model works without attention layer. Could the authors comment as why is that?
- Why is CEM applied after Q network learning in the single task learning?

",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,,https://sites.google.com/view/concept2robot,,,82.0,flxrirLbxzg
"The use of simple control schemes with only a few basic sensors and no feedback allows improved stability when traversing unforeseen rough terrain by applying a single controller. Exploiting multiple controllers simultaneously can further improve robustness but is often mechanically hard to implement, especially when stiffness modulation is a controller. To overcome this limitation, we investigate and simulate a leg shape that applies variable leg stiffness and free-leg length. The leg shape couples the physical parameters with the leg angle of a monopod, while the leg orientation is governed with only a simple control law during the flight phase. We study the usage of an optimal controller coupling and show that it can increase robustness to perturbations in the initial horizontal velocity when traversing unknown rough terrain. This work presents the process of obtaining the optimal coupled parameters and demonstrates its benefits. This work also lays the foundations for a conceptual leg shape to exhibit the controllers physically.",RSS 2020 Contributing authors letter of agreement signed.pdf,Adar Gaathon (Technion - Israel Institute of Technology)*; Amir Degani (Technion - Israel Institute of Technology),Adar Gaathon (Technion - Israel Institute of Technology)*; Amir Degani (Technion - Israel Institute of Technology),RSS 2020 Gaathon Degani rev2 submitted.pdf (1037507 bytes); RSS 2020 Contributing authors letter of agreement signed.pdf (220573 bytes),RSS 2020 Gaathon Degani rev2 submitted.pdf,1159.0,83.0,,3.0,A Variable Rolling SLIP Model for a Conceptual Leg Shape to Increase Robustness of Uncertain Velocity on Unknown Terrain,[Not Answered],0.442458535283951,"
## Small points

* Why specifically are you looking at this leg shape?
* great background section.  Each paper's contribution is briefly explained.
* Please explain in more detail your rationale for selecting that specific cost function against the many other design and performance considerations.  Give an example why a leg optimized for that criteria is better suited for locomotion.
* good explanation of how inconsistencies were weeded out via penalizaton.
* the first paragraph of IV-A is a bit hard to read. Where is $f_{val}$ defined?  I assume it is the result of your optimization process?  Help out your audience a bit.
* Each section dives straight into details without providing an overview of what is to be done and the general approach.  Without that, all this detail is a bit excessive because we don't know how you will synthesize this into a comparitive study.  What are you comparing? The methods section is one such example.
    * why polynomials?
    * what are the five design variables for constant SLIP?  Unclear.
    * I assume the 9 polynomials for the two variable slip models are $\alpha_{(1,2,3)}$, $\beta_{(1,2,3)}$ and $\gamma_{(1,2,3)}$?
    * What is the goal? Is it to compare leg designs or leg models?  Is comparing one equivalent to the other?  Please explain your rationale and motivation in more depth.
* is it feasible that the resulting optimal designs might not be able to be fabricated?  If so, how could you bound for manufacturability?",Agreement accepted,"This paper is well written and clearly presents the work that was done. The questions addressed in the paper are fundamental for the trajectory planning of dynamic legged robots. To the best of my knowledge, the proposed approach is novel and original. The following issues should perhaps be better explained in the paper:
1- The choice of the polynomials used to represent the possible variations of the free-length and the stiffness seems arbitrary.  Could other functions be used? Are these polynomials the best possible choices?  Perhaps this choice could be better justified. 
2- How are the query points selected in the simulations? Numerical results are provided for the query points and it would be nice to provide a better justification for their choice. 
Finally, in Section III: 'gravitational velocity' should be 'gravitational acceleration'",Agreement accepted,"Overall the manuscript is well organized and the analysis of the results is fairly complete and informative. The results show promising potential if authors can experimentally demonstrate their findings in future work. However, sections I and II are poorly written and do not define well the problem being solved. Authors should include a cartoon or schematic of the model and provide animations of some of the results to help the reader contextualize and appreciate the contributions of the manuscript. The presentation of the leg model have no context and is very confusing. The actual physical system that the study is representing with the reduced model in figure 1 needs to be described in more detail in the introduction. It is not obvious how the reduced model in Fig. 1 represents any sort of robot leg. Having some familiarity with  the ""RHex"" robot (https://robots.ieee.org/robots/rhex/) helps understanding, but authors make no explicit mention of that.
Other issues that must be addressed before publication include: 

The first three sentences of the abstract are very generic and have no context. Abstract should have an opening sentence that clearly identifies the topic of the manuscript. Next there should be a brief motivation of the solution presented. Followed by a summary of the result and potential outcomes. 

Very confusing and unfounded argument in the second paragraph of the introduction, correlating the number of controllers implemented and the stability/robustness of the system. There are no references cited for such claim.

""Minimalistic control"" is only defined in section II.A but the term is utilized in the abstract.

Define ""y_{rel}"" before using it in paragraph 3 of II.B. 

The variables utilized for the optimization process in III.C are confusing. It reads as if the authors utilize ""y_{rel}, y_0"" and ""p,q"" interchangeably, but this is not defined anywhere. A cartoon or schematic illustrating the optimization scenario and defining the variables utilized would be very helpful. 
 
Is the acronym definition correct in the third paragraph of the intro ""leg angle during descent (SLR)""? Or to authors mean the utilize a ""swing-leg retraction"" controller during flight? Please clarify.

Add the text ""for unknown horizontal velocity"" and ""for known horizontal velocity"" to the caption of figures 4 and 5, respectively. In fact, please modify the caption of all figures which show the values of the cost function to facility identification of separate experiments. 

There's no mention of the impact dynamics model utilized. Is impact considered in the simulation? If not, please include this claim and any other assumption made for the model.

Authors mention: ""[...]  to prevent crashing on the ground which is severely penalized."" Define what crashing mean and how it is penalized.

References [12] and [30] refer to the same paper.
",Agreement accepted,,,,07/16 15:00,07/16 17:00,[Not Answered],[Not Answered],,,83.0,iB_L2vGyVD4
"High-density afferents in the human hand have long been regarded as essential for human grasping and manipulation abilities. In contrast, robotic tactile sensors are typically used to provide low-density contact data, such as center-of-pressure and resultant force. Although useful, this data does not exploit the rich information content that some tactile sensors (e.g., the SynTouch BioTac) naturally provide. This research extends robotic tactile sensing beyond reduced-order models through 1) the automated creation of a precise tactile dataset for the BioTac over diverse physical interactions, 2) a 3D finite element (FE) model of the BioTac, which complements the experimental dataset with high-resolution, distributed contact data, and 3) neural-network-based mappings from raw BioTac signals to low-dimensional experimental data, and more importantly, high-density FE deformation fields. These data streams can provide a far greater quantity of interpretable information for grasping and manipulation algorithms than previously accessible.",RSS Author Agreement.pdf,Yashraj Narang (NVIDIA)*; Karl Van Wyk (NVIDIA); Arsalan Mousavian (NVIDIA); Dieter Fox (NVIDIA),Yashraj Narang (NVIDIA)*; Karl Van Wyk (NVIDIA); Arsalan Mousavian (NVIDIA); Dieter Fox (NVIDIA),RSS_2020_Camera_Ready.pdf (4301458 bytes); RSS Author Agreement.pdf (414509 bytes),RSS_2020_Camera_Ready.pdf,1292.0,84.0,,3.0,Interpreting and Predicting Tactile Signals via a Physics-Based and Data-Driven Framework,https://sites.google.com/nvidia.com/tactiledata,0.8852533132369208,"Both the setup used to collect data, and the architectures of the models, rely on well-understood methods, and in that sense the contribution of the paper is primarily at a system level. The finger itself is also an established design. Still, in this form, it is likely to be of high value to the community, as is the dataset itself, which is released as part of the paper. 

However, I find that the key contribution of the paper is subtly different, even though it is not stated in this form: the fact that such a model *can* be trained for a sensor such as the BioTac, while perhaps suspected, had not been previously established. The community has long suspected that useful information is hidden in the raw data from the BioTac. This paper takes an important step forward in quantifying this assumption, and the results are positive (yes, the information is there). 

Other recent or contemporary studies are obtaining similar results with fingers that obtain similarly distributed raw signals with different modalities (piezoresistance, optics, etc.) This points to the fact that the results shown in this paper are likely to generalize beyond just the BioTac, and towards a general approach of building fingers with raw data meant to be interpretable by learned models. 

The paper is well written, and the methods and techniques are sound. Additional areas of improvement include the results: the paper essentially presents the absolute minimum needed to support its conclusions, even though significantly more thorough analysis could be done with the data set that has been collected. Example questions that could be answered include:
- How is performance affected by indentation force? One could expect better performance at higher force levels, but needs to be quantified.
- How are results expected to generalize to contacts with indenters not part of the training? This could be established via a leave-one-out analysis where an indenter is left out of the training set and used to evaluate.
- How do models transfer between fingers? E.g. how does a model trained on finger 1 transfer to finger 2?
- Etc.
To free up space for such an analysis, the authors could remove the highly preliminary section on electrode value synthesis, make the setup description slightly more compact, and generally rewrite sections of the paper for compactness.

In conclusion, the paper is sound, well written, and presents conclusions and system-level contributions that could be of great value to the community. A significant expansion of the data analysis section would significantly improve the value and the strength of the paper.
",Agreement accepted,"This is a sound paper that provides important information for the community working with the BioTac sensor. The information includes a dataset of contact locations, forces, and raw electrode values; a 3D FE model to estimate deflection; and ad-hoc tactile mappings.

My major concern with this paper is the generalization of its results, to both BioTacs and other tactile sensors. 

For the first case (i.e., generalization to BioTacs), the authors, for instance, claim that it is provable that the resulting 3D FE model generalizes over multiple interactions. This is however no found in the manuscript and validation experiments are limited as low forces (<0.5N) were neglected. 0.5N for the analyzed case (i.e., tactile information) seems indeed a high value.

For the second case (i.e., generalization to other tactile sensors), it is not clear how the methods could be tailored to other tactile sensors. The paper in its current state is very BioTac oriented, to the point that I think this word should be included in the title. The manipulation community could benefit if the authors could make the discussion broader, or at least give insights about the applicability of the techniques to other technologies. I think this could improve the impact of this work.
",,"In this paper, the authors proposed some solutions to advance the modeling and interpretation of tactile sensors, focusing on a commercial tactile sensor that is widely used in the research community, the BioTac. In particular, their outcomes include a thorough dataset for the sensor, consisting of three sensors, nine indenters, a randomly-generated set of indentation points and a mechanical registration procedure. They also created a very accurate, three-dimensional finite element model of the BioTac. Finally, they validated the model’s accuracy and generalizability across physical interaction experiments.
The paper is very well written, clear and detailed.
Here follows some comments for each session.

Introduction:
The work is well motivated and the contributions are listed and presented in a clear way.

Related work:
The way this section is structured and, in particular, the summary at the end of section A, help having a clear overview of the state of the art in this field.
Please, use RMS instead of rms and explain the acronym the first time is used. This is repeated across all the sections.

Methods:
Also this section is well written and full of details. I like the idea of collecting the dataset using multiple indenters. Most of the dataset available do use only one intender.
It would be interested to have a comparison in results with the case when data from each indentation sequence was not contiguous during the training of PointNet++. 

Results:
The results obtained are very good and they are explained in a clear manner.
I think that section D of the experimental could be omitted and left for future work.

Conclusions:
Also this section is very clear and provides nice ideas for future development.

Extra notes:
Please, check proper spaces between words and citations (e.g. Section II.A: methods[20]).",Agreement accepted,,,,07/16 15:00,07/16 17:00,,https://youtu.be/wLA-WKaeyN4,,,84.0,xwvV3BPkxhU
"Training robotic policies in simulation suffers from the sim-to-real gap, as simulated dynamics can be different from real-world dynamics.Past works tackled this problem through domain randomization and online system-identification.The former is sensitive to the manually-specified training distribution of dynamics parameters and can result in behaviors that are overly conservative.The latter requires learning policies that concurrently perform the task and generate useful trajectories for system identification.In this work, we propose and analyze a framework for learning exploration policies that explicitly perform task-oriented exploration actions to identify task-relevant system parameters.These parameters are then used by model-based trajectory optimization algorithms to perform the task in the real world. We instantiate the framework in simulation with the Linear Quadratic Regulator as well as in the real world with pouring and object dragging tasks.Experiments show that task-oriented exploration helps model-based policies adapt to systems with initially unknown parameters, and it leads to better task performance than task-agnostic exploration.",RSS2020_Author_Agreement.pdf,"[Jacky Liang](https://www.jacky.io)Saumya Saxena, Oliver Kroemer, ",Jacky  Liang (Carnegie Mellon University)*; Saumya Saxena (Carnegie Mellon University); Oliver Kroemer (Carnegie Mellon University),RSS2020_Author_Agreement.pdf (64467 bytes); Learning Active Task-Oriented Exploration Policiesfor Bridging the Sim-to-Real Gap-1-10.pdf (4270622 bytes),Learning Active Task-Oriented Exploration Policiesfor Bridging the Sim-to-Real Gap-1-10.pdf,39.0,85.0,,3.0,Learning Active Task-Oriented Exploration Policies for Bridging the Sim-to-Real Gap,https://sites.google.com/view/task-oriented-exploration/,0.6138539758220379,"After reading the paper, the method and results are clearly exposed.

 I lack better motivation for the need and importance of the approach given its strong assumptions: there is known model of the dynamics of the system, and obtaining a policy with it for a given tasks is not difficult.

The real world results are only shown for simple tasks which have low dimensional policies and parameters spaces. Moreover, some tasks seem too tailored to show that task oriented is more beneficial than the agnostic counter part. Therefore it is hard to assess if the contributions of the paper are of true significance. 

Below I share my comments by section.

ABSTRACT
This sentence was hard to parse: “learning exploration policies that explicitly perform task-oriented exploration actions to identify task-relevant system parameters”.  

INTRODUCTION

The introduction starts with  RL, but RL is not mentioned in the abstract. 
The sentence “However, due to differences in simulation and real-world dynamics …” is too long. (The last part “ the distributional differences between…” might be redundant)

Name consistency: simulation-to-reality gap or sim-to-real gap

This sentence is unclear: “use models learned from real-world data to directly perform trajectory optimization for manipulation tasks”. How does that relate to adapting simulation parameters? Or to the sim-to-real gap?

I would be more clear in this sentence “generalize on environments with different physics”, because real world has just one physics. I agree that generalizing to different environment parameters would help like object mass.

Introduce the proposed approach and contributions sooner in the introduction. The first 4 paragraphs could be reduced and more clear and to the point on what is the problem that this paper aims to solve and why is it hard and worth studying.

In this sentence “It explicitly learns an exploration policy that interacts with the real world
and identifies the initially unknown parameters of a known model” it is unclear if the exploration policy is leaned in simulation or in the real world. 

Avoid unnecessary repetition in this sentence: “The experiments show that task-oriented exploration helps model-based policies adapt by identifying system parameters and that task-oriented exploration leads to better task performance than task-agnostic exploration.”
Also it is unclear if model-based policies actually adapt. My first understanding was that this paper first estimates the parameters and then execute the task without any “going back to re-estimating them” in the world. 

I would suggest to put more emphasize even at the introduction on what parts should happen in simulation and which in the real world.

Reference Fig 1 in the intro to help comprehension. Also Fig 1 and Fig 2 do not seem to be referenced anywhere in the text.

Stating clearly the contributions of the paper (like in bullet points) would help the reader.

RELATED WORK

There is a large emphasis on domain randomization (DR), however in this work it is assumed that the model is known and the parameters of the world can be estimated. Therefore the task policy becomes determined by the estimated parameters and not learned. How does that link with DR?

I think the authors should emphasize more that the reason why their approach is useful is that instead of learning one policy that fits “all”, they learn a way to estimate parameters that will work well in different scenarios as long as the model describing the system is right. Therefore they can generalize better. I am missing this reflection when putting emphasis on DR. That might also help reduce the long discussion about DR. 

I would also make more clear the transition between DR and Sys-Id. If the distributions for DR are not too big, it could still be useful or combined with Sys-Id. They are not opposite ideas.

There is a big jump when talking about residual learning. Residual learning assumes that the models aren’t perfect, but that is something that might be true even when using this work and it is not addressed. How is this work better or does not require residual learning?  Why Active sys-Id would not require residual learning? 

The paragraph “By contrast ….” starts with passive vs. active sys-id but ends taking about task-agnostic vs. task-oriented. I would suggest splitting it for clearness.


Compared to reference [8], this works is more restrictive as it assumes that the model is known and only a few parameters have to be estimated. This assumption probably does not apply to many complex tasks. Why is it worth exploring then? The work would benefit for more clearly stating the importance of developing methods that assume known-models.

METHOD OVERVIEW

Grammar: “that will make the model sufficiently match real-world dynamics”

During the section, it is said that the approach also requires models of the objects, the robot, as well as their initial states. However, these are parameters that the approach could actually try to estimate. By assuming they are known, this make the problem less applicable to realist situations.

The authors say that real-world dynamics are stochastic while simulation is deterministic.
How is then the uncertainty in the real world modeled? Shouldn’t it be part of the parameters to estimate? 

I think it would be really interesting to see how both policies (exploration and task) are optimized simultaneously. Maybe even the parameters themselves could be optimized to be as meaningful as possible rather than pre-fixed.

This sentence is confusing : “This procedure is active as it interacts with the real system to
generate informative observations”  if learning the exploration policy is done in simulation.

LQR

“Our goal is … task cost min J  “   it is unclear what “min J” means.

In the LQR case, it is assumed a very particular form of the dynamics where the parameters theta are the eigenvalues of the matrix A, and B is independent of any parameter. How reasonable is that? It feels that B could also depend on theta. Are there “real” systems that show this behavior where B does not depend on gravity, mass, friction etc? Giving an example where that is the case would help.

Equation 8 assumes that function g corresponds to the L2 norm between trajectories, I would make that more explicit in the text.

The results on the simulated LQR are clear and show lower variance and better performance for the task oriented exploration.

REAL WORLD

Explain in more detail why it is important or relevant to pick these two tasks (pouring and dragging), and the expected differences between having analytical model + discrete action space  vs.  full simulation + continuous action space. 

POURING

For the first task, there is only one model parameter  (the angle tilt of the cup).
The system is engineered to show the importance of task-oriented exploration by adding  a ""distraction cup"" so that when learning the exploration policy it makes more sense to always lift the task from which you pour. 

The task also requires during the exploration face to always lift each cup at the beginning. Then the learnt exploration policy starts choosing which cup will be lifted again.

As expected, the task oriented approach does better at realizing that it is better to just gather observations from the target cup. Results clearly confirm that.

What does the g in “14g” and “22g” mean? Is it grams? I would be consistent between the use of kg and g.

The results in Fig. 5 still have large variances which makes it less clear if it is fair to claim that task oriented is better (very likely that is the case though).  That might be because only 10 trials where used per method. Taking a much larger number would help assess the validity of means and variances. With only 10 samples their values are likely to fluctuate.

DRAGGING

This task has 3 model parameters to estimate. And the exploration policy is defined with 2 parameters.  As a result, both real world tasks are quite low dimensional in both policies and parameter spaces.

In more high dimensional spaces and for harder tasks, aiming to solve the action exploration problem in a task-oriented fashion might be too hard or costly, specially if posed as an RL problem. It is not clear to me that for such systems a simple task independent approach could simplify the problem and make the estimation of the parameters better/faster. 

Why a ratio of 3 : 1  between rotational and transnational?

There are no results comparing passive vs. active exploration.

FIGURES

The text on some graphics is too small. 
",Agreement accepted,"The paper contributes to an important and active area of research in robotics. The authors insight into optimizing for inference of task-relevant system parameters is valuable and the proposed approach is very interesting. I commend the authors in particular for their methods section, the explanation of the approach is clear and figure 2 is very helpful in understanding the approach. I have two comments on how the paper could further be improved:

First, the metrics and system identification setup, in particular for the methods section and the LQR experiments, would benefit from clarification. It is my understanding that system identification is done with the goal of minimizing errors in prediction, not error in identified parameters. The task-agnostic system identification proposed for LQR in simulation is to reduce 2 norm error in true system parameter (which is not available in the real world and only doable in simulation). I would be very curious to see the performance of the task-oriented approach proposed by the authors versus traditional system identification on the LQR system where the input is white noise (or close to it) and parameters are optimized for predictive performance of the model. This comparison would be a fairer approach to traditional system identification methodology. The usage of regret loss ratio is also not intuitive to me, and is not repeated for the other experimental setups (others use cost) so I am not clear as to why this choice was made.

Second, it would be good to include the citations to \cite{kolev2015physically} and \cite{fazeli2017parameter} for system identification through contact, \cite{ allevato2019tunenet } for a similar approach to predicting system parameters using a learned model -- can be used as a benchmark, and \cite{weber,ogawa} for related system identification through contact approaches that are less learning based.

@inproceedings{kolev2015physically,
  title={Physically consistent state estimation and system identification for contacts},
  author={Kolev, Svetoslav and Todorov, Emanuel},
  booktitle={2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)},
  pages={1036--1043},
  year={2015},
  organization={IEEE}
}

@article{fazeli2017parameter,
  title={Parameter and contact force estimation of planar rigid-bodies undergoing frictional contact},
  author={Fazeli, Nima and Kolbert, Roman and Tedrake, Russ and Rodriguez, Alberto},
  journal={The International Journal of Robotics Research},
  volume={36},
  number={13-14},
  pages={1437--1454},
  year={2017},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{allevato2019tunenet,
  title={TuneNet: One-Shot Residual Tuning for System Identification and Sim-to-Real Robot Task Transfer},
  author={Allevato, Adam and Short, Elaine Schaertl and Pryor, Mitch and Thomaz, Andrea L},
  journal={arXiv preprint arXiv:1907.11200},
  year={2019}
}

@article{weber2006identification,
  title={Identification of contact dynamics model parameters from constrained robotic operations},
  author={Weber, M and Patel, K and Ma, O and Sharf, I},
  year={2006}
}

@inproceedings{ogawa2014dynamic,
  title={Dynamic parameters identification of a humanoid robot using joint torque sensors and/or contact forces},
  author={Ogawa, Yusuke and Venture, Gentiane and Ott, Christian},
  booktitle={2014 IEEE-RAS International Conference on Humanoid Robots},
  pages={457--462},
  year={2014},
  organization={IEEE}
}",Agreement accepted,"The paper is very clearly written, well presented, and well motivated. The distinction in the performance metric for the exploration policy w.r.t. existing sysID methods (whether they are optimized for information gain, parameter convergence, or prediction error) is clear. 

 The LQR analysis, while a bit limited due to questionable simplifications (B=I, n=m, R=0, the last being particularly strong), is still sufficiently insightful. 

However, the paper lacks somewhat in the verification and experimental validation. The pouring task is rather contrived. If its sole purpose was to be a toy example highlighting the natural limitation of never picking up the actual test glass, the impact is somewhat diminished owing to an unnecessarily poor baseline. What if the task-agnostic exploration policy used another performance metric - e.g., prediction performance (active SysID as classified in the paper)? Similarly, it is not readily clear why a passive SysID method (e.g. refs [21, 23, 29] in the paper) that leverages task policies + online estimation would also not be competitive. The same applies to the object dragging example. It appears that the task-agnostic method is worse than random exploration even with more data. This strongly suggests an unfairly disadvantaged baseline. 

The second limitation which is briefly alluded to in hindsight in the object dragging example is on the implementation of the method. Given the additional ‘meta’ layer aspect to the problem structure (exploration guiding estimation, which influences policy, and eventually performance), there is an additional layer of complexity in optimizing for the desired exploration policy. The provided robot examples are both extremely low-dimensional (1 and 3), and use finite-differencing for gradient estimation. This naturally will not be scalable in its present algorithmic form. Some discussion of this limitation is warranted, perhaps along with an additional example that has some differentiable components to reduce the need for finite-differencing. Alternatively, the authors may wish to consider blackbox (derivative-free) optimization techniques that can scale to moderately-sized problems. ",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,,,,,85.0,lwefvSL4C-E
"A shared grasp is a grasp formed by contacts between the manipulated object and both the robot hand and the environment. By trading off hand contacts for environmental contacts, a shared grasp requires fewer contacts with the hand, and enables manipulation even when a full grasp is not possible. Previous research has used shared grasps for non-prehensile manipulation such as pivoting and tumbling. This paper treats the problem more generally, with methods to select the best shared grasp and robot actions for a desired object motion. The central issue is to evaluate the feasible contact modes: for each contact, whether that contact will remain active, and whether slip will occur.  Robustness is important. When a contact mode fails, e.g., when a contact is lost, or when unintentional slip occurs, the operation will fail, and in some cases damage may occur. In this work, we enumerate all feasible contact modes, calculate corresponding controls, and select the most robust candidate. We can also optimize the contact geometry for robustness. This paper employs quasi-static analysis of planar rigid bodies with Coulomb friction to derive the algorithms and controls. Finally, we demonstrate the robustness of shared grasping and the use of our methods in representative experiments and examples.",RSS2020_Author_Agreement_signed.pdf,"[Yifan Hou](http://www.cs.cmu.edu/~yifanh/), [Zhenzhong Jia](https://scholar.google.com/citations?user=9dypDcAAAAAJ&hl=en), [Matthew T. Mason](https://www.cs.cmu.edu/~mason/)",Yifan Hou (Carnegie Mellon University)*; Zhenzhong Jia (SUSTech); Matthew Mason (Carnegie Mellon University),RSS2020_Author_Agreement_signed.pdf (168253 bytes); RSS20PartialGrasp.pdf (2614444 bytes),RSS20PartialGrasp.pdf,1293.0,86.0,,3.0,Manipulation with Shared Grasping,,0.11553477999860301,"Automatic generation of hybrid force-position control commands for object manipulation is exciting. This paper makes important contributions to this line of work, particularly analyzing the conditions for the stability/feasibility of contact modes and also proposing a way to quantify the stability of a contact mode.
My suggestions for improving the paper are:
* While the demonstrated manipulation experiments are impressive, the paper can provide a deeper connection to the algorithms. For example in Section VII-B, it is said that object/hand velocity is input, but it would be good to be clear what these velocities are and whether they leave any freedom in terms of which mode to choose. A presentation of the different stability margin values for these different modes that could have been chosen by the algorithm, and even experiments with different contact modes, would have connected the experiments better to the paper's main body.
* While the experiments in VII-D show that checking whether the stability margin is positive or not can be used to analyze the feasibility with respect to different geometries, it does not really say anything about whether a positive stability margin that is larger than another positive stability margin, makes the former contact mode more stable than the latter. 
* Some symbols used in the algorithms are not defined: G, bG, C_{AF}.
* Equation 23 should use \Delta(Fj,Ei), not D(Fj,Ei). ",Agreement accepted,"The paper studies ""shared grasping."" This is an important topic. Similar ideas have been examined since the 1980s. However, the present form of the paper lacks clarification of its novel contributions. The four suggested contributions enumerated on p.1 have been around. The manuscript can further be revised to clarify how the authors have improved them.

Sec. I Introduction even begins with false statements:
1. Grasping is among the most popular ways of robotic manipulation because of its robustness.
-> This is not true. Making grasps robust is not a straightforward issue.
2. A grasp can neutralize disturbance forces in any direction up to a magnitude deter- mined by the gripping force.
-> Not true if not force-closure grasps.

Sec. II Related Work seems acceptable.

Sec. III studies ""partial grasping."" Does it mean ""shared grasping?"" The term is not defined anywhere. In any case, the reviewer is not sure of its novelty. The section results in the relationship between contact forces and external wrenches. The reviewer doesn't see any difference between it and the ""grasp map"" addressed in Murray, Li, Sastry's textbook. Unfortunately, the notation of the paper is more difficult to follow (actually the entire text has this problem).

Sec. IV also exhibits a similar problem. There are many ways to check force equilibrium through, for example, linear programming as also mentioned by the authors. Although the paper claims to present a geometric approach, it is already well known that such a geometric analysis is doable by visualizing wrench cones on the projective plane.

Sec. V shows some similarity with the literature on grasp quality measure, which also has a long history. Sec. VI needs to be revised with an analysis of the completeness and correctness of the two algorithms. The results presented in Sec. VII could have been more exhaustive.",,"This paper studies manipulation by ""shared grasping"", where the object is contacted by a hybrid force-velocity controlled rigid-body robot hand and a stationary environment.  The approach is based on rigid bodies and Coulomb friction, enumerating a discrete set of possible contact modes, evaluating feasible velocities, and finding force-motion combinations that satisfy the quasi-static assumption.  There is also a nice experimental component demonstrating the feasibility of the approach.

A drawback of this paper is the lack of clarity in the presentation.  A few examples are outlined below.

1) The abstract and the introduction mention planar manipulation, but Section III.A says that the ""derivations in this section work for both 3D and planar problems.""  This creates a lack of definiteness that weakens later parts of the paper.  The discussion of contact modes and rotating the normal by 90 degrees to get ""the"" tangential contact screw is specific to planar cases, and the quick ""fix"" of mentioning of discretizing a cone doesn't do the 3D case justice.  The graphical representations and some of the other discussion are also specific to the planar case.  It would be better for the paper to focus on planar manipulation for definiteness (including giving dimensions of important vectors and matrices), then perhaps mention near the end what would need to be done to generalize it to 3D.  This is already mentioned in the conclusion anyway.

2) Reciprocal, repelling, and contrary are implicitly defined, at best, and the matrices N and M before eq (3) are not clearly described, including their relation to eq (2) and why they would be different from each other.

3) Some other variables and terms are not clearly defined.  It appears both ""velocity"" and ""twist"" are used to mean the same thing.  F is defined to include an undefined object force f_O, which only later we are told is zero.  F is not used in equations (6)-(8), so perhaps it should be saved for the force section III.B.2.  Section IV.A talks about the ""feasibility of (10)"" but this is unclear without context.  (10) is a vector equation and a vector inequality.  I assume ""feasibility of (10)"" refers to the existence of a tau' (and an f_H?) that satisfies (10) given other variables in (10)?  (Similarly for the later mention of the ""feasibility of Equation (14)"".)  There is also mention of the ""dimension of force control and velocity control"" without definition.  The description of IV.B.1 is not explicit that velocity control means choosing V.  It is not clear if the ""geometrical approach"" provided in Section IV (as opposed to linear programming or other computational methods?) is only for visualization or understanding, or if it has some other benefit.  Statements like ""Make sure the order of vectors is correct"" don't seem to fit a technical paper.  The rationale for the choice of the terms ""V-infeasible"" and ""V-impossible"" is unclear.

Overall the paper appears correct, and a nice step in the direction of automatic planning and execution of multi-mode manipulation sequences.  As currently written the paper is difficult to read, however, partially for the reasons above.  
",,,,Agreement accepted,07/16 15:00,07/16 17:00,,https://youtu.be/tyNhJvRYZNk,,,86.0,JkRoWO9YagE
"Learning-based control aims to construct models of a system to use for planning or trajectory optimization, e.g. in model-based reinforcement learning.  In order to obtain guarantees of safety in this context, uncertainty must be accurately quantified.  This uncertainty may come from errors in learning (due to a lack of data, for example), or may be inherent to the system.  Propagating uncertainty forward in learned dynamics models is a difficult problem.  In this work we use deep learning to obtain expressive and flexible models of how distributions of trajectories behave, which we then use for nonlinear Model Predictive Control (MPC).  We introduce a deep quantile regression framework for control that enforces probabilistic quantile bounds and quantifies epistemic uncertainty.  Using our method we explore three different approaches for learning tubes that contain the possible trajectories of the system, and demonstrate how to use each of them in a Tube MPC scheme.  We prove these schemes are recursively feasible and satisfy constraints with a desired margin of probability.  We present experiments in simulation on a nonlinear quadrotor system, demonstrating the practical efficacy of these ideas.",RSS2020_Author_Agreement.pdf,David Fan (Georgia Institute of Technology  )*; Ali Agha (Jet Propulsion Laboratory); Evangelos Theodorou (Georgia Institute of Technology),David Fan (Georgia Institute of Technology  )*; Ali Agha (Jet Propulsion Laboratory); Evangelos Theodorou (Georgia Institute of Technology),RSS2020_Author_Agreement.pdf (25864 bytes); Deep_Learning_Tubes_For_Tube_MPC__RSS_camera_(4).pdf (8790886 bytes),Deep_Learning_Tubes_For_Tube_MPC__RSS_camera_(4).pdf,1307.0,87.0,,3.0,Deep Learning Tubes for Tube MPC,[Not Answered],0.06335332768912799,"The paper presents very interesting ideas that combine promising research directions. Nonetheless, the here presented results are not convincing enough (yet), partly hard to follow, written in a rather convoluted way, and overloaded with notation, which changes constantly. 

1) My main criticism is that there is no comparison to other learning based (tube) MPC methods. In particular, what happens if the same amount of data is used for naive estimates of the tube? Is this significantly worse? How much more conservative are the tubes if worst-case error estimates of the dynamics are used and propagated through time. As is, It is hard to assess if this approach improves the current state of the art. 

2) Further, the theoretical analysis is also hinging on some hidden details that should be elaborated in more detail. In particular, the used alpha probabilities are only asymptotically true. When finite data is used, it is not clear how the neural network will behave and thus, there are no finite sample bounds, which makes the presented results void. Right in front of Theorem II.1 is an assumption of i.i.d. data, which is crucial for most convergence results. For dynamical systems, this naturally is not the case and data is highly correlated. 

3) I would also recommend to streamline the structure and remove any unnecessary notation. For example, the paper starts with a standard dynamical system in (1), then the notation of the disturbance changes because a model is used; however, the notation for the state stays the same in (2). In (5), there is now completely new notation for the same object, which is afterward substituted in the proofs anyways. Finally, in (16) and (17), this changes again in addition to having a different notation for each parametrization. 

4) I do not understand why z is introduced in Sec. 2-A as a potentially lower dimensional object. This should create in my opinion many theoretical problems, however this assumption is dropped later anyways.


Minor comments:
- no reference to Fig. 1;
- Figures should be at top or bottom of the page; 
- I think something is off with the color scheme in Fig. 2. Not clear what gray is and further, green and purple are interchanged from top to bottom although the same methods are presented for different systems.
- Eq. (9): why not use the divergence operator: ""div"" instead of defining something new that looks a gradient.
- Many of the ""there are no structural knowledge assumptions"" are hidden in Sec. 2 D in my opinion. This should be double checked with the claims in the introduction;
- The numerical example in Page 4 is in an odd place and should be moved to the experimental section;
- The part after Eq. (22) and (23) is very vaguely written and should be reworded;
- Spacing around equations and figures is very tight and non-standard, which makes reading the paper unpleasant;
- language.",Agreement accepted,"
Overall, I thought this paper was strong. The proposed setting is interesting, and the approach of learning quantiles for probabilistic tube MPC is a good approach. However, there are a few issues with the paper that, if addressed, would result in a substantially stronger paper. 

First, z is left vague throughout the paper. Concrete examples of what this quantity is when it is introduced would increase reader understanding. 

Learning of generic quantiles is discussed briefly but never expanded upon, or included in experiments. The authors should investigate this case! The authors should also discuss how their monotonicity criterion may be extended to this general quantile model. This model has applications in risk allocation for chance constrained programming, and thus is useful. 

There are a few disconnects between the goals of the paper and the stated theoretical results. 
- The authors prove probabilistic results for the case in which the quantile loss is minimized (in expectation). These results are almost certainly not satisfied, and this is only addressed briefly in the experimental results for the triple integrator system. The investigation of this point should be expanded. 
- Theorem 3.1 and 3.2 provide probabilistic bounds for one step (assuming theorem 2.1 holds, which should be stated more clearly). However, this is never connected to change constraint satisfaction, which could be used for probabilistic safety verification. 

I understand that in this field (deep learning-based safe control), it is difficult (or currently impossible) to design a framework that exactly satisfies all the desires safety criteria. However, the authors should aim to expand their current theoretical results as much as possible, and be more forthcoming with the limitations of their approach. 

The most substantial flaw of the paper are the experimental results. As mentioned above, the validity of the theoretical results/investigations of the fundamental claims/ablations are only performed for the triple integrator system, and these diagnostic experiments should be expanded to more systems and more depth. Moreover, the only nontrivial system investigated in a quadrotor with a stabilizing controller. It is unclear if the dynamics of this system are substantially different from a linear system. 

This latter point highlights a second important shortcoming of the experimental section: there are no baseline models implemented. The authors should include baselines, even if they are simple ablated models. This would provide evidence that the proposed components are providing value. 

Overall, this paper is interesting, but would be strengthened substantially by addressing the comments above. 

Typos:
- In reference 29, Jonathan How's name is written as Howl. Upon looking on google scholar, is appears that it is incorrect there as well. Make sure to check/clean your references!
- In page 2, column 2, paragraph 1, it should be referred to as the ""reinforcement learning community"", not the ""reinforcement community""
- the word ""the"" is repeated twice before ""projection"" in the second last paragraph of page 2, and again before ""loss"" just below eq. 7
- It appears that the curve corresponding to the star markers in figure 10 is not defined in the caption?
",Agreement accepted,"While the paper is in general well presented, some of the figures are not very clear or adequately explained. For example, in Fig. 1 the tube does not contain the tracking trajectory and this fact is not acknowledged. This also occurs in Figs. 8 and 11, which on the other hand provide an explanation for this behavior: the nominal tracking trajectory violates the constraints? What other baselines have you compared the approach to? Others exist, such as [29] with similar looking plots to Fig 11 - how does this approach compare? Fig 11 suggests that the approach works, but how is it better than the state of the art in robust MPC?

The clarity of Fig. 2 would be improved if a different color was chosen for the histogram (blue is already used for the 3-sigma bounds using GP moment matching) and on a minor note the green line is dashed, not dotted. Finally, Fig. 10 should present blue circles instead of blue stars for consistency. Additionally, the meaning of the grey lines shown is not explained.

Furthermore, the paper assumes well-calibrated epistemic uncertainty without adequately delving into its consequences: the authors should explain how would poor choosing of beta and W (which is done by hand) impact their methods. Regarding the results, having hardware experiments instead of just simulations would make the paper stronger, since the direct applicability of the methods presented is not guaranteed. In addition, it would be interesting to see the results of Alg. 1 and 3 for the quadrotor simulation and compare them with the results obtained with Alg. 2.

Are the tube dynamics learned using open or closed-loop data? I assume it is closed loop, but then aren't the learned tube dynamics also possibly a ftn of the controller used to get that data?

Minor points:
- what ""function"" are you referring to just prior to (3)?

Typos include:
- recieved on page 1
-  ""the the"" on page 2 and page 3, 
- ""it is propagated in forward in time"" in page 4, 
- ""R^{1}8"" in page 8, 
- ""more a more"" in page 8 
- numerous typos in the citations 
- the second equation in (7) it should be L^alpha(y,r), 
- the safe set C in (13g) is only introduced in the next page, in Alg 1.
- the authors should review the rules on the use of ""which"" vs ""that""
",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,[Not Answered],[Not Answered],,,87.0,jD73lPkrUrg
"In this paper, the issue of model uncertainty in safety-critical control is addressed with a data-driven approach. For this purpose, we utilize the structure of an input-ouput linearization controller based on a nominal model along with a Control Barrier Function and Control Lyapunov Function based Quadratic Program (CBF-CLF-QP). Specifically, we propose a novel reinforcement learning framework which learns the model uncertainty present in the CBF and CLF constraints, as well as other control-affine dynamic constraints in the quadratic program. The trained policy is combined with the nominal model-based CBF-CLF-QP, resulting in the Reinforcement Learning-based CBF-CLF-QP (RL-CBF-CLF-QP), which addresses the problem of model uncertainty in the safety constraints. The performance of the proposed method is validated by testing it on an underactuated nonlinear bipedal robot walking on randomly spaced stepping stones with one step preview, obtaining stable and safe walking under model uncertainty.",RSS2020_Author_Agreement.pdf,Jason Choi (UC Berkeley); Fernando Castañeda (UC Berkeley); Claire Tomlin (UC Berkeley); Koushil Sreenath (Berkeley)*,Jason Choi (UC Berkeley); Fernando Castañeda (UC Berkeley); Claire Tomlin (UC Berkeley); Koushil Sreenath (Berkeley)*,Final Submission RSS 2020.pdf (795875 bytes); RSS2020_Author_Agreement.pdf (448083 bytes),Final Submission RSS 2020.pdf,1334.0,88.0,,3.0,"Reinforcement Learning for Safety-Critical Control under Model Uncertainty, using Control Lyapunov Functions and Control Barrier Functions",[Not Answered],0.36849624936165204,"The presentation is clear and the reader can follow the main ideas. Even though the paper does not present any theoretical guarantees on feasibility (rightfully noted in the discussion), the articulation of the derivation of the framework and the  application to bipedal robots are both convincing. Bibliographical entries should be given a general check for consistency and typos.

",Agreement accepted,"This is an important contribution to RSS, and addresses some of the pressing problems in the real-world implementation of QP based control laws. In addition, given that there are sim-to-real problems with the direct deployment of DNN based control laws, this manuscript provides a fusion of QP and RL based methods for walking. In other words, this method not only learns the uncertainties in the system, but also provides safety guarantees that are very critical for proper deployment of control in hardware. However, there are some concerns in the manuscript that require a thorough analysis. They are listed below:

1) It is important to note that measuring accelerations are hard, since they are very noisy. There is no discussion on how this can be addressed in hardware. In addition, the losses used for training are dependent on the acceleration terms. I feel that this will be detrimental to the implementation of these types of QPs in hardware.

2) This has been a common problem with all the CLF-QP implementations that I have seen: why feedback linearization based QPs? There are no alternative CLFs? Why so much reliance on IO linearization? In other words, why should the RL framework learn the uncertainties in a IO control law?

3) I could be wrong, but I have not seen an experimental implementation of the exponential type of CBFs that were described in the manuscript (for walking). What are the possible reasons? And how can this limit the implementation of methods proposed in this manuscript?

4) There was no detailed discussion on the feasibility, although there was a short sentence in VIII. Why is the proposed CLF-CBF-QP not feasible?

5) What happens if the model is scaled down (to 0.5) instead of scaling up. Also what is the affect of joint friction, damping, introduction of motor losses, bending of links etc. These are some of the problems associated with the experimental implementation. 

6) DDPG was used for learning the uncertainty. Please comment on other techniques like PPO, SAC etc. 

This direction is promising, and can serve as the first steps towards bridging the gap between model based methods and learning. Given that this is a simulation result, I feel that a detailed analysis of the practicality of this method is required. There should be a discussion on what are the potential drawbacks, and provide some resolutions.
",Agreement accepted,"Per the reviewer's knowledge, the paper presents an original work. It is well written and easy to follow. The proposed approach has the potential to significantly impact on addressing model uncertainty in robotics and autonomous systems. 

Here are some detailed comments:
- In (27) and (28): the decoupling matrix in u_{\theta} is defined in terms of \tilde{f} and \tilde{g}. Why they become identity when applying it to dynamics in (2)? Also, should \alpha and \beta be switched in (28), according to the definition in (27)?
- The authors claim that that \dot{V_{\epsilon}} and B^{(r_b)} are computed using numerical differentiation. I wonder how much noise such numerical differentiation introduces to the computation, especially for high relative degree CBFs. My concern is that the process may introduce noisy control inputs that destabilize the system.
- What is the simulation environment for training and validation? How do you model the groud contacts? The simulation results show that the friction cone constraints are violated in some cases, but the robot continues to walk. In reality, the violation could cause a failure in walking. 
- While the RL-CLF-QP method has better performance in satisfying the friction cone constraints, it exhibits larger tracking errors than the standard L1 method. It would be nice if the authors provide a plausible explanation of such phenomena. ",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,[Not Answered],[Not Answered],,,88.0,434qTQPhpn0
"This paper presents fast non-sampling based methods to assess the risk of trajectories for autonomous vehicles when probabilistic predictions of other agents’ futures are generated by deep neural networks (DNNs). The presented methods address a wide range of representations for uncertain predictions including both Gaussian and non-Gaussian mixture models for predictions of both agent positions and controls. We show that the problem of risk assessment when Gaussian mixture models (GMMs) of agent positions are learned can be solved rapidly to arbitrary levels of accuracy with existing numerical methods. To address the problem of risk assessment for non-Gaussian mixture models of agent position, we propose finding upper bounds on risk using Chebyshev’s Inequality and sums-of-squares (SOS) programming; they are both of interest as the former is much faster while the latter can be arbitrarily tight. These approaches only require statistical moments of agent positions to determine upper bounds on risk. To perform risk assessment when models are learned for agent controls as opposed to positions, we develop TreeRing, an algorithm analogous to tree search over the ring of polynomials that can be used to exactly propagate moments of control distributions into position distributions through nonlinear dynamics. The presented methods are demonstrated on realistic predictions from DNNs trained on the Argoverse and CARLA datasets and are shown to be effective for rapidly assessing the probability of low probability events.",RSS2020_Author_Agreement (1).pdf,Allen Wang (MIT)*; Xin Huang (MIT); Ashkan Jasour (MIT); Brian Williams (Massachusetts Institute of Technology),Allen Wang (MIT)*; Xin Huang (MIT); Ashkan Jasour (MIT); Brian Williams (Massachusetts Institute of Technology),RSS2020_Author_Agreement (1).pdf (168364 bytes); RSS2020.pdf (997807 bytes),RSS2020.pdf,108.0,89.0,,3.0,Fast Risk Assessment for Autonomous Vehicles Using Learned Models of Agent Futures,[Not Answered],0.517952313562731,"The core contribution of this paper is a method for rapidly making predictions for whether or not a particular self-driving car (SDC) trajectory will collide with another vehicle or a pedestrian, under a Gaussian or non-Gaussian assumption. Second, they develop TreeRing, a tree-search-like algorithm for computing probabilities of rate events, so that they can use non-Gaussian models of probability. Finally, they apply to deep neural network models trained on the Argoverse and CARLA datasets.

I think these are strong contributions that could be useful in a variety of real-world robot applications in the future. Estimates of these sorts of rare-event probabilities seem extremely important in any situation where robots will coexist with humans.

-------------------

This paper summarizes methods for fast estimation of collision probabilities from either Gaussian or non-Gaussian models. A collision is considered to be any time when another agent enters an ellipse around the vehicle center.

I think the paper is largely well-written, but I had a few notable issues. In particular, Sec. IV is on risk assessment, and dives into the moment-based SoS methods that this approach is based on. The primary goal is to discuss how these moments are computed, in (A) for new reference frames. Throughout this section I was a bit confused about *what* exactly these moments were, or how we compute them from our observations of other actors in the scene. IV.B seems to consist of a relatively straightforward equation for agent risk, and then a lit of references we could use to solve this expression. 

Fig. 1 is a nice illustration of the sorts of multi-modal predictions we want to analyze, but is a bit hard to read. Why isn't the ego vehicle trajectory directional? It's hard to understand how we expect time to be flowing or what we expect to happen. Why use red squares for agent observations?

I would have apreciated the examples from Sec. V being applied earlier and carried through the paper, just to give me something more concrete to follow. I also think it's hard to keep track of the full list of assumptions the authors are making:
- elliptical collision detection region
- characteristic functions of controls
- moments up to some order

The experiments support the main thesis of the paper, but I had some reservations. They compare two methods with Monte Carlo simulations and show that they are able to compute error in less time, and with lower maximum relative error. Authors should include error bars on these means, since they're dealing with 170 scenarios. I'm also suspicious of the fact that the authors used Imhof -- one of their proposed methods -- as ground truth. Of course this method had zero error. Why can't they use observed trajectories from the dataset? I also found the TreeRing results hard to interpret.

All in all, I think this paper has a lot of interesting ideas but its clarity and experiments could be improved.

Side note, please compile the LaTeX in your supplement. It's very hard to read, and I'm not sure what I'm supposed to be getting out of it. You mention the appendix but not the supplement in the text.

Minor:
pg. 3: ""drivers high level"" --> ""driver's high-level""
pg. 3: ""across a $n$ node trajectory"" --> ""... an $n$ node...""
pg. 7: ""outcomes becomes is"" --> ""outcomes are""",Agreement accepted,"The authors present an approach to produce a fast estimation of the risk for trajectories of autonomous vehicles in violating some environmental constraint. 

The approach relies on the reduction of GMMs problems to the problem of approximating the cumulative distribution function of a quadratic form in a multivariate Gaussian. In fact, as there aren't exact solutions for getting the CDF of QFMVG, approximation methods are used.

The paper is sound, and the motivation about providing fast and accurate algorithms for real-time applications is a central topic of research in embedded systems. To the best of my knowledge, the described approach has certain elements of novelty, in particular the usage of the two moment-based approaches (one based on Chebyshev's Inequality and one on a sum-of-squares program) to evaluate the upper bound on risk. For Gaussian mixture models, even if used in other applications like physical climate models for instance, the efficiency of the solution using numerical approximations of the cumulative distribution functions is supported by experimental results.

The experimental part is quite clear, and illustrated well the potential of the approach. Wrt section VI.A, I would have liked to see some comments on the difference of performances of the methods depending on the scenarios. I wonder if there are some scenarios that performed better than others, and why. Sec. VI.A/B. Here, a simple DNN architecture has been tested (which is fine).  When analysing uncertainty estimation, and in particular in the problem of calibration of DNNs, it is possible for the architecture to have an effect on the results. Are the results here independent of the architecture? Then, why (in sec. IV.B.3) the optimizer encounters numerical issues?

I found the paper well written, and the approach well motivated. The assumption that generally agents plan in a continuous space, rather than in a discrete (grid) space is both true and realizable with difficulties from the planning point of view. In any case, the empirical results are convincing. Wrt future work, is considering the issue of calibration for DNNs relevant, when considering the uncertainty on risk assessment as a whole?",,"In this paper the authors present methods for fast assessment of collision risk between autonomous vehicles and other agents, which is a critical for ensuring safety. The authors consider the case where learned probabilistic estimates of the positions or control inputs (along with a dynamics model) for the agents are available. They provide practical methods for risk assessment when the position estimates are parameterized as Gaussian Mixture Models as well as the general Non-Gaussian case. For predicted control inputs, they provide a novel algorithm TreeRing to automatically generate the expressions for moment propagation through a dynamics model. 

Overall, the paper is well-written and derivations are easy to follow. However, in some instances there is a lack of exposition (such as in ""The Non-Gaussian Case"" in Section IV-A where it is unclear how Q exactly factors in). In Section III, the authors make an independence assumption across time stating a similar trend in other behavior prediction literature. This seems like an important assumption for their theoretical derivations however, they do not provide a citation or discuss why it is a reasonable idea. 

The experimental analysis supports the major arguments of the paper. However, crucial descriptions of the experimental setup seem to be missing such as the details of data generated from Argoverse and CARLA datasets, the training parameters for DNN predictors and the exact specifics of generated scenarios for risk assessment. It is also not clear how the errors in the predictions of the DNN affect the quality of risk assessment. Incorporating these in the revision will help make the experimental section more convincing.

Grammatical errors:
Page 7: Model Description under VI-B: ""output becomes is instead a set of"" is grammatically incorrect.
",,,,Agreement accepted,07/16 15:00,07/16 17:00,[Not Answered],[Not Answered],,,89.0,bMbsc6vJPoQ
"Creating accurate spatial representations that take into account uncertainty is critical for autonomous robots to safely navigate in unstructured environments. Although recent LIDAR based mapping techniques can produce robust occupancy maps, learning the parameters of such models demand considerable computational time, discouraging them from being used in real-time and large-scale applications such as autonomous driving. Recognizing the fact that real-world structures exhibit similar geometric features across a variety of urban environments, in this paper, we argue that it is redundant to learn all geometry dependent parameters from scratch. Instead, we propose a theoretical framework building upon the theory of optimal transport to adapt model parameters to account for changes in the environment, significantly amortizing the training cost. Further, with the use of high-fidelity driving simulators and real-world datasets, we demonstrate how parameters of 2D and 3D occupancy maps can be automatically adapted to accord with local spatial changes. We validate various domain adaptation paradigms through a series of experiments, ranging from inter-domain feature transfer to simulation-to-real-world feature transfer. Experiments verified the possibility of estimating parameters with a negligible computational and memory cost, enabling large-scale probabilistic mapping in urban environments.",RSS2020_Author_Agreement_Signed.pdf,"[Anthony Tompkins](https://github.com/MushroomHunting), [Ransalu Senanayake](http://www.ransalu.com/), [Fabio Ramos](https://www.sydney.edu.au/engineering/about/our-people/academic-staff/fabio-ramos.html)","Anthony Tompkins (The University of Sydney)*; Ransalu Senanayake (Stanford University); Fabio Ramos (NVIDIA, The University of Sydney)",RSS2020_Author_Agreement_Signed.pdf (54559 bytes); RSS_2020_HOTPOT_Camera_Ready.pdf (8279594 bytes),RSS_2020_HOTPOT_Camera_Ready.pdf,51.0,90.0,,3.0,Online Domain Adaptation for Occupancy Mapping,https://github.com/MushroomHunting/RSS2020-online-domain-adaptation-pot,0.9721396551268491,"I believe this paper to be very interesting and the general ideas worthy of publication. The problem setting promises to be useful for many applications, and the computational burden (which is addressed in this work) in this class of approaches is currently one of the main limiting factors hindering widespread adoption. My main worries with the current state of the work are its understandability and its evaluations. 

As for the understandability, without knowing prior work in that space, the present paper is relatively hard to parse. Although Sec. II-a considerably helps, knowing [13] or one of the Hilbert Maps papers seems still to be required. Furthermore, I feel that the work should state more clearly / early that it casts the mapping tasks in new environments as a domain adaptation task.

Second, the evaluation considers mostly other approaches of similar type. However, as the authors' stated goal of this work is overcoming the limitations for real-world use, it would be interesting to see how the proposed approach performs in comparison with lidar mapping techniques that are currently used in practice. This also requires new metrics allowing for comparison of the 
different map types.


Minor Remarks: 
- I do not understand while inter-domain and intra-domain adaptation are considered different contributions. In that context, I also wonder if it is worth using a broad set of atoms in practice that is applicable to different cities.
- on p.2, the authors claim that the parameters of the model are typically learned through a complicated log-likelihood loss. In what sense is that loss complicated?
- I think Algorithm 1 would be easier to understand if all the operations that are currently described as text (e.g., ""Transfer the source parameters..."") would be rephrased as methods and each method would be briefly described in the text. 
- Related to the previous point: how are the minimization processes in eq. (6) and eq. (7) carried out in practice? How is the prediction in Fig. 7 carried out in practice?
- For the evaluation, it would be interesting to see how the performance of the algorithm scales with the number of atoms. In the spirit of Meta-Learning, can one hand-craft a set of particularly representative atoms? 
- Also, it would be interesting to know if and how the uncertainty depends on the lidar type. Admittedly, I can spontaneously not think of suitable datasets to evaluate this.",Agreement accepted,"Originality:
POT and its refined style have been implemented in this paper. However, not much discussion has been made to explain how to derive solutions to Eqs. (6)-(7).

Quality:
This paper lacks sufficient details to explain the proposed algorithms, e.g. POT. The evaluation is limited and lacks insightful discussion on why the POT performs better than the others. Fig. 7 is NOT given in the paper but it plays a key role in the proposed algorithm. The entire paper contains significant linguistic errors that makes the paper less convincing. 

Clarity:
It seems that Eqs. (6)-(7) form the basics of the newly proposed algorithm. These equations are quite standard and not sure how these equations lead to a new algorithm.  In addition, in the evaluation, it is not clear in which case the proposed algorithm performs better than the others. The authors mentions efficiency of the algorithm but this is not validated in the current form.

Significance:
Due to the lack of clarity and novelty, this paper and the proposed algorithm are not significant. More comparisons in the experiments are required. ",,"The paper builds on existing work (Automorphing Bayesian Hilbert Maps, or ABHM) to generate occupancy maps. As the computational complexity of ABHM prohibits scaling to real-time scenarios, the paper proposes optimal parameter transport. The transfer parameter, or coupling matrix can be computed close to real-time, which is a significant improvement to computing the Bayesian model parameters. Overall, the proposed approach is a clever combination of existing techniques. The paper describes the problem clearly and motivates the use of optimal parameter transport well. 

The technical quality of the paper needs some improvements. While the motivation behind the parameter transport is clear, it is questionable that the proposed approach provides good results, or not. Computing the NLL may give us some hint about it, but I'm not sure how well does the parameter transport works in actual real-life scenarios. For example, Fig. 3 explains the optimal solution to ABHM, but when looking at Fig. 9 or 11 it is difficult to assess the quality of the POT solution. Also, the actual explicit/implicit adaptation of the model parameters is not detailed and it is questionable if the optimization of the coupling matrix provides always a reasonable result. What happens if my source dictionary does not have the geometrical features in as my target sample has? Overall, the algorithm does not seem to provide guarantees that the resulting occupancy map is indeed close to optimal.

The clarity of the paper is good, it is well written. I appreciate the abundant illustrations / figures that explain the idea behind the technically not so clear details. It also helps to understand the motivation behind the work better. Minor comment: don't use ""on the other hand"" without ""on one hand ..."".

The significance of the work is difficult to assess due to the vague technical details. While the computational speed up is indeed an impactful result, the quality of the resulting occupancy map is difficult to assess, as there are no optimality guarantees (as opposed to the base algorithm, ABHM). It is also unclear how this technique can be adapted to other problems, such as the grasping task mentioned in the discussion.",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,https://github.com/MushroomHunting/RSS2020-online-domain-adaptation-pot,https://youtu.be/qLv0mM9Le8E,,,90.0,qAW3oofiRUE
"Dynamic games are an effective paradigm for dealing with the control of multiple interacting actors. This paper introduces ALGAMES (Augmented Lagrangian GAME-theoretic Solver), a solver that handles trajectory optimization problems with multiple actors and general nonlinear state and input constraints. Its novelty resides in satisfying the first order optimality conditions with a quasi-Newton root-finding algorithm and rigorously enforcing constraints using an augmented Lagrangian formulation. We evaluate our solver in the context of autonomous driving on scenarios with a strong level of interactions between the vehicles. We assess the robustness of the solver using Monte Carlo simulations. It is able to reliably solve complex problems like ramp merging with three vehicles three times faster than a state-of-the-art DDP-based approach. A model predictive control (MPC) implementation of the algorithm, running at more than 60Hz, demonstrates ALGAMES' ability to mitigate the ""frozen robot"" problem on complex autonomous driving scenarios like merging onto a crowded highway.",RSS2020_Author_Agreement_Signed.pdf,"[Simon Le Cleac'h](https://simon-lc.github.io/), [Mac Schwager](https://web.stanford.edu/~schwager/), [Zachary Manchester](https://engineering.stanford.edu/people/zachary-manchester)","Simon Le Cleac'h (Stanford University)*; Mac Schwager (Stanford, USA); Zachary Manchester (Stanford)",RSS2020_Author_Agreement_Signed.pdf (39999 bytes); LeCleach2020.pdf (816665 bytes),LeCleach2020.pdf,120.0,91.0,,3.0,ALGAMES: A Fast Solver for Constrained Dynamic Games,https://rexlab.stanford.edu/projects/ALGAMES.html,0.41344942418991,"Clear introduction, including motivation and contribution statement. See my comments to the contribution statement in the answer in the first box. The remainder of the paper is also clear and well structured.

The related works section provides a good overview of the state of the art, the different methodologies and how they compare to the method proposed by the authors. The following paper addressed similar scenarios to the ones proposed in this paper and might also be of interest to the authors:
Social behavior for autonomous vehicles, by Wilko Schwarting, Alyssa Pierson, Javier Alonso-Mora, Sertac Karaman, Daniela Rus. Proceedings of the National Academy of Sciences, Dec 2019, 116 (50).

Mostly clear problem formulation except for the constraints C in Eq (3). Here a forward reference to where they will be described (and a short intuitive description) might be enough.
It is also worth to clarify that here both the X and U of all players are computed in the optimization, even if in the arg min only X and U^v are present. 
Finally, clarify why it is not required to have the cost function of all players in the optimization to solve a joint minimum.

Method:
In Eq 4 justify why a penalty term is added for C but not for D.
In G^v the derivatives of D are present, yet in G the constraints D are directly stacked. Why are they treated differently?
Function “IncreasingSchedules” is undefined.
The discussion section is useful and fair.

In the experimental setup, why do you choose unicycle kinematic model? Does the method not work well for bicycle model, which is more realistic for autonomous cars?
In Fig.5 how is the maximum constraint violation defined? Are higher values (closer to 0) better or worse? Is your solver faster but provides less safe solutions?

Sec. VII is a nice addition to adapt the plan. The current version is a proof of concept and should be treated as such since the authors show qualitative results of a single run. How does the proposed framework compare to standard MPC with constant velocity assumption? How is its performance over multiple runs? Where does it “break”?

Nice video illustrating the approach. Suggestions and questions:
-	For the left turn scenario: how does the nash equilibrium strategy perform? You could compare both.
-	For the merging scenario. Why the car does not just merge behind the red one in some scenarios where it is more efficient?
-	Pedestrian scenario: the avoidance and adaptation would also be achieved with a standard MPC. What is the difference in performance with respect to the proposed approach?
Minor comments:
The notation m^v is confusing m_v might be clearer to not confuse the notation with exp(m,v).",Agreement accepted,"Pros: This paper is well written: The game theoretic formulation of the autonomous driving scenario is well motivated. A comprehensive review of the related works are provided in the second section, which identifies the limitations of the previous works. The discussion about the limitation of the proposed algorithm in Section IV is enlightening.  The numerical results and comparisons are convincing.

Cons: This article could be improved in the following ways:

(1) A strong assumption of this work is the access to an accurate estimate of other agents' objective function. The evaluation uses a simple quadratic objective function, and the only interaction between agents are through the no-collision constraint. It seems questionable whether this proposed algorithm can handle the real-world situations in which the objective function of agents could be more complicated functions with significant modeling uncertainty. 

(2) The first stated contribution that ""this work proposed a general solver for dynamic games aimed at identifying Generalized Nash Equilibrium strategies"" seems to be a bit of an overclaim. As the proposed approach heavily relies on gradient-based optimization approach, it does not handle games of non-differentiable dynamics and objectives.

(3) The second stated contribution that ""A real time MPC implementation of the solver able to handle noise, disturbances, and collision constraints"" is very incremental. Also, the achievement of safety without an accurate objective function seems to be due to MPC, not due to the proposed solver. ",Agreement accepted,"The paper has developed a very nice Augumented Lagrangian GAME-theoretic Solver (ALGAMES), which is able to solve dynamic games with multiple players and nonlinear state/input constraints, satisfy the first-order optimality conditions with a quasi-Newton root-finding algorithm, enforce constraints with Lagrangian formulation. Robustness against noises/disturbance of the proposed algorithm has also been demonstrated by a MPC implementation in autonomous driving. Faster convergence of the proposed algorithm over existing methods such as iLQGames has also been provided by implementation.",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,https://github.com/RoboticExplorationLab/ALGAMES.jl,https://rexlab.stanford.edu/projects/ALGAMES.html,,,91.0,hM41Qh9wOEA
"There is increasing demand for automated systems that can fabricate 3D structures. Robotic spatial extrusion has become an attractive alternative to traditional layer-based 3D printing due to a manipulator's flexibility to print large, directionally-dependent structures. However, existing extrusion planning algorithms require a substantial amount of human input, do not scale to large instances, and lack theoretical guarantees. In this work, we present a rigorous formalization of robotic spatial extrusion planning and provide several efficient and probabilistically complete planning algorithms. The key planning challenge is, throughout the printing process, satisfying both stiffness constraints that limit the deformation of the structure and geometric constraints that ensure the robot does not collide with the structure. We show that, although these constraints often conflict with each other, a greedy backward state-space search guided by a stiffness-aware heuristic is able to successfully balance both constraints. We empirically compare our methods on a benchmark of over 40 simulated extrusion problems. Finally, we apply our approach to 3 real-world extrusion problems.",Author Agreement.pdf,"[Caelan Garrett ](http://web.mit.edu/caelan/www/),  [Yijiang Huang ](http://web.mit.edu/yijiangh/www/),  [Tomás Lozano-Pérez ](https://people.csail.mit.edu/tlp/),  [Caitlin Mueller ](http://www.caitlinmueller.com/)",Caelan Garrett (MIT)*; Yijiang Huang (MIT Department of Architecture); Tomas Lozano-Perez (MIT); Caitlin Mueller (MIT Department of Architecture),Extrusion_RSS_2020.pdf (5049765 bytes); Author Agreement.pdf (515079 bytes),Extrusion_RSS_2020.pdf,27.0,92.0,,3.0,Scalable and Probabilistically Complete Planning for Robotic Spatial Extrusion,,0.11944313810946401,"This paper addresses a problem of growing interest in 3D printing and proposes a comprehensive planning framework for the spatial extrusion of models with robotic manipulators. The paper is well written, exposes the challenges in the application domain, and proposes an effective method to address them. Good results are presented. 

One interesting aspect of the work is that it addresses the underlying motion planning problem while taking into account the stiffness of the model being built, such that the structure is always stable and undergoes no deformation while being built. The treatment of avoiding “dead-end” states is clearly needed and the proposed method successfully avoids such states leading to the successful completion of multiple prints. The submitted video demonstrates the robotic arm printing real physical models correctly and efficiently.

One comment for improvement of Figure 1-left is that the shown model should be clearly indicated to be a real physical one given that the image gives the impression to have a virtual model displayed on top of the image of the physical robot. 

The work includes a theoretical analysis of the proposed algorithm with details included in the submitted supplementary material. Since the overall method involves several components and the paper proposes probabilistic complete algorithms, it would be useful to state since the beginning in the introduction expected real performance with given real problem instances, for example with respect to Figure 7 which indicates that not all instances tested were solved.

The work has multiple components, and as expected, details are sometimes not commented or cited to follow previous work. For instance, one point that is not commented is if edge subdivision could transform unsolvable cases into solvable ones. Edge refinement is a typical strategy to augment decision resolution in a discretized problem instance. The extrusion of material in edges is governed by equation 4, which seems to enforce that edges in the model can only be straight line segments, however, it should be possible to follow generic curves. The arm trajectory generation depends on PlanConstrained, which is referenced to come from previous work, and it would be useful to have an overview of how the straight segments are achieved. The restriction of the orientation of the end-effector to the mentioned hemisphere, while it makes sense, could perhaps have an impact in preventing a feasible problem to be solved in special cases requiring extreme end-effector maneuvers. It is however understandable that not all details can be explored in the paper. Overall the paper presents a comprehensive solution that successfully addresses the stable spatial extrusion of models.
",Agreement accepted,"Originality: The originality of this paper lies in the traditional planning tools to the new application of framework extrusion problems. In addition to traditional 3D layer printing, the framework printing is a rather under-explored area in robotics. This is interesting and useful in model construction. It would be helpful to have additional references about other assembly/construction works for better supporting the originality of the application by highlighting the differences with the current setup.

Quality: The paper includes the detailed  implementation for two algorithms, i.e., forward progression and backward regression. Both of them benefit from the heuristics expressing stiffness constraints. The analysis of failure cases leads to the fast dead-end check component, which fits geometric constraints. Because of the single goal state feature and framework stiffness structure, the backward regression algorithm works better.

The result section VIII needs some strengthening. The comparison with previous work Choreo is brief and limited to two extrusion problems. Extending the experiments to all 40 extrusion problems in the benchmark would be more persuasive.  A discussion paragraph of performance comparison with Choreo is highly recommended. 

Currently the evaluation is limited to the trajectory solving time and construction success ratio. Does it make sense to also evaluate the quality of final trajectories? E.g.,  in terms of the end effector’s path? i.e.,. trajectory length and printing time compared with Choreo?

Clarity: The paper introduces a lot of notation but it is easy to follow. Overall it is well written and clear.

Significance: The significance of this paper lies in the evaluation on large scale instances of an interesting problem domain. The video, showing the Kuka robot printing the Klein bottle is impressive since the height of the frame structure gives extra complexity in terms of stiffness.
",,"*****************
Specific comments
*****************
(*) III.A - this section is the hardest to read and contains multiple variables that were not introduced. My take home message from this section was ""I have no idea how stiffness can be computed but I believe it can....""

(*) A figure describing the set of orientations opposite to the direction of pn -> pn' would be helpfull

(*) Algorithm 1 - why do we need to sample an endpoint? One endpoint is connected to the already-extruded structure. Shouldn't we always extrude from that point?

(*) I found the discussion at the end of VII.A meaningless since it is only related to the proof that is only provided in the supplumentary material and is not part of the paper.

(*) Fig 6 is hard to understand, the values are in fonts so small that they are useless

*****************
Typos 
*****************
(*) page 2, left column: ""might is a lower-dimensional submanifold"" -> ""might be a lower-dimensional submanifold""

(*) page 2, right column: ""element element"" -> ""element""

(*) Section W, Figure X, Algorithm Y, Definition Z should have a capital S, F, A and D, respectively.

(*) Definition 3 has an extra period.

(*) Page 5 left column: ""moving in a forward direction proves to advantageous for satisfying the stiffness constraint."" -> ""moving in a forward direction proves to be advantageous for satisfying the stiffness constraint.""

(*) Page 7 left column: ""extrusion planning simply requires a identifying a totally-ordered subset"" -> ""extrusion planning simply requires identifying a totally-ordered subset""

(*) Last sentence of first paragraph in section VIII - missing period before ""We"".

*****************
References
*****************
(*) Sometimes hyperlinks are embedded and sometimes they are given as urls.
(*) Sometimes conference names are given with and sometimes without abbreviations (29 vs 45)
(*) Ref 29, {RRT-Connect} -> RRT-Connect
(*) Sometimes ISBN and / or doi values are provided and sometimes they are not.",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,https://github.com/caelan/pb-construction,https://youtu.be/RsBzc7bEdQg,,,92.0,1HRFeTMNsxU
"We introduce a reconfigurable underactuated robot hand able to perform systematic prehensile in-hand manipulations regardless of object size or shape. The hand utilises a two-degree-of-freedom five-bar linkage as the palm of the gripper, with three three-phalanx underactuated fingers---jointly controlled by a single actuator---connected to the mobile revolute joints of the palm. Three actuators are used in the robot hand system, one for controlling the force exerted on objects by the fingers and two for changing the configuration of the palm. This novel layout allows decoupling grasping and manipulation, facilitating the planning and execution of in-hand manipulation operations. The reconfigurable palm provides the hand with large grasping versatility, and allows easy computation of a map between task space and joint space for manipulation based on distance-based linkage kinematics. The motion of objects of different sizes and shapes from one pose to another is then straightforward and systematic, provided the objects are kept grasped. This is guaranteed independently and passively by the underactuated fingers using a custom tendon routing method, which allows no tendon length variation when the relative finger base position changes with palm reconfigurations. We analyse the theoretical grasping workspace and manipulation capability of the hand, present algorithms for computing the manipulation map and in-hand manipulation planning, and evaluate all these experimentally. Numerical and empirical results of several manipulation trajectories with objects of different size and shape clearly demonstrate the viability of the proposed concept.",RSS2020_Author_Agreement.pdf," [Qiujie Lu](https://luqjie.wixsite.com/home),  [Nicholas Baron](https://www.imperial.ac.uk/reds-lab/people/),  [Angus B. Clark](http://www.angus-clark.co.uk/),  [Nicolas Rojas](http://www.imperial.ac.uk/people/n.rojas)",Qiujie  Lu (Imperial College London)*; Nicholas Baron (Imperial College London); Angus Clark (Imperial College London); Nicolas Rojas (Imperial College London),RSS2020_Author_Agreement.pdf (141026 bytes); The RUTH Gripper Systematic Object-Invariant Prehensile In_Hand Manipulation via Reconfigurable Underactuation.pdf (3140426 bytes),The RUTH Gripper Systematic Object-Invariant Prehensile In_Hand Manipulation via Reconfigurable Underactuation.pdf,131.0,93.0,,3.0,The RUTH Gripper: Systematic Object-Invariant Prehensile In-Hand Manipulation via Reconfigurable Underactuation,,0.8341185931187659,"The paper makes multiple relevant contributions. The overall concept is promising. The idea of decoupling the five-bar DOFs from the flexion DOFs by routing the flexor tendons through the linkage is great. The manipulation planning algorithm, based on lookups through pre-computed configuration tables, is also very well suited for the mechanism, and achieves good performance. (I suspect a kd-tree for nearest neighbors in a small 3D data set is overkill on most modern computers, but conceptually a good idea.) This level of in-hand manipulation, with robustness to object shape, is very rarely seen in the literature.

The paper is generally clear and well-written, in both the description of the mechanism and its algorithmic components. The only aspect that I found a bit harder to understand was the spring mechanism that keeps all three fingers oriented such that they all flex towards a common point located between them. The authors use the term “finger direction” - perhaps use “finger orientation” or “finger flexion direction” instead? I did eventually figure it out, with the help of the video.

While the paper is certainly interesting and opens up some very relevant avenues for research, I feel that it only scratches the surface of the complex behavior of these manipulation tasks. Assume that a grasp has been established, and the hand is reconfigured by the base motors. The bases of the fingers are fully constrained to move in a prescribed path, while the tips are partly constrained by the object itself. If these constraints are not mutually compatible (and there is no reason to expect them to be), the hand must adapt, either through finger reconfiguration in the null space of the flexor tendon, or through contact rolling or slipping. The spring mechanism also exerts moments on the fingers, further complicating the force equilibrium of the whole system.

These are all very complex behaviors. I suspect that it is possible that during such a process the grasp could be lost. When does that happen? How can we guard against such behavior? I also suspect that such reconfigurations are the main reason for the experimental discrepancies between desired and achieved final object pose. Finally, some of the experiments in the video show out-of-plane object rotation, which I assume is not desired or accounted for, and likely stemming from the same source.

As presented here, the hand is also primarily usable for planar (2D) manipulation. The fingers are indeed capable of producing enveloping (power) 3D grasps. However, if any of the fingers makes contact on multiple links, it is not clear if manipulation is still possible. It is likely that any finger with multiple contacts is over-constrained if its base moves (due to movement of the five-bar linkage). I suspect that is why all manipulation examples are essentially planar, with fingertip grasps only. Even for planar grasping, the hand has 2 manipulation DOFs to control 3 object DOFs, thus can only manipulate along a 2D manifold. It is not clear if and how such a method would apply to 3D grasping, creating a 6 DOF object movement space. The method is still highly compelling even given these considerations, and I am not asking the authors to solve all of 6DOF manipulation in a single paper - the work shown here represents important steps in that direction. However, I feel like these aspects should be explicitly mentioned and discussed here.

In conclusion, I believe this is a very relevant paper, with strong ideas and good performance in practice on tasks rarely seen. It opens up some very interesting future work directions and questions, so the reader can be left hoping for a more in-depth analysis. Such an analysis could either be included here (taking the paper to a next level) or be carried out in future work.
",Agreement accepted,"Originality
The paper is original, it presents a design that is novel and brings something new to the literature. However, I did find a major drawback that is only addressed at the very end as a small drawback, while I think it's actually a concept thing. The idea of the gripper is explained as the gripper grasps an object, and then this can be moved thanks to the base 5-bar linkage. At the same time, authors say the gripper can reconfigure into different grasp types. This is true, but what is happening is that the gripper reconfigures into different grasps types while it is grasping. Meaning the fingertips change orientation and move along the object surface while the in-hand manipulation is happening. In practice, it is a small motion probably due to high frictions, but it probably means a lot of force is needed to move the 5-bar linkage, and it may be even impossible to do so if the gripper is performing a power grasp with several contacts in each finger. This fact is minimized in the paper, as the only examples of in-hand manipulation that are shown are with objects held in the fingertips. But I was a bit disappointed, as it would have more value to be able to move an object that is actually strongly grasped, which is not doable in other gripper designs where the in-hand manipulation has to necessarily happen with precision grasps. 

In addition, the gripper is not compared to any other hand. I know it is difficult to compare a hand that is so particular. But there are other designs that also perform in-hand manipulation with little actuation. The claim of the paper is that they can do it with only 3 actuators and simple planning and control, but maybe it would be interesting to compare volumes of manipulation workspace for different gripper designs with low actuation and simple control, and in-hand manipulation capabilities. There are so many hands that with original and simplified designs achieve variate and large in-hand manipulations, for instance
Ma, Raymond R., and Aaron M. Dollar. ""An underactuated hand for efficient finger-gaiting-based dexterous manipulation."" 2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014). IEEE, 2014.
or
Amend, John, and Hod Lipson. ""The JamHand: Dexterous manipulation with minimal actuation."" Soft robotics 4.1 (2017): 70-80.

These hands above are just a few, and one could argue that they achieved larger amounts of in-hand manipulation with similar or less actuation. I believe that this hand has a value, but I also missed this kind of discussion in the paper, to understand what this hand brings to the landscape and why we should care about considering yet a new design to the large amounts of existing ones.

Another thing that could be easily compared to other designs is the error in positioning during in-hand manipulation planning and execution, at least compared to the reported ones in the papers such as
Hang, Kaiyu, et al. ""Hand–object configuration estimation using particle filters for dexterous in-hand manipulation."" The International Journal of Robotics Research (2019).

At least, to have an idea if the reported errors in position is something normal for in-hand manipulation.

Quality and clarity
In my opinion, the quality of the paper is good, but it could be improved making the text and figures a bit more clear in the following aspects:
 1- There is not a clear diagram of the 5-bar linkage and the nomenclature used, the name of the joints, the location of the points P, which one is the link 1...to 5, etc... You can figure it out but you need to make your own drawing. It may be fixed adding labels to Fig.2, but remember to also add the points P1...P5 referred in Sec. III - C. Also, this figure needs to be cited at the very beginning of Section II.A.

 2- There is a lot of confusion between what they call ""grasping workspace"", ""manipulation workspace"" and ""manipulation map"", which are basically the same thing. It seems like authors wanted to use different names so that it seemed they were doing more computations, but this just increases readability. There one single workspace in this design, which is composed of a continuous XY slide, and for each point, it corresponds to a different orientation of the object, (X,Y,Phi). So, the grasping workspace is the XY slide, and the manipulation workspace is the whole thing, and the manipulation map the correspondence (X,Y -> phi). This should be simplified for clarity.

 Significance
 The paper adds to the idea that underactuated and simple grasping can still achieve in-hand manipulation capabilities. It is a new design that could lead to variations, using n-bars mechanisms. I think it is a good idea, but it could have more significance if the design is shared in an open-hardware fashion (sharing building instructions and models) and if some of the comparisons I mentioned before are reported.",Agreement accepted,"This is generally a well-written paper. Both the text and the visuals have been carefully prepared and edited. The key innovation seems to be the capability of reconfiguring the arrangement of fingers. This can already be seen with some off-the-shelf grippers such as the Barrett Hand, but the presented gripper features a more sophisticated five-bar, closed-chain, 2-DOF linkage architecture.

However, some key arguments don't seem to be supported well. For example,
(1) It shouldn't be true that the gripper is able to perform grasping and manipulation ""regardless of object size or shape."" 
(2) It is not clear what the advantages are in ""decoupling grasping and manipulation."" In other words, how can ""prehensile in-hand manipulation"" be not dependent on grasping?
(3) The adoption of three fingers and a five-bar linkage needs more justification: why three fingers will produce the best results and why they need to be coordinated in a coupled manner on the parallel mechanism.
(4) Algorithm 1 is for forward kinematics, which is a well-known topic. Its necessity is thus unclear. And it doesn't have to be called an algorithm. It's just a simple iterative procedure.
(5) Algorithm 2 is not justified well, either. It's basically for simple path planning with no complex C-obstacles. There are many other existing algorithms. 

In the evaluation part, the object handoff scenarios presented in the video seem too straightforward. Many other grippers can be controlled to perform equally well. The significance of the results can also be better clarified. For example, if the goal is to translate/rotate the object at hand as in Table I, why not just taking advantage of the mobility of the arm, instead of the gripper? The use of the contrived objects also makes it hard to figure out the real significance of the presented approach.",,,,Agreement accepted,07/16 15:00,07/16 17:00,,https://youtu.be/87yT-pyJQNY,,,93.0,ahCuRTtBgXk
"Robot teams are increasingly being deployed in environments, such as manufacturing facilities and warehouses, to save cost and improve productivity. To efficiently coordinate multi-robot teams, fast, high-quality scheduling algorithms are essential to satisfy the temporal and spatial constraints imposed by dynamic task specification and part and robot availability. Traditional solutions include exact methods, which are intractable for large-scale problems, or application-specific heuristics, which require expert domain knowledge to develop. In this paper, we propose a novel heterogeneous graph attention network model, called ScheduleNet. By introducing robot- and proximity-specific nodes into the simple temporal network encoding temporal constraints, we obtain a heterogeneous graph structure that is nonparametric in the number of tasks, robots and task resources or locations. We show that our model is end-to-end trainable via imitation learning on small-scale problems, generalizing to large, unseen problems. Empirically, our method outperforms the existing state-of-the-art methods in a variety of testing scenarios.",Signed_Agreement.pdf,"[Zheyuan Wang](https://phejohnwang.github.io/), [Matthew Gombolay](https://core-robotics.gatech.edu/people/matthew-gombolay/)",Zheyuan Wang (Georgia Institute of Technology)*; Matthew Gombolay (Georgia Institute of Technology),v2HetGAT_RSS20_final.pdf (2374009 bytes); Signed_Agreement.pdf (164368 bytes),v2HetGAT_RSS20_final.pdf,1254.0,94.0,,3.0,Heterogeneous Graph Attention Networks for Scalable Multi-Robot Scheduling with Temporospatial Constraints,,0.319005038421987,"The paper presents a method for learning Q-values for solving multirobot task allocation and scheduling problems. The scheduling constraints are expressed with task start time, end time and duration. The scheduling problem is formalized as an MDP with a Q function that is learned by an heterogeneous graph attention network.  The graph attention network is built as an extension of STNs. The learning process uses schedules built by experts or produced by exact optimization methods. The optimization function used for most of the experiments is the makespan, but some results are reported for a different function. Since the data are synthetic it is hard to map them to specific robotics applications. The paper mentions that the robots are manipulators working on a large piece. 
The paper is generally clear and well organized.  It would have been useful to mention a few additional things in the paper:  (1) an indication on the number of robots expected. Since the method is centralized the number will likely be limited. (2) some more details on how the method will be used, i.e. offline at the planning stage or online at execution time? what sensor information is available to the robots? ",Agreement accepted,"I think this paper makes a good contribution. it is mostly related to [25], but it extends it in a meaningful way.

The core part of the paper is section IV, and I have to say it is not obvious to follow. In particular, the simplification trick seems essential to reduce the complexity of the model, but is not 100% clear to me. A small working example (not more complex than the one in Fig 2) would help.

In term of generalizing the results, or providing a more general ""take home message"", it would be interesting to have some more insights on how you derived the reduction formulas 10-14. I am not questioning their appropriateness, but it would be interesting to know the process.

The following points deserve attention.

- how do you pick q_0 in Eq.(18)? You state that its value is chosen empirically, but you do not provide guidance on how to pick it, nor state what you eventually used.

- you use Gurobi to exactly solve the MILP problem given at page 2 and apply some cutoff times. How big are those instances (in terms of number of optimization variables and number of constraints)?

-at page 7 you state ""during evaluation on large and ex-large problems, ScheduleNet was able to find higher-quality solutions than Gurobi,"" This needs to be better explained. Gurobi finds the optimal solution, so please put this sentence into context, because stated this way it does not appear to be correct.

- Figure 6 needs to be redone b/c it is very hard to read (had to magnify to 400% to get some clarity)

- I am puzzled by some of the data shown in figure 6. For example, comparing subfigures b and c it seems like that Gurobi is faster in solving a medium problem with 10 robots than solving a medium problem with two or five robots. This does not make much sense to me. In general I found the section about computational times a bit confusing because you start saying that it is not easy to have a fair comparison. Did you at the very least run all tests on the same system? I understand the different implementations etc, but since you mentioned different CPU/GPUs it is fuzzy.",,"The paper builds and extends [25] for heterogeneous GNNs.
While the paper is generally easy to follow, there are some points that remain unclear:
1. The contribution over [25] seems incremental. Some basic understanding of the system is missing from this paper (making it a bit difficult to follow if reading it and assuming it's self-contained, which it should be), and there are many repetitions in the ideas between the papers. 
2. Some technical comments:- ""Stacking FC layers on top of GNNs is computationally expensive and not memory efficient,especially when evaluating a large number of state-action pairs"" How expensive are they? There are many practical solutions today that are pretty efficient, for example LeNet, AlexNet, VGG, GoogLeNet, ResNet.
- Why do you need to learn Q separately? it could have been very interesting to realize this through the network. 
- How are the tasks defined to the network? are they only predefined?
- Why are the neural networks necessary in the system?  it seems like the Q value is computed directly, and you don't actually need the NN to evaluate it...
- A figure describing the network architecture and flow could significantly help in understanding the system.  Providing the WX+b equations is just not informative enough.
- Using softmax without explaining the outputs and the derived probabilities is confusing. Are the robots given the task with highest probability? is it given a task at random using those probability distributions?
- How many samples are sufficient for learning a policy?
- Regarding imitation: is this done only for similar problems? general problems?  how is the reward calculated? there is missing information here",,,,Agreement accepted,07/16 15:00,07/16 17:00,,,,,94.0,W5eXnJDPm_Q
"The multiple-path orienteering problem asks forpaths for a team of robots that maximize the total rewardcollected while satisfying budget constraints on the path length.This problem models many multi-robot routing tasks such asexploring unknown environments and information gathering forenvironmental monitoring. In this paper, we focus on how tomake the robot team robust to failures when operating inadversarial environments. We introduce the Robust Multiple pathOrienteering Problem (RMOP) where we seek worst-caseguarantees against an adversary that is capable of attacking atmost \alpha robots. Our main contribution is a general approximationscheme with bounded approximation guarantee that depends on\alpha and the approximation factor for single robot orienteering.In particular, we show that the algorithm yields a (i) constant factorapproximation when the cost function is modular; (ii)log factor approximation when the cost function is submodular;and (iii) constant-factor approximation when the cost functionis submodular but the robots are allowed to exceed their pathbudgets by a bounded amount. In addition to theoretical analysis,we perform simulation study for an ocean monitoring applicationto demonstrate the efficacy of our approach.",RSS2020_Author_Agreement-signed.pdf,"Guangyao Shi, [Pratap Tokekar](http://tokekar.com/) [Lifeng Zhou](https://lfzhou917.github.io/)",Guangyao Shi (University of Maryland)*; Pratap Tokekar (University of Maryland); Lifeng Zhou (Virginia Tech),Robust_Multiple_path_Orienteering_Problem_final.pdf (783288 bytes); RSS2020_Author_Agreement-signed.pdf (111774 bytes),Robust_Multiple_path_Orienteering_Problem_final.pdf,1270.0,95.0,,3.0,Robust Multiple-Path Orienteering Problem: Securing Against Adversarial Attacks,,0.33806512337564,"The problem addressed is certainly within the domain of a conference like RSS, and the results seem practical enough that one can readily imagine an impact on practitioners.  From an algorithmic perspective, the problem particularly interesting because the underlying single-robot problem is already NP-hard.  The results seem, at a broad level, to be correct, but there are a number of issues with the technical presentation that should be cleaned up (details below).  A series of simulation experiments are described that quite nicely demonstrate the robustness obtained from the approach.

The statement of Problem 1 is somewhat ambiguous about the roles of N and \alpha.  Are they part of the input?  Note in particular that the stated inputs for Algorithm 1 do not match the ""Given"" clause for Problem 1.

Line 10 in Algorithm 1 is unclear, because the ""such that"" portion does not seem to be a proposition.  Perhaps the \forall should be omitted?  Likewise, Line 17 is difficult to interpret, because f is a function on sets -- what does the set containment notation mean there?

The statement ""Each of the \alpha paths in S_1 are better than those in S_2"" is unclear.  Does it mean that each path in S_1 is better than every path in S_2?  Or that some correspondence is maintained between elements of S_1 and elements of S_2?  Is this perhaps the invariant discussed later?

The proof of Proposition 1, in its first line, seems to assume that the intersection will be only a single vertex.  This is, obviously, not sufficient to prove Proposition 1 fully.
",,"This paper introduces RMOP, a variant of MOP (Multiple-path Orienteering Problem) considering robustness. Based on MOP, the problem takes into consideration potential adversarial situations (e.g., due to attack or robot failures). While similar problems (e.g. Matroids Team Surviving Orienteering Problem) assume the potential failures associated with edges in the environment, the paper examines scenarios with failures on alpha paths with most rewards and failures on random alpha paths. The paper points out that RMOP translates to a delicate balance between redundancy and coverage. For RMOP, the paper proposes an approximation scheme. The performance of the algorithm is examined by quantitative results and theoretical proofs on the bound for both modular and submodular reward structures.

The paper is well-written and clear. The RMOP problem is interesting and can be of practical value. The main contribution of the research is the proposed algorithm and the analysis that shows the algorithm yields a constant factor guarantee when the reward structure is modular and a log factor when reward structure is submodular. The guarantees are of theoretical interest, though not surprising, given earlier results mentioned in the paper.

It is not clear how strong is the constant factor guarantee for the RMOP problem; additional characterization seems warranted. That is, as alpha robots are removed from the team, the expected loss of reward is a fraction of the total reward, say 1/3. If the approximation MOP solution has a large constant, say, 3, then the loss due to approximation is much larger than the loss due to adversarial effect. The submodular case, with a log factor, seems worse. There are two ways to address this: (1) refine the guarantee to be 1+epsilon, which may not be very feasible, or (2) compare the result with an exact solver, which seems possible via integer programming for the modular setup. 

The current comparison with SGA seems to indicate that the algorithm has limited add-on value. Using SGA is only marginally worse when alpha is fairly big compared with the total number of robots. When alpha is small (more practical?), SGA seems to be better.

In figure 4, when |SA|=0, the rewards of NG seem to be around 1/6 of SGA rewards while there are only up to 7 robots. Given that the starting robot positions are randomly chosen, this suggests that the environment is rather small such that all robots are likely to go to the same region? How are the rewards distributed in the environment? Is the algorithm sensitive to the distribution of starting robot positions and that of the rewards? It would be appreciated that the paper could mention the reward distribution as well and show a qualitative example for NG as they have done for the other two algorithms. 

Another issue with the empirical evaluation is the limited comparison. That is, the result is only compared with SGA and NG, two incomplete algorithms. It seems not difficult to attempt a one-shot (i.e., without the while loop) greedy algorithm and also an IP based full solution as mentioned above. 

To summarize the comments, the theoretical elements are of some interest but the evaluation is not fully backing things up. The following improvement would validate the contribution of the paper (a stronger theoretical guarantee is desirable but may not be feasible):

1. Explore the possibility of an optimal algorithm. Is one possible via IP, which are frequently used for OP and team OP? It seems this would be possible for the modular case. The method will likely not scale as well, but would be a more meaningful comparison when the number of vertices is small. 

2. Comparison with NG seems not very meaningful given the structure of the problem - I suggest comparing with a non-iterative version of the RMOP algorithm (i.e., go through the while loop once). 

3. Justify the value of RMOP in comparison with SGA. It seems that bigger alpha is not as natural as smaller alpha, where SGA performs well. 

4. Be more specific about the data, e.g., how is the data obtained/generated?  


As a minor comment, MOP seems to be better known as TOP (team orienteering problem).
",,"The authors describe a new variant of the multi-robot orienteering problem that allows for robot failures. An algorithmic solution is presented, and theoretical guarantees are shown. The algorithm is compared in simulation to competing algorithms and shown to outperform them as the number of failures increases.

-It would have been helpful if the authors had provided a bit more motivation for this particular variant of OP. What domains is this relevant for? Can you estimate the chance of failure in these domains? If failures occur rarely, you'd prefer to use SGA because it has superior performance. Thus, the real-world impact of the algorithm is somewhat questionable.

The paper deals with a relevant topic that will likely be of interest to those working on informative path planning and related problems (a growing community). The development of the robust variant is interesting and novel, though it may be a special case of the adaptive submodular path planning problem (see below). The algorithmic contribution is strong, and the theoretical contribution is well presented and impactful. The simulations are sufficient to show the benefits as the number of failures increases. Additional comments are listed below:

-I'm somewhat surprised no citations were compared to Gaurav Sukhatme's work at USC. There have been a number of relevant papers from his lab that deal directly with the robotic orienteering problems, e.g.:
https://ieeexplore.ieee.org/abstract/document/8794103

-Authors should consider Daniel Golovin's work on adaptive submodularity in terms of formulation. Is this a special case of adaptive submodular multi-robot path planning?

-It's a bit unfortunate that the algorithm performs worse than competing algorithms with no or few attacks. This seems to come from the conservativeness of the algorithm which provides the robustness. Would it be possible to modify the algorithm so it takes into account a prior on the number of attacks? Since SGA is a subroutine, the algorithm seemingly could simply revert to it if the attacks are of low enough probability that they are not worth considering.
",,,,,07/16 15:00,07/16 17:00,,,,,95.0,x8QEZnOKxuA
"A framework is presented for handling a potential loss of observability of a dynamical system in a provably safe way. Inspired by the fragility of data-driven perception systems used by autonomous vehicles, we formulate the problem that arises when a sensing modality fails or is found to be untrustworthy during autonomous operation. We cast this problem as a differen- tial game played between the dynamical system being controlled and the external system factor(s) for which observations are lost. The game is a zero-sum Stackelberg game in which the controlled system (leader) is trying to find a trajectory which maximizes a function representing the safety of the system, and the unobserved factor (follower) is trying to minimize the same function. The set of winning initial configurations of this game for the controlled system represents the set of all states in which safety can be maintained with respect to the external factor, even if observability of that factor is lost. This is the set we refer to as the Eyes-Closed Safety Kernel. In practical use, the policy defined by the winning strategy of the controlled system is only needed to be executed whenever observability of the external system is lost or the system deviates from the Eyes-Closed Safety Kernel due to other, non-safety oriented control schemes. We present a means for solving this game offline, such that the resulting winning strategy can be used for computationally efficient, provably-safe, online control when needed. The solution approach presented is based on representing the game using the solutions of two Hamilton-Jacobi partial differential equations. We illustrate the applicability of our framework by working through a realistic example in which an autonomous car must avoid a dynamic obstacle despite potentially losing observability.",RSS2020_Author_Agreement.pdf,Forrest Laine (UC Berkeley)*; Chih-Yuan Chiu (UC Berkeley); Claire Tomlin (UC Berkeley),Forrest Laine (UC Berkeley)*; Chih-Yuan Chiu (UC Berkeley); Claire Tomlin (UC Berkeley),Eyes_closed_safety_kernels-8.pdf (1003235 bytes); RSS2020_Author_Agreement.pdf (237261 bytes),Eyes_closed_safety_kernels-8.pdf,1245.0,96.0,,3.0,Eyes-Closed Safety Kernels: Safety of Autonomous Systems Under Loss of Observability,,0.895270046164332,"This paper proposes a framework that provides guarantees (with respect to the modeling choice) that an autonomous system will remain safe even if it loses observability (i.e., eyes closed) of an agent in its environment. The problem is framed as a zero-sum Stackelberg game in which the autonomous agent is trying to find a policy that maximizes a function representing the safety of the system, and the unobserved agent is trying to minimize that same function (which implies a worst-case outcome). This work leverages existing work on Hamilton-Jacobi (HJ) reachability analysis for safety-critical systems and applies the theory in a new setting where observability of the other agent is lost. This work is motivated by an important problem that may arise in safety-critical situations, and appears to be, to the best of the author's knowledge, the first work to account for such situations in a provably safe way.

In general, this work tackles an interesting problem, and provides strong safety guarantees. However, there are some concerns regarding this work.

While it leverages existing HJ-reachability theory, there are some details presented in this paper that seem a little hand-wavy and may require a deeper investigation and explanation in order to make the ideas more concrete.
The formulation presented is that when observability is lost, the ego agent takes steps to avoid the backward reachable tube which is computed to avoid the forward reachable set of the unobserved obstacle over a time horizon t_f. This provides strong safety guarantees with respect to the model, however, the framework will become overly conservative as t_f increases since the forward reachable set will continue to grow as the time horizon increases. As such, the paper's comment that ""by taking t_f very large in the computation of (26), we can obtain guarantees for arbitrarily long intervals of time"" is theoretically true, but is not practically useful as it will result in an extremely overly-conservative solution. Further, the follow up comment, ""In practice, often the value functions of interest are convergent anyways"" applies to backward reachable sets/tubes, but not forward reachable sets (and see additional comment 3 below). Further, if the forward reachable set over a time horizon t_f is the terminal set for the backward reachable tube computation, why would the backward reachable tube be used since the forward reachable set is relevant for a time horizon t_f only, and not for any time in [t_0, t_f].
The high-level idea of this paper makes a lot of sense (i.e., use backward reachability to avoid the set of states where the unobserved object could be in), however, the details, implications, and practical significance are unclear and require revision to fully flesh out the details and make them more concrete.

The discussion on related work is primarily focused on HJ reachability, however, it misses literature that deals with planning under occlusion which this work is very similar to. The discussion provides a good overview on existing HJ reachability-based methods, and also briefly discusses other stochastic planning works that assume complete observability of the environment. However, there is a lot of unmentioned literature that deals with the problem of (1) predicting where occluded/unobserved agents are, and (2) provide safe planning strategies to account for those occluded/unobserved agents. The problem that motivates this work---when the perception system fails and the robot can no longer observe the other agent in the environment---draws many parallels to problems dealing with occlusion and I suggest the authors consider adding this into the related work discussion. See the references listed below for examples. Understandably, since this work, to the best of the author's knowledge, is the first to develop a planning algorithm assuming observability to be lost at any instance, literature on this specific problem may appear sparse. However, I encourage the authors to re-work the motivating problem a little more because practically speaking, if the perception system of a robot operating in a safety-critical setting fails, it would be uncommon for the system to continue typical operation. Instead, I would imagine there would be a high-level safety protocol that requires the system to come to a stop, or something similar. For instance, there are existing planning algorithms that ensure the that existence of a safe trajectory always exist (e.g., ensure that the vehicle is able to execute a collision-free evasive trajectory at any time [d]) regardless of how the environment evolves. In general, although the problem presented is very interesting and uniquely set up, it, in many ways, is similar to many existing works not mentioned in this paper and I suggest adding additional discussion in the related work and a revision of the motivating problem.

Finally, while an example is provided to demonstrate the reachable sets described in this work, it is one step short in demonstrating how the proposed method addresses the key motivating problem, that is, ensuring safety when a robot loses observability of another agent in the environment. While the visualizations of the reachable sets are instructive, it does not fully demonstrate how these sets/value functions can be used to solve the formulated problem, and to what extent they are practically useful. I suggest providing an experiment (in simulation or on hardware) where a robot, while operating, suddenly loses observability of the other car, and is required to perform the proposed collision avoidance controller. The other agent could be controlled by a human, or follow the worst-case policy, to show that regardless of what the other agent does, the ego-robot remains safe. Further, a baseline/heuristic comparison would help exemplify the strengths of the proposed approach. For instance, using a Kalman filter to estimate the state of the observed agent, and then letting the uncertainty/variance over the estimated position grow as no new observation measurements are received, and using that (growing) uncertainty ellipse to inform the robot planner.

Overall, this paper tackles a very interesting problem and leverages mature existing work in HJ reachability theory to provide strong safety guarantees. However, there are some weaknesses in this work which makes it difficult to fully appreciate the technical and practical impact of this work. Suggestions for improvement are provided, and additional comments and references are listed below.


Additional comments:
- The model used is a simple unicycle model, not a Dubin's car model (see http://planning.cs.uiuc.edu/node660.html). The current model allows the vehicle to turn in place (when speed is zero), which would not be a very realistic model for a car. It is okay for the purposes of this example, but would be important to keep in mind for future versions of this work.

- Perhaps I have misunderstood the theory, but it seems odd that that Equation (27) is a minimization problem. Since we want to increase V_tilde as much as possible, I would have thought that we would have to choose a u_int that maximizes dV_tilde/dt as this would correspond to the system moving towards safer regions.

- There is a sentence that says, ""In practice, often the value functions of interest are convergent anyways, in which case they converge to the infinite-horizon solution. When that is the case, the safety guarantees never expire."" This seems a little hand-wavy. I think what is meant is that if the control authority/actuation of the other agents is at most equal, then the value function will converge because the other agent will never be able to ""catch up"" to the ego-agent in the limiting case, even if one agent has a transient advantage over the other.

- There is another vague and general statement, ""We note that almost all systems of practical consideration are in fact decomposable in the manner outlined, and therefore do not introduce any additional conservatism."" This seems like a bold statement without any references to support this claim. I suggest adding additional references, or examples or criteria on a system that would make this true, or adapt the language of this statement to be less bold and wide-sweeping. It is okay to focus only on systems that are decomposable.

- In general, the sections were very long and thus making the paper was quite difficult to read; I suggest breaking up the sections into smaller sub-sections. Further, the paper seems to half-heartedly re-derive HJ reachability theory. Perhaps a more concise approach would be to formally state the HJ reachability problem, and relevant equations while providing relevant citations, and then show how that formulation is used specifically for this particular problem formulation.

References:

[a] M. Yu, R. Vasudevan and M. Johnsson-Roberson, ""Occlusion-Aware Risk Assessment for Autonomous Driving in Urban Environments,""  IEEE Robotics and Automation Letters, vol. 4, no. 2, pp. 2235-2241, 2019

[b] P. Orzechowski, A. Meyer, M. Lauer, ""Tackling Occlusions & Limited Sensor Range with Set-based Safety Verification"", IEEE International Conference on Intelligent Transportation Systems, 2018

[c] E. Galceran, E. Olson and RR.yan M. Eustice, Augmented vehicle tracking under occlusions for decision-making in autonomous driving. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3559-3565, Hamburg, Germany, September 2015

[d] C. Pek, and M. Althoff, ""Computationally Efficient Fail-safe Trajectory Planning for Self-driving Vehicles Using Convex Optimizaton"", IEEE International Conference on Intelligent Transportation Systems, 2018",,"I have several concerns focused around wording, implementation, and references to prior work:

1. The approach proposed in the paper is implemented using the Level Set Toolbox. Does the approximation generated by the Level Set Toolbox generate a conservative approximation to the true 1-super level set? If the approximations generated in  the paper are conservative, the authors should mention that property. If not, then I am not sure why the the proposed method deals with the potential loss of observability in a provably safe manner. A clarification should be made in the revision to ensure that readers appreciate the significance of the result. 

2. There are others who've proposed to plan while treating potentially observed or only partially observed models using forward and then backwards reachability. Though these methods do not treat the problem as a zero-sum, differential game (as is done in this paper), they are probably worth referencing as motivation for this approach. For instance:

Ahn, Heejin, Karl Berntorp, and Stefano Di Cairano. ""Reachability-based Decision Making for City Driving."" 2018 Annual American Control Conference (ACC). IEEE, 2018

3. Have the authors considered how to apply this to real-world or specifically more realistic vehicle models? Some discussion of that is probably merited given the difficulty of applying some of these reachability based approaches. Again, to appreciate the significance of the approach, such a discussion would be helpful.",Agreement accepted,"The paper is overall well-written and easy to follow.

Minor comments:

1- It seems that you should use ""Lemma"" instead of ""Theorem"" for Theorem1 &2, as they are direct results of already published work.

2- Please give more details and the limitations regarding your assumption: ""the external system can observe the internal system at all times"". In the case of two autonomous cars, it is possible that both systems cannot observe each other.

3- In Section III, as you are only considering the 2D case, the cars are rather disks and not spheres.

4- The analysis in Section III. A. ""Notes on Computation"" is interesting, can you provide more details regarding your statement: ""Nevertheless, with clever representation of the systems, many interesting problems more complicated than the one presented here can be solved using this framework.""
",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,http://github.com/4estlaine/eyes_closed,,,,96.0,mEDus3elZnI
"We present a method for learning to perform multi-stage tasks from demonstrations by learning the logical structure and atomic propositions of a consistent linear temporal logic (LTL) formula. The learner is given successful but potentially suboptimal demonstrations, where the demonstrator is optimizing a cost function while satisfying the LTL formula, and the cost function is uncertain to the learner. Our algorithm uses the Karush-Kuhn-Tucker (KKT) optimality conditions of the demonstrations together with a counterexample-guided falsification strategy to learn the atomic proposition parameters and logical structure of the LTL formula, respectively. We provide theoretical guarantees on the conservativeness of the recovered atomic proposition sets, as well as completeness in the search for finding an LTL formula consistent with the demonstrations. We evaluate our method on high-dimensional nonlinear systems by learning LTL formulas explaining multi-stage tasks on 7-DOF arm and quadrotor systems and show that it outperforms competing methods for learning LTL formulas from positive examples.",RSS2020_Author_Agreement.pdf,"[Glen Chou](http://web.eecs.umich.edu/~gchou/), [Necmiye Ozay](http://web.eecs.umich.edu/~necmiye/), [Dmitry Berenson](http://web.eecs.umich.edu/~dmitryb/)",Glen Chou (University of Michigan)*; Necmiye Ozay (University of Michigan); Dmitry Berenson (U Michigan),rss_cameraready_body_v2.pdf (1700055 bytes); RSS2020_Author_Agreement.pdf (251229 bytes),rss_cameraready_body_v2.pdf,1303.0,97.0,,3.0,Explaining Multi-stage Tasks by Learning Temporal Logic Formulas from Suboptimal Demonstrations,,0.51790217715691,"-    As most papers of temporal logic inference adopt a data-driven approach, this paper also uses the model of the system in solving the inference problem. This may raise concerns as to how the proposed approach may apply when the model of the system is unknown, which is a more common scenario for such problems. Besides, one may ask if the model of the system is already known, what is the purpose of inferring the temporal logic specifications.Wouldn’t solving the optimization problem for computing the control signals based on the known model and the known objective function (which is assumed in Section IV and V) more straightforward? The purpose of “explaining” may not be convincing for many applications.

-  The authors are correct in saying that “TeLEx cannot distinguish between multiple different tight \theta_p”. 

-  The syntax in Section III is not for syntactically co-safe LTL, but for syntactically co-safe parametric LTL as there are temporal parameters [t_1, t_2] in the Until operator.

-   The authors need to significantly expand the related work section by adding more recent results. For example, the following work provides a method to infer temporal logic formulas from positive data only.

Information-Guided Temporal Logic Inference with Prior Knowledge, 2019.

The following work provides a method to infer spatial temporal logic formulas from positive data only (although the classification problem is also discussed).

Graph Temporal Logic Inference for Classification and Identification, 2019.


- Some descriptions need to be more precise. “…and achieve a cost that is optimal to some degree”. It is clear what “optimal to some degree” means.


Minor comments:

-      The axis labels in the figures are very small. The authors should enlarge them. The margins between figures and texts should be increased as well.

-      Symbols such as \hat{\xi_{xu}} are usually written as \hat{\xi}_{xu}.
",,"This paper studies the problem of learning multi-stage tasks given a
set of positive demonstrations from an approximately optimal
demonstrator.

Specifically the paper studies a variant of Apprenticeship Learning/
Inverse Reinforcement Learning (IRL) where parts of the task are described
by a fragment of Linear Temporal Logic.

This follows a growing body of literature studying similar problems to
address the fact traditional IRL (which learns rewards) does not
easily extend to sequential tasks.

The approach taken in this paper is at a high level straightforward,
but technically impressive. The authors propose:

1. Parameterizing the space of cost functions and specifications.
2. Searching for parameters that ""explain"" the demonstrations.

However, like tradtional IRL - or perhaps more so - the notion of
""explains"" is highly degenerate.

To alleviate this, the authors propose searching for parameters that
make the demonstrator ""approximately-optimal"" where optimality is
independently evaluated both at level of low level control and with
respect to the specification.

The low level critera optimality critera is given as the KKT
conditions, which is never fully justified. I must admit, that while
thorough, the exposition in this section was quite terse and at some
points dizzying. Even being somewhat familar with the embedding of
temporal logic constraints as Mixed Interger Constraints, I found this
part hard to follow.

The specification optimality critera later comes in the form of
""minimality"" in the atomic propositions ""visited"". While I'm not
certain, believe that this minimality might correspond to the path
length in the monitoring buchi-automata, subject to dynamic
feasbility.

The ultimate algorithm resembles counter example guided inductive
synthesis. The essential idea is to check if there exists a formula
with a bounded syntatic dag size that is approximately optimal - where
the ""approximate"" comes in the form of a meta-parameter bounding the
distance from optimality. The refuted formula are used to synthesize
counter example trajectories which serve to tighten the concept class.
If no formula of a particular DAG size exists, the DAG size is
increased.

This length based search, which uses a previously establish SAT
encoding of possible parse trees, is both systematic and serves to
regularize the learner, since larger formulas are more expressive -
and thus easier to overfit.

Finally, the paper evaluates this technique on a number of impressive
examples and demonstrates that the addition of goal directed behavior
does indeed output perform a similar techniques for learning from
positive examples.

My primary concerns are:

1. How does this algorithm deal uncertainty in the dynamics. The
   approximate KKL satisfaction seems like a potential solution, but
   doesn't explicitly model agents accounting for risk.

2. Is there a way to measure how ""confident"" the algorithm is in the
   returned result. 

3. Is there a way to robustify this algorithm to mis-labeled or unlabeled
   examples. In particular, it seems to me that incorrectly refuting
   a trajectory in step 10 of Alg 1 could have disastrous consequences
   for learning algorithm, despite ample evidence in the rest of the
   demonstrations. 
",Agreement accepted,"The paper considers an important and challenging problem, and propose a clever framework based on the intuition that human demonstrators are mostly optimal on the continuous part of problems, and sub-optimal on the discrete, combinatorial part.  The technical exposition is sound, and comprehensive. Thus, the contributions of the paper are significant, and will impact research into synthesis of explanations.  There are minor issues with the clarity in some parts of the paper (see below), and some explanations could have been expanded to give a better intuition on the technical approaches.  However, these are perhaps mostly constrained by the page limit.

Minor:
- Is the set of atomic propositions fixed, a priori known (not their parameters)?  It seems to be the case, but it is not well highlighted.  This is an important assumption.
- The temporal logic you consider is not scLTL.  It resembles BLTL [Jha 2009,doi:10.1007/978-3-642-03845-7_15] and STL over discrete signals.  It is a fragment of scLTL/LTL that specifies bounded languages.
- The sentence ""For a length T trajectory, [...] the minimum of $t_2$ and $T-t$."" is ambiguous.  Do you consider semantics similar to finite LTL (i.e., interpret over finite traces) or change the operator's upper time bound?
- There is a simpler encoding part of eq. (7) by taking $M=N_i^{\mathrm{ineq}}$ and $M_\epsilon = 1$.  The constraints become $\mathbf{1}^Ts_i^j \leq N_i^{\mathrm{ineq}} (1+ \mathbf{Z}_{i, t}^j) - 1$ and $\mathbf{1}^Ts_i^j \geq N_i^{\mathrm{ineq}} \mathbf{Z}_{i, t}^j$.  The big-M method is great, but also suffers from numerical problems if it is too high.  It is worth optimizing your constants in constraints.
- Is the size of the DAG (and thus the size of the learned formulae) fixed, i.e. $N_{\mathrm{DAG}}$?  This is an important assumption.
- The explanation around eq.(12) is vague.  What is $\Phi_{u,v}^t$, and why does it depend on time?
- The decision of spec-optimality should be explained.  It is not immediately clear what the definition captures.
- In Fig.3, the formula $\diamondsuit ((p_1 \lor lnot p_1) \land (p_2 \lor lnot p_2))$ is always true since $p_i \lor lnot p_i = \top$.
- You should comment on the bounded-suboptimal assumption as it related to demonstrations of tasks, and why this should hold in practice.  Also, how would you be able to estimate $\delta$, and what range would you expect it to be, i.e., twice, ten or a hundred times the optimal.
- Are the $F_j$ variables in Pb.8 the same as the ones described in Sec.IV.B?
- In Sec.VI: ""both $\theta^c$ and $\theta^s$ *are* known"".
- There is a reference error, assumptions 3 and 4 are in the supplemental document.  However, the reference in the paper should point to assumptions 1 and 2, respectively.
- An extended runtime performance analysis would improve the paper.
",,,,Agreement accepted,07/16 15:00,07/16 17:00,,https://youtu.be/cpUEcWCUMqc,,,97.0,8DPtL1-KeoM
"The theoretical unification of Nonlinear Model Predictive Control (NMPC) with Control Lyapunov Functions (CLFs) provides a framework for achieving optimal control performance while ensuring stability guarantees. In this paper we present the first real-time realization of a unified NMPC and CLF controller on a robotic system with limited computational resources. These limitations motivate a set of approaches for efficiently incorporating CLF stability constraints into a general NMPC formulation. We evaluate the performance of the proposed methods compared to baseline CLF and NMPC controllers with a robotic Segway platform both in simulation and on hardware. The addition of a prediction horizon provides a performance advantage over CLF based controllers, which operate optimally point-wise in time. Moreover, the explicitly imposed stability constraints remove the need for difficult cost function and parameter tuning required by NMPC. Therefore the unified controller improves the performance of each isolated controller and simplifies the overall design process. ",Agreement.pdf,Ruben Grandia (ETH Zurich)*; Andrew Taylor (Caltech); Andrew Singletary (Caltech); Marco Hutter (ETHZ); Aaron Ames (Caltech),Ruben Grandia (ETH Zurich)*; Andrew Taylor (Caltech); Andrew Singletary (Caltech); Marco Hutter (ETHZ); Aaron Ames (Caltech),Agreement.pdf (84383 bytes); RSS_2020_final.pdf (1421519 bytes),RSS_2020_final.pdf,1316.0,98.0,,3.0,Nonlinear Model Predictive Control of Robotic Systems with Control Lyapunov Functions,,0.992134554816248,"This paper develops a way to combine nonlinear model predictive control with control Lyapunov approaches.  The main methodological contribution is on page 4 in Section III, where the authors use the idea in [38] to constrain the first input to satisfy the CLF stability requirement (by selecting the control from the set U_CLF that satisfies the Lyapunov requirement).  The development up until that point is an overview of CLF and nonlinear optimal control techniques.  (The writing in these background sections is exceptionally clear about formulation and what the authors implemented.)  

No proofs are provided regarding what requirements are made on the underlying dynamical system. For instance, if the authors were to apply this technique to the cart-pole system as it goes through the horizontal singularity, they would find that NMPC cannot find a solution because there is no solution that satisfies the CLF requirement at the singularity.  This is not a problem for the implementation of the idea, but it does indicate an unknown restriction on when the technique can be applied.  This lack of information about generality is reinforced in the very simple simulated and experimental system.  With a four dimensional state space, most NMPC techniques would succeed at the stabilization task (but without a priori guarantees from the CLF).  The authors would ideally apply these techniques to a realistic robotic system where alternative approaches start to fail to be realizable in real-time. Even the Segway in 3D motion and with obstacles could be convincing.  

The paper is very well written and very clear, something the reviewer appreciates.  The simulated and experimental outcomes are thoroughly discussed.  This may be one of the few papers I have read that has reproducible results based on the exposition. ",,"The paper is clearly written and, despite having to invoke a fair amount of techniques and results, does a good job at providing a coherent exposition. Minor improvements to the exposition: in Section III.A, it's confusing to call something ""a stabilizing control input"" (because it satisfies the CLF eq at a particular point in time) while at the same time acknowledging that it is in general not stabilizing (because it should satisfy the CLF equation everywhere in time). A better wording convention should be used here; the subindex LLS is never defined (although one can deduce it corresponds to Lyapunov level set?); in Section III.C, the reference output should be y_d, not y. Finally, I was expecting the conclusions to mention the incorporation of safety constraints (a la CBF) into the proposed design.",Agreement accepted,"Overall the paper is of good quality and clearly presented. The problem studied in the paper could have a significant impact on model-based robotic control design. It combines the formal stability guarantees of Lyapunov functions and the optimal performance resulted from the MPC. Moreover, the comparative study presented in the paper provides a benchmark or a guideline for applying NMPC with CLFs for dynamic robotics platforms. 

In nearly all simulation and experimental results, the baseline NMPC-10 controller outperforms all CLF-NMPC controllers both in terms of control and computational performances. Moreover, NMPC-10 satisfy (or slightly violating) the Lyapunov function constraint without explicitly imposing it (as shown in Fig. 3 and Fig. 4). This phenomenon leads me to question whether the CLF constraints are necessary. While the authors argued that the NMPC-10 controller requires tuning of the cost function, they did not explicitly compare the amount of tuning required for NMPC-10 compared to other CLF-NMPC controllers.

Here are some minor comments that the authors should address them in their revision:
1) What is CLF V on Page 5? It is not defined anywhere in the paper.
2) The data in Table I shows the average input norm over a 2 second time horizon. Nevertheless, this comparison may not be fair, as some controllers may require a longer time horizon to stabilize. 
3) Please also show the actual velocity of the segway in Fig 6 (Top). It will show the tracking performance of the proposed controllers, which is an important verification of their effectiveness.",,,,Agreement accepted,07/16 15:00,07/16 17:00,,https://youtu.be/weNv-FlRKiE,,,98.0,cCKv6wuorlY
" We propose a new technique for pushing an unknown object from an initial configuration to a goal configuration with stability constraints. The proposed method leverages recent progress in differentiable physics models to learn unknown mechanical properties of pushed objects, such as their distributions of mass and coefficients of friction. The proposed learning technique computes the gradient of the distance between predicted poses of objects and their actual observed poses, and utilizes that gradient to search for values of the mechanical properties that reduce the reality gap. The proposed  approach is also utilized to optimize a policy to efficiently push an object toward the desired goal configuration. Experiments with real objects using a real robot to gather data show that the proposed approach can identify mechanical properties of heterogeneous objects from a small number of pushing actions.",RSS2020_Author_Agreement-signed.pdf,"[Abdeslam Boularias](http://rl.cs.rutgers.edu), [Changkyu Song](https://sites.google.com/site/changkyusong86/)",Changkyu Song (Rutgers University); Abdeslam Boularias (Rutgers University)*,RSS2020_Author_Agreement-signed.pdf (138879 bytes); ChangkyuRSS2020_camera_ready.pdf (9282496 bytes),ChangkyuRSS2020_camera_ready.pdf,1305.0,99.0,,3.0,Learning to Slide Unknown Objects with Differentiable Physics Simulations,https://sites.google.com/site/changkyusong86/research/rss2020,0.470561427393907,"The paper presents a method for using differentiable physics to perform planar pushing tasks. By utilizing a differentiable formulation of planar pushing equations of motion, the robot can simultaneously push an object to the desired target and identify the internal mass distribution of the object.

The derivations appear to be correct (though I admit I did not check them in detail), and the overall method comes across as well thought-out and very reasonable. The complete system is also logically designed, and evaluated well.

The results seem compelling.

Overall, I think this is a reasonably strong paper.

My main reservations about this work are somewhat higher-level. First, with modern automatic differentiation tools, actually building a differentiable simulator, especially under the restricted settings considered in this paper, is not actually all that difficult. That's not a bad thing -- a contribution does not have to be difficult to be valuable. But I'm finding it hard not to think that the paper somewhat overcomplicates matters with two pages of dense linear algebra. Would it not be enough to describe the forward simulation formulation (which is not new, and based on textbook-standard equations), and then just state that the method relies on differentiating through these equations of motion? The derivatives are complicated, but not hard to automate.

Second, I can't help but think that, though the notion of differentiable physics for robotic control is very promising, in some sense this paper takes the ""easy way out"" by considering a restricted setting where many of the most challenging facets of this problem (establishing and breaking contacts in 3D, non-convex and complex collision shapes, etc.) are removed by construction, and therefore the lessons from this paper might be difficult to generalize to more general 3D manipulation scenarios.

Lastly, I can't help but think that, for the demo that is actually shown in the accompanying video, this method is a bit overkill -- while I really do appreciate the technical aspects of the approach and I think it's interesting and valuable, in the end it enables a robot to push a hammer to the edge of a table. Somehow it seems like tasks like this can be solved more easily in other ways, for example by relying more heavily on feedback as opposed to detailed modeling and system identification. This is of course a bit of a digression, and I do think the paper should be judged on its merits, but perhaps this remark might suggest to the authors that a more convincing evaluation would increase the impact of the work.",Agreement accepted,"Originality
===========
The idea is original. Representing a non-homogeneous object as a collection of homogeneous cubes is not very novel, but the application to estimating friction and mass seems original.

Quality
=======
The theory seems sound, the experiments are convincing.

Clarity
=======
The paper reads OK. The detailed derivations make the paper hard to read. I'd suggest shifting some of them to an appendix and add a few illustrations instead in the main paper. The assumptions and limitations need to be pointed out more clearly. The planning and control approach is also somewhat unclear.

Significance
============
An important step towards efficient system identification for sliding objects.

Major Comments:
===============
- Sect. III: If I understand that correctly, you take the maximum extensions of the object as seen from above and project them down on the surface to get the shape for the cuboids. An illustration would help here. Why do we need a partial 3D view, in the end it seems like everything is done with a single layer of cuboids (so just background subtraction and an overhead camera would suffice).

- I'd make more explicit earlier on that everything is purely 2D, i.e., you don't estimate the height of the center of mass etc.

- Please point the assumptions and limitations of the model out more explicitly. In the end we only have a friction coefficient for Coulomb friction. So the model cannot identify more complex friction effects like stiction, which might actually make a big difference between the careful, small pokes for system identification and the long pushes for moving the object in the end.

- It should also be pointed out earlier that the forces are always parallel to the surface.

- Sect. VI: Is a bit vague, if this is about a policy gradient approach, explain it in RL terms (reward, state, action, policy). Maybe just call it gradient-based optimization. It is also very vague what the gradient actually is.

- Fig. 3: The mass distribution of the book seems rather strange, you'd expect almost no mass on the left half (cover) while the estimated distribution has a clear diagonal structure. This diagonal structure is also present in the other objects. Where does this come from? An artefact of how data is collected?

- Sect. VIII: It would also be nice to include a figure illustrating the results of planning and control, e.g., push arrows around an object that are color-coded according to their quality.

Minor Comments:
===============

- Equation 1 -> Eqaution (1) , use \eqref{}

- Algo 2: the formatting of multi-letter subscripts is very inconsistent (e.g. _waypoint vs. _left), as is the formatting of the variable ""improvement""",Agreement accepted,"This is a beautiful paper. I think the experimental evaluation of contributions (1) and (2) is compellingly shown in Fig 3. The results section is comprehensive, the paper is well written. As far as I can tell contribution (1) is original, but the idea of differentiating a simulator with LCP constraints was recently shown by Amos and Kolter (https://arxiv.org/abs/1810.13400).

It is worth clarifying how high-resolution can we make the 3D voxel grids representing the object, while still being able to estimate the mass and the friction.   ",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,,https://www.youtube.com/watch?v=2LQl5Ibeb0E,,,99.0,BRda93s1HIg
"For robotic arms to operate in arbitrary environments, especially near people, it is critical to certify the safety of their motion planning algorithms. However, there is often a trade-off between safety and real-time performance; one can either carefully design safe plans, or rapidly generate potentially-unsafe plans. This work presents a receding-horizon, real-time trajectory planner with safety guarantees, called ARMTD (Autonomous Reachability-based Manipulator Trajectory Design). The method first computes (offline) a reachable set of parameterized trajectories for each joint of an arm. Each trajectory includes a fail-safe maneuver (braking to a stop). At runtime, in each receding-horizon planning iteration, ARMTD constructs a parameterized reachable set of the full arm in workspace and intersects it with obstacles to generate sub-differentiable, provably-conservative collision-avoidance constraints on the trajectory parameters. ARMTD then performs trajectory optimization over the parameters, subject to these constraints. On a 6 degree-of-freedom arm, ARMTD outperforms CHOMP in simulation, never crashes, and completes a variety of real-time planning tasks on hardware.",RSS2020_Author_Agreement.pdf,"[Patrick Holmes](https://pdholmes.github.io/) [Shreyas Kousik](https://www.shreyaskousik.com/)Bohao Zhang, Daphna Raz, Corina Barbalata, Matthew Johnson,  [Ram Vasudevan](http://www.roahmlab.com/)",Patrick Holmes (University of Michigan); Shreyas Kousik (University of Michigan)*; Bohao Zhang (University of Michigan); Daphna Raz (University of Michigan); Corina Barbalata (Louisiana State University); Matthew Johnson Roberson (University of Michigan); Ram Vasudevan (University of Michigan),RSS2020_Author_Agreement.pdf (668302 bytes); RSS_2020_armtd_final_submission.pdf (5154170 bytes),RSS_2020_armtd_final_submission.pdf,1325.0,100.0,,3.0," Reachable Sets for Safe, Real-Time Manipulator Trajectory Design",,0.41735049408402003,"
Originality
According to the authors, the use of reachability sets had not been used before on arm robots due to the curse of dimensionality. However, reachability sets have been used for other systems, such as quadrotors. e.g., the approach of “Safety control of robots under computed torque control using reachable sets” by Perreira, Althoff seems to be dealing with articulated systems as well. 

Clarity
The paper is difficult to read. The second section is unnecessarily long and introduces terms used two or three pages after that. It would help the readability of the paper to introduce these terms immediately before using them.
The method is divided into two parts, an offline and an online part.The online part seems to be quite computationally expensive. It is not clear how well the method will generalize in setups where small time discretizations may not be possible. The method seems too dependent on selecting small enough time steps to be able to compute the trajectory. If the time step is too small, it seems that the method may not finish successfully. 

In Section III and IV, where the main contribution of the article is presented, there are some issues with the writing. The description could be improved if the Theory and Implementation subsections for each of the steps (e.g., trajectory parameterization and joint reachability sets) are merged. This would greatly improve readability, since otherwise, there is a huge block of general text, definitions, lemmas, etc. that are presented outside the global context of how they fit into the proposed method. The presentation of Lemma 10 could also be moved to Section IV-B2, where it is directly used in the collision-avoidance constraint generation. Additionally, some terms, such as $n_{JRS}$, are used but never explained.

Significance
The paper presents an online approach to achieve safety guarantees using reachability sets for robots arms. The authors claim that the use of reachability sets for this problem is novel although the results do not clearly show that the approach is better than existing ones. Safety is achieved by a similar manner to ICS, where safe maneuvers have to be provided.

An interesting contribution is the parallelized algorithm to construct the reachable sets. This algorithm takes advantage of the GPU used in the experiments although it seems highly dependent on having access to a desirable computational setup in order to achieve similar results to CHOMP.

In the first set of results (with Random Obstacles), CHOMP and the proposed method both find a similar number of trajectories, which reach the goal (82% and 84% respectively), although CHOMP is able to do so in less time. It is said that 18% of trajectories found by CHOMP are infeasible. It is possible that the default settings and straight-line initialization may contribute to this issue. A better comparison would be to tune the CHOMP parameters as well as run CHOMP for the same time (with restarts given different initializations) as the proposed method.  In the trajectory optimization subroutine, ARMTD makes use of arm trajectory waypoints computed by a high-level planner (such as RRT*), which could have been provided as a stronger initialization for CHOMP. At the same time, it is not clear what is the benefit of using a planner like RRT* to provide an initialization if there is no collision checking taking place. It is also not clear what distance metric is used to compute the mean normalized path distance (MNPD).

While describing the second set of benchmarks (Hard Scenarios), it may be good to illustrate the benchmarks in the manuscript rather than relegating this to the supplementary material. Computation times are not provided for any of the Hard Scenarios benchmark.
",,"This paper presents a receding-horizon trajectory planning framework, named ARMTD, for collision-free planning of manipulators with safety guarantees. The receding-horizon planning style is such that the trajectory to split into segments and the algorithm is planning the next segment while executing the prior one. There is an offline and online stage. The offline stage begins with a parameterized continuum of joint space trajectories and then computes parameterized joint reachable sets (JRS) of these trajectories (described in Sec III). Online, ARMTD construct a RS (reachable set) from the JRSs while also considering constraints: obstacle-collisions, self-collisions and joint limits. Given these characterizations, ARMTD solves a trajectory optimization problem, optimizing for a single trajectory under these constraints (Sec IV). The receding-horizon, speed limits and acceleration limits are constructed such that the manipulator can always safely come to a stop if the next segment can't be planned for. 

The mathematical heart of the paper is center around using zonotopes and rotatotopes, as introduced in Sec. II. Sec. II provides a very detailed walk-through of the notation, definition and assumption necessary throughout the approach. Therefore, even if the reader is initially unfamiliar with the concepts, the paper patiently introduces each one. Likewise, the offline and online computations (described in Sec. III and Sec. IV respectively) are divided into a discussion on theory and a discussion on implementation.  This allows the reader to focus their attention towards the aspects they care about and read the paper at different levels. In particular, separating the implementation details led the theory section to be much more understandable. 

The use of seeding ARMTD with a waypoints from a planner was perhaps a more confusing element. Sec. III, which described offline computation, did not detail how existing waypoints would be used to generate the JRSs. Algorithm 3, which fully presents ARMTD, also does not seem to make these waypoints very explicit. Therefore, the discussion of using RRT* to seed the planner in the experimental section caught be by surprise and raised several implementation questions, such as how the waypoints were parameterized and whether the RRT* planning time was included in the results summary. Furthermore, Table II compares ARMTD using a straight line path and a path from RRT* against CHOMP using only a straight-line path. If possible, it seems a full comparison would also include CHOMP seeded with the same path from RRT*. 

Below are several questions with respect to the experimental demonstration: 
- Given that CHOMP is not a receding-horizon planner, why is it the natural planner to compare against? 
- Is there any insight to be gained for why CHOMP fails (or generates paths in collision)?
- The mean solve time (MST) compares one planning iteration of ARMTD to CHOMP's planning of an entire trajectory. Given the differing styles of planning, there isn't an obvious timing metric that is more fair. Would considering overall time (i.e. planning time + execution time) better account for the online planning? And would the general takeaways with respect to planning time be similar? 
- ARMTD ""records"" 84/100 goals in the Random Obstacles setup. What does ""record"" mean? Does this mean that it hits the fail-safe maneuver? If so, what is the equivalent for CHOMP, that it cannot solve the optimization? 


Below are a few other minor questions/suggestions/comments: 
- In Sec II, after definition 4, the text mentions the assumption that obstacles are static. If we had knowledge of where the object is (ignoring the perception issues, as the paper fairly points out, is a different problem) and there were bounds on the object's velocity and acceleration, could the method deal with dynamic obstacles? Several of the demonstrations in the supplementary video seem to confirm that it can. Therefore, it might be helpful to clarify this assumption. 
- The supplementary material contains the full proofs, explains how to account for the self-collision constraint, more formally introduces a few aspects and adds some explanatory details. Sec IV.C was a particularly nice summary that would be great to add to the main text if spacing allows. 
- Fig. 2 was quite helpful to my understanding of the offline and online computation described in Sec III and IV. it might be helpful to more directly reference the figure throughout the theoretical explanations. 
- The introduction categorizes previous work into safety enforced by the path planner, the trajectory planner and the tracking controller. This was an interesting categorizaton and informed a detailed discussion. 
- It wasn't immediately obvious to me why Lemma 13 is a useful property and it might help to briefly elaborate on this. 
- The normalization of path length in the experimental section is a nice method for making an abstract metric more understandable. 
- In definition 5, I believe in defining generators it should be ""g^{1}..g^{p}"" not ""g^{1}...g^{1}""",,"Overall, the work is of high quality and the results are impressive, but the paper is not matching the quality of the work.

While the first part is clear and easy to follow, there are some notions that are not clearly motivated and explained. For instance the slicing is introduced
early in the paper but its objective only explained later. As the paper introduces many notions, it is hard to get the global picture of how everything works together.
I would recommend producing a figure detailing the offline and the online step to provide a gentler introduction to the method.

A particularly annoying point concerns the supplementary document that seems to be simply a previous version of the paper. This strikes me as a demonstration
of laziness; how hard was it to simply extract the proofs and relevant information from the document and present them on a reduced file ?
The sections are not even properly refered in the paper. This is clearly not acceptable.


This being said, again I think the work is of high quality.",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,https://github.com/ramvasudevan/arm_planning,https://youtu.be/ySnux2owlAA,,,100.0,6tjnh1Yxr_Q
"This paper presents a reinforcement learning approach to synthesizing task-driven control policies for robotic systems equipped with rich sensory modalities (e.g., vision or depth). Standard reinforcement learning algorithms typically produce policies that tightly couple control actions to the entirety of the system's state and rich sensor observations. As a consequence, the resulting policies can often be sensitive to changes in task-irrelevant portions of the state or observations (e.g., changing background colors). In contrast, the approach we present here learns to create a task-driven representation that is used to compute control actions.  Formally, this is achieved by deriving a policy gradient-style algorithm that creates an information bottleneck between the states and the task-driven representation; this constrains actions to only depend on task-relevant information. We demonstrate our approach in a thorough set of simulation results on multiple examples including a grasping task that utilizes depth images and a ball-catching task that utilizes RGB images. Comparisons with a standard policy gradient approach demonstrate that the task-driven policies produced by our algorithm are often significantly more robust to sensor noise and task-irrelevant changes in the environment.",RSS2020_Author_Agreement_Form.pdf,Vincent Pacelli (Princeton University)*; Anirudha Majumdar (Princeton),Vincent Pacelli (Princeton University)*; Anirudha Majumdar (Princeton),main.pdf (3785909 bytes); RSS2020_Author_Agreement_Form.pdf (75459 bytes),main.pdf,1212.0,101.0,,3.0,Learning Task-Driven Control Policies via Information Bottlenecks,,0.09448027446897699,"Originality:
The problem formulation is not novel [A]. The motivation that such methods are robust to changes in the environment has also been studied [B].
The method itself uses [4], but the policy gradient formulation is original, including differentiating through the MINE and stabilizing it with EMA.

Quality:
The paper is quite well written.
Issues:
- Despite the recent trend to call every trade off with information rate ""information bottleneck"", the latter refers to a specific trade off between two information quantities [44]. Eq. (3) uses instead the much earlier concept of rate–distortion [C], and particularly sequential rate–distortion [D], although the approximation that x_t and y_t are independent of phi loses the sequential nature.
- The equation for pi (Section II) is confusing, because the LHS gives the impression that the policy has no memory. It is also inaccurate, because the RHS omits the dependence between \tilde{x}_{t-1} and y_t.
- It is unclear what is gained by Theorem II.1. Is the paper claiming that the RHS of (6) is a good proxy for its LHS? But the LHS is not our objective, because of the very restrictive (5) (which is made increasingly restrictive by minimizing I[x_t, \tide{x}_t] ).
- In what sense is (3) a ""first-order approximation"" of (6)? They coincide when the beta of (3) is 1 and that of (6) tends to 0, but can otherwise be very different.
- How much is performance improved by having time-variant theta, phi, and psi?
- Prior work solves the problem optimally in the linear–Gaussian case [E]. Since the domain in example IV.A is linear, the paper should compare the proposed method with the optimal solution.
- In all experiments, instead of fixing beta to an arbitrary value (which one?), it would be useful to show a curve of the value as a function of beta. This will also reveal different phases of qualitatively different control behaviors.
- The reported standard deviation is presumably over the variability of the domain. No error bounds on the mean estimation are given, making it hard to evaluate statistical significance.
- Presumably the method encourages completely ignoring features that are completely task-irrelevant. However, in Table II, it performs extremely poorly on several backgrounds, which suggests that this is not the case. No explanation of this is provided.

Clarity:
The paper is very clear.


[A] Information theory of decisions and actions, Tishby and Polani, Perception–Action Cycle, 2011
[B] Trading value and information in MDPs, Rubin et al., Decision Making with Imperfect Decision Makers, 2012
[C] Elements of information theory, Cover and Thomas, 2006
[D] Control of LQG systems under communication constraints, Tatikonda et al., CDC 1998
[E] Minimum-information LQG control part ii: Retentive controllers, Fox and Tishby, CDC 2016
",Agreement accepted,"The authors address the important policy of learning policies that generalize to novel environments and conditions. To do so, they propose a reinforcement learning algorithm (TDPG) which learns a policy operating on a state representation that is simultaneously useful for the control task at hand while containing as little information as possible from the sensory input. Specifically, the learned policy consists of two parts: a conditional distribution q(xb_t | xb_{t-1}, y_t) that is used to perform bayesian filtering over the latent state representation xb, as observations y are observed; and the distribution \pi(u_t \mid xb_t) which defines a distribution over actions given a latent state. Reasoning about the latent state explicitly allows directly minimizing the mutual information between the true state x and the task representation xb. In this way, the resulting policy is robust to changes in task-irrelevant aspects of the input, such as the background color or texture. Furthermore, the policies learned try to accomplish the task while minimally relying on the sensory input. The key technical tool used to accomplish this is to create an information bottleneck by explicitly minimizing the mutual information between the learned state representation and the sensory input while optimizing the policy.

The paper is well written and clear. The proposed approach is very well motivated, and the proposed algorithm seems like a good approach to solving the problem statement. I found the related work section to be quite thorough yet concise and to-the-point, providing the right level of background necessary to understand the author's contributions. Below are my main concerns with this work:

Connection to entropic risk:
- I found the presentation of the objective (3) from the perspective of mutual information to be clearer than the viewpoint as a first-order approximation on the bound of the expected cost on a different distribution. The notation is (6) is somewhat unclear, but I assume that the expectation is taken under some distribution pc_t(\tau) = \prod_{t=0}^T pc_t(x_t, xb_t, u_t), where each pc_t satisfies equation (5). My main confusion stems from how I am supposed to interpret (5). The general idea of minimizing the cost under a worst-case choice from a set of possible distributions makes sense for robustness, the set of distributions defined by (5) is not intuitive. The authors should spend more time explaining how optimizing (6) is intuitively the right thing to do. Right now, that motivation is unclear, and since the true objective is only an approximation to (6), Theorem II.1 and the connection to entropic risk adds little to the motivation of the paper.


Scalability:
- The approach requires training a neural network to compute a KL divergence for each time step, all in order to perform a single gradient step on the policy networks parameters \phi and \psi. The algorithm is only tested on problems with limited state space size (max dimension 5), and limited time horizon (max 25, and only 1 on the grasping problem). Furthermore, the training seems to require significant tuning of hyperparameters such as the EMA coefficient \alpha and the number of training epochs of MINE per outer gradient step. These factors lead me to question how easily the suggested algorithm will generalize to more complex control tasks and higher dimensional systems.
- Furthermore this requires training with access to the underlying state, which may be hard to access in certain domains. Can this same approach be extended to operate on image observations directly, by minimizing the mutual information between the image observations and the learned representation? It seems to me that this directly handles the issue of preventing distractors such as background textures being a factor in the learned policy. What is the main bottleneck preventing this? Scalability of MINE? The paper would be strengthened with a discussion of this point.",Agreement accepted,"As mentioned above, I believe that the topic of the paper is very important, and I find the proposed method very interesting. The paper is well written and mostly clear.
There are however a few issues and open questions in my opinion: 

a) For the proposed method, access to the underlying state is needed at training time. However, if we have access to the state, why would one use the observation (e.g. image) instead of the state directly? It would be good if the paper could give some realistic examples.
In fact, it is not clear to me why, for the arguments in this paper, there needs to be a separation between state and observations. Couldn't it just consider fully observable settings, where part of the state is irrelevant to the task?

b) Somewhat related, it is not clear to me how having less information about the latent state helps with being robust to changes in the observation (which do not affect the state). E.g. in the ball-catching experiment, the policy is shown to be robust to changing background. However, the background is not part of the state, hence there is no reason the proposed information bottleneck should have encouraged invariance to the background.

c) Related to this, why would one not place the information bottleneck between policy and observation, rather than state. Wouldn't this encourage to get rid of irrelevant information? Wouldn't this have the additional benefit of not requiring access to the hidden state?

d) All the experiments are very small and constructed. It would be very nice to have one more experiment which is a bit more standard or realistic. (and where it is well-justified to have access to the state at training time but not at test time, see a)

e) Finally, there are a few minor issues concerning clarity:

e1) The term task-driven is used throughout the paper to describe the proposed method. In my opinion this term is not clear or even misleading. I think it would be better if the paper used a term which describes more clearly the main idea of using only relevant information. Or otherwise the term should at least be defined at the very beginning of the paper.

e2) Theorem II.1 comes somewhat out of the blue and needs more explanation. How does this theorem exactly relate to this paper? It needs to be clear which term in the theorem corresponds to which term in the paper, and why the statement of the theorem is relevant. From what I understand, \check{p} can be any distribution, so it might correspond to an entirely different policy. How does this theorem hence tell us something about the robustness of the policy? Also, from (5) it seems that with higher mutual information (rhs), we can tolerate a larger shift in the distribution (lhs).
Finally, in the end the paper does not use the objective function suggested by the theorem. Therefore, the theorem seems currently somewhat decorative to me. So the paper should either establish a clear relationship or remove the theorem.

e3) After equation (15), the paper states that some assumptions are made: x_t, y_t are independent of phi and x_t, y_t, \tilde x_t are independent of psi.
It would be good to go into some more detail about this. Why is this necessary, couldn't we do a similar trick as in (12), (13) to get the derivative? What error are we making by these assumptions? Why can we expect the error to be small?

e4) Could the paper make computing the gradients of the mutual information term more explicit? I.e. after (15), the paper should go into more detail.

e5) Experiments: Am I understanding correctly that for the baseline everything is identical, except that the mutual information term in the cost is missing?",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,https://github.com/irom-lab/trc-nn,https://www.youtube.com/watch?v=Mwv0kkRveas,,,101.0,nzLyRHON24E
"Enabling robots to learn tasks and follow instructions as easily as humans is important for many real-world robot applications. Previous approaches have applied machine learning to teach the mapping from language to low dimensional symbolic representations constructed by hand, using demonstration trajectories paired with accompanying instructions. These symbolic methods lead to data efficient learning. Other methods map language directly to high-dimensional control behavior, which requires less design effort but is data-intensive. We propose to first learning symbolic abstractions from demonstration data and then mapping language to those learned abstractions. These symbolic abstractions can be learned with significantly less data than end-to-end approaches, and support partial behavior specification via natural language since they permit planning using traditional planners. During training, our approach requires only a small number of demonstration trajectories paired with natural language—without the use of a simulator—and results in a representation capable of planning to fulfill natural language instructions specifying a goal or partial plan. We apply our approach to two domains, including a mobile manipulator, where a small number of demonstrations enable the robot to follow navigation commands like “Take left at the end of the hallway,” in environments it has not encountered before.",RSS2020_Author_Agreement_filled.pdf,"[Nakul Gopalan](https://nakulgopalan.github.io/), [Eric Rosen](http://cs.brown.edu/people/er35/), [George Konidaris](http://cs.brown.edu/people/gdk/), [Stefanie Tellex](http://cs.brown.edu/people/stellex/)",Nakul Gopalan (Georgia Tech)*; Eric Rosen (Brown University); Stefanie Tellex (Brown University); George Konidaris (Brown),language_to_symbols_camera_ready.pdf (1368985 bytes); RSS2020_Author_Agreement_filled.pdf (63430 bytes),language_to_symbols_camera_ready.pdf,1294.0,102.0,,3.0,Simultaneously Learning Transferable Symbols and Language Groundings from Perceptual Data for Instruction Following,,0.7944012526749921,"STRENGTHS
- This paper makes a major contribution to the field of learning symbols for robot actions and mapping verbal instructions to the actions.
- The authors describe their approach in depth with a lot of technical detail.
- The evaluation of the approach is very thorough.

WEAKNESSES
- The only weakness I can see in this paper is that it does not discuss the limitations of the approach. It would be good to add a discussion about the shortcomings of the new approach and future work that needs to be done to improve on the limitations.

SUGGESTIONS FOR IMPROVEMENTS
- I saw some minor typos, but those can easily be fixed by proofreading the paper one more time before submitting the camera-ready version.

ORIGINALITY
This paper presents original work.

QUALITY
The presented approach, the evaluation, and the writing is of very high quality.

CLARITY
The paper does a lot of technical details, but does require a lot of different background knowledge. I do understand that the authors probably had to cut down a lot of text to fit into the page limit of the conference, however, it would have been good to add to introduce some of the technology used to make the text easier to understand. The paper is structured and written well overall, but it is quite hard to understand.

SIGNIFICANCE
This paper is highly significant and makes a great contribution to the research in this area.",Agreement accepted,"The paper describes a very interesting approach to instructing a robot. This focuses on navigation on a mobile manipulator and an autonomous car. The paper is well presented and I think that the significance of the work is high enough to be published at this venue. 
I have a few concerns that should be addressed for the final camera-ready version.

Originality: While I believe that the combined approach is original and highly interesting, I struggled identifying which components are a contribution of the paper and which components have been reused from other publications. Please, clearly state the contribution of this paper when describing the system. Currently, it is dominated by a list of all the approaches that have been combined to make this work.

Clarity: While I agree to a certain extent that your approach works in novel environments, they are not entirely unknown or novel to the overall system since your approach requires a pre-existing map. Please, make clear that the ""novel/new/unknown"" refers to environments that you have not trained on but that a map is known a-priori to the robot navigation system.

Please, explain how your complex sentence example including mentioning an art painting and a fountain relates to what has been learned. In fact, the art painting and the fountain should be ignored as the system will only pick up on left, end, and right keywords as far as I understand it.

For future work, it would be great to see how this works with spoken language. This might need to include some more sophisticated NLU apart from Seq2seq. 

Minor things: the abstract should also mention the car example, the paper requires minor proofreading.",Agreement accepted,"The paper proposes a framework that automatically learns the latent space of symbols from a set of demonstration trajectories and then uses accompanying textual instructions to train a sequence-to-sequence architecture that ""translates"" language to the sequence of learned symbols. The termination sets of option-like skills constitute the symbol space, the size of which is not known a priori. The framework uses a non-parametric Markov model (MDP-HMM) to segment action sequences and identify the corresponding skills. The corresponding termination sets, as represented in terms of the robot-centric observations (LIDAR), are clustered and used to train a single-class SVM classifier for each cluster. The corresponding policy can be learned or planned when the dynamics are known. The framework then trains an encoder-aligner-decoder neural sequence-to-sequence model to ""translate"" language to the sequence of symbols (termination sets/classifiers). The method demonstrates the symbol-learning and language understanding components on an existing driving dataset and evaluates the full system on a mobile manipulator.

The use of a symbolic representation of the robot's state and action space is commonly used as an abstraction between natural language and the robot's low-level action space. As noted, exceptions include recent sequence-to-sequence models that map language directly to low-level actions, which incur significant sample complexity. Particularly promising with this paper is that it proposes simultaneously learning the space of symbols and the language understanding model directly from demonstration trajectories paired with natural language instructions. While this is not new (e.g., see the work of Kollar et al., 2013; Thomason et al., 2016), it is certainly the minority among existing work in grounded language acquisition for robots.

The paper is on its way to making a real contribution to the community, however there are several limitations of the current framework that need to be addressed:

* The ability of the framework to scale to diverse tasks, environments, and language is questionable. The discussion and results consider a simple navigation task that consists of going straight down a hallway (or roadway) and turning. The tasks involve no more than five (or three? see below) symbols and an output sequence length of at most five, yet segmentation, clustering, and classification require a fair bit of data cleanup and parameter tuning. Meanwhile, the extent to which the symbols generalize to novel environments is a function of the generalizability of the classifiers and underlying policies, which depends on the diversity and size of the training data. Scalability becomes even more of an issue as the size of the action space increases (e.g., to include joint torques, which the paper suggests are considered here). Meanwhile, sequence-to-sequence language models are well known as being sample inefficient and while the the limited length of the output sequence alleviates some of the sample complexity, scalability will be a problem as the number of symbols increases. Unfortunately, the relative simplicity of the navigation tasks and, in turn, the limited diversity of language, that are considered doesn't alleviate this concern.

* In its current form, symbol learning is decoupled from language understanding in that, while the trajectories are accompanied by textual instructions, the instructions are not used to to discover the set of skills. Instead, the framework uses existing skill discovery methods, and then trains a vanilla sequence-to-sequence model to learn to map language to symbol sequences. Testament to the limited role of language is that the discussion of the language understanding component of the framework is limited to half of a column.

* The experiments are limited in terms of complexity and lack a compelling baseline as well as a thorough quantitative evaluation. The NPS dataset is nice in that it provides a demonstration on ""real-world"" data, but it requires a fair bit of post-processing and significantly restricts the length of sequences. The KITTI dataset, which is widely used in the vision and robotics communities, would have been a more appropriate testbed as it provides the relevant action data directly. Further, the NPS evaluation is rather limited, considering only three behaviors, which are essentially the self-driving version of those used for the on-robot experiments, and a maximum output sequence length of two.

The on-robot experiments are similarly limited, involving behaviors that are very similar to those above, short sequence lengths, and *only* three test instructions. It is impossible to assess the merits of the approach with such a restricted evaluation. An user experiment with dozens of instructions would not be difficult to perform.

Meanwhile, the quantitative evaluation is limited to the accuracy of the language understanding model, which isn't particularly compelling given the small symbol space and sequence length and the fact that the language understanding component is a vanilla off-the-shelf seq2seq model. Like other work in language understanding, the paper should provide a more detailed quantitative evaluation, e.g., the fraction of time the robot stopped within a fixed distance from the correct destination (or a similar path-focused evaluation) for a dozens of different instructions; or an evaluation of how far the robot is from the destination if it isn't reached.


COMMENTS

* It is not apparent why termination sets are sufficient to define the set of skills. Most of the skills that are referenced (straight down hallway, turn left/right) are functions of the path segment and not simply the end point. It is for this reason that many of the symbols used by existing approaches to robot language understanding reason over paths.

* The paper should clarify claims regarding the framework's generalizability and the lack of a dependency on simulation. The extent to which the method generalizes to novel environments is determined by the generalizability of the the classifier and underlying policy. However, the performance of the classifier is a function of the features that are provided and the diversity of the training data, though even then DNN-based classifiers exhibit far better generalizability. Meanwhile, there is little discussion of how policies are represented (other than that DPMS can be used), while learning policies that generalize to different environments would require significant on-robot or simulation-based data. Unfortunately, the experimental evaluation lack sufficient evidence regarding environment variability to justify these claims.

* In light of the above comments on scalability, the paper oversells the data efficiency of the framework. The ability to learn skills from 75 demonstrations is likely due to the simplicity of the navigation task, which can be described by a handful of symbols. Similarly, the diversity of the language, which is limited by the simplicty of the domain, would explain the fact that the sequence-to-sequence model can be trained with so few sequence pairs.

* Does the seq2seq output include a stop token or is the output sequence length predetermined? The statement that the maximum sequence length for the on-robot trials was three to five suggests that there is, but elsewhere the text implies that the length is predefined.

* For the NPS evaluation was the segmentation learned (with the number of behaviors fixed to three) or were the trajectories segmented by hand (as suggested by the text at the end of Section IV.A).

* For the on-robot experiments, my reading is that three symbols were used, but that one symbol included two clusters/classifiers. Is that correct?

* How similar were the three instructions that were used for the on-robot experiments to those seen during training?

* The sentence that is given near the end of page 7 is compelling, but I can only assume that the method is relying on the ability to detect intersections. Presumably the robot was positioned in the hallway at the start, and thus there was only one intersection and presumably the method would fail had there been perceptual aliasing. What about the water fountain? It is not referenced above as a learned endpoint.

* Clarification is needed with regards to what is assumed to be known a priori of new environments. If the policy associated with each skill is not learned (which would require significant on-robot training and/or simulation-based training, both of which the paper emphasizes as not necessary), then I would presume that the robot has access at the very least to a occupancy-grid (or similar) map. This is in contrast to some existing work that does not assume the environment is known, and instead uses an ""exploration"" policy that is given (Matuszek et al., 2012) or learned (Hemachandra et al., 2015).

* Related, if a map is assumed to be known, at what spatial and angular resolution does the framework search over candidate termination locations?

* For the data collection, how familiar were subjects with the environment? People unfamiliar with the space are likely to give more generic instructions than people who know the environment well.

* The notation in the first equation on page 4 is confusing.

* The paper states that torques constitute the action space, however the on-robot trials use longitudinal velocity and angular rate, and the NPS evaluation considers direction and distance of motion. There are also references to using camera data in place of LIDAR for termination set classification, however training a classifier that reasons over image data would be far more sample complex.

* It would be nice to see a discussion of the implications of performing skill discovery only over actions rather than actions and states.

* The paper should discuss the robustness of the clustering and classification methods, e.g., to the choice of the minimum cluster size, the noise parameter, and the threshold for merging clusters.

* How is inference performed over the output of the sequence-to-sequence model? Is beam search used?

* How are the hyperparameters chosen?

* What ensures that the symbols are interpretable as stated in the conclusions?


REFERENCES

Felix Duvallet, Thomas Kollar, and Anthony Stentz. ""Imitation learning for natural language direction following through unknown environments.” ICRA 2013.

Sachithra Hemachandra, Felix Duvallet, Thomas M. Howard, Nicholas Roy, Anthony Stentz, and Matthew R. Walter. ""Learning models for following natural language directions in unknown environments."" ICRA 2015

Thomas Kollar, Jayant Krishnamurthy, and Grant Strimel. ""Toward interactive grounded language acquisition."" RSS 2013

Cynthia Matuszek, E. Herbst, Luke Zettlemoyer, and Dieter Fox. “Learning to parse natural language commands to a robot control system.” ISER 2012

Jesse Thomason, Jivko Sinapov, Maxwell Svetlik, Peter Stone, Raymond J Mooney. ""Learning Multi-Modal Grounded Linguistic Semantics by Playing 'I Spy'."" IJCAI 2016",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,https://github.com/nakulgopalan/change_point_detection.git,https://vimeo.com/388650000,,,102.0,JZbc8cLG3dA
"Formation of subgroups and thereby the problem of intergroup bias is well-studied in psychology. Already from the age of five, children can show ingroup preferences. We developed a social robot mediator to explore how a robot could help overcome these intergroup biases, especially for children newly arrived to a country. By utilizing an online evaluation of collaboration levels, we allow the robot to perceive and act upon the current group dynamics. We investigated the effectiveness of the robot's mediating behavior in a between-subject study with 39 children, of whom 13 children had arrived in Sweden within the last 2 years. Results indicate that the robot could help the process of inclusion by mediating the activity. The robot succeeds in encouraging the newly arrived children to act more outgoing and in increasing collaboration among ingroup children. Further, children show a higher level of prosociality after interacting with the robot. In line with prior work, this study demonstrates the ability of social robotic technology to assist group processes. ",RSS2020_Author_Agreement_filled_signed.pdf,"[Sarah Gillet](https://www.kth.se/profile/sgillet), [Wouter van den Bos](http://bits-of-information.org/DDN/), [Iolanda Leite](https://iolandaleite.com/)",Sarah Gillet (Royal Institute of Technology)*; Wouter van den Bos (University of Amsterdam); Iolanda Leite (KTH),RSS2020_Author_Agreement_filled_signed.pdf (223685 bytes); RSS___Robot_Mediator_Fostering_Collaboration_v7_camera_ready.pdf (3591183 bytes),RSS___Robot_Mediator_Fostering_Collaboration_v7_camera_ready.pdf,56.0,103.0,,3.0,A social robot mediator to foster collaboration and inclusion among children,,0.75591365626606,"There is a lot to like about this research and this paper submission. The authors tackle an important problem of out-group social behavior, and they do so with an autonomous system. They also work with children, a user population that is difficult to recruit and study.  All of this is commendable. Furthermore, the paper is very well written and easy to follow. The descriptions are for the most part complete and precise. Overall I really liked this work. 

I want to point out in particular the great Introduction. It provides a good motivation and succinctly conveys the most related work and how it motivated this research. 

There are a few shortcomings that make this paper somewhat incomplete. The main ones are:

1) The authors to not position their work with respect to a specific gap related work. It is not clear what is missing in the state of the art and thus what the unique contribution of this work is. 

2) As a result, the music-based puzzle seems somewhat out of the blue and it is not motivated well. There is some hint in Section   III (""We aimed for creating a task...""), but a better description of why this is a good test case for the issues studied is necessary.

3) Most importantly, perhaps, is that it is never quite clear what the game is that the children play with the robot. What is the puzzle they are solving? What is considered a solution. Without this information it is hard to evaluate the validity of the rest of the findings. What would cause a child to choose one action over another is completely opaque in this description. 

4) The hypotheses should be presented more clearly and motivated. The word ""more"" is not clear - more compared to what? Also, an explanation of where the hypotheses come from is missing. 

5) Why did the authors need an additional measure beyond the in-game logs? Why is the dictator game important? Couldn't authors find the pro-social behaviors within the game? 

6) Finally, I found most of the effects quite small and some going in the counter-hypothesized direction. In that light, the authors' discussion as if the hypotheses were confirmed seems overreaching. Perhaps a more focused experiment on just the out-group socialization with a baseline of in-group behavior would have been a clearer thing to evaluate. 

Some smaller issues include: 

1) The radar graphs are not a very clear way to show the effects, as they mix the geometric metaphor with the scale. It took me a long time to parse what to read in these graphs. 

2) The description of the autonomy in overly algorithmic and mathematical terms deducts from the readability of the paper. The same behaviors could have been described more concisely and more clearly in prose. 

Overall, this is very interesting research, which clearly took great effort and is in the right direction, but it requires some more clarity to be useful as a published paper. ",Agreement accepted,"The authors present an autonomous robot designed to assess and mediate collaboration in a children's musical puzzle activity. They also present the experimental protocol and results for a user study designed to test the effectiveness of the robot; claiming that the results show the robot was successful in encouraging newly arrived children to be more outgoing and increasing collaboration between the native children. I think the work presented is original, well situated in the literature and that the robot application/use case and exeperimental protocol is generally very well considered and justified, and that the system has potential for real-world impact. I would definitely like to see the code required to replicate the study made available online such that others could utilise the same protocol for investigating the impact of robots on child group dynamics. However I have some concerns about (1) the design of the control condition robot and (2) presentation/analysis of the results with regards to the hypotheses and claimed findings. 

On (1) the control condition was designed such that the robot would pick a child at random and randomly suggest placing a cube in one of their own zones versus one of the other children's zones with a 50:50 chance - such that half of such suggestions referred to a child's own zone and half referred to another child's zone. Considering these suggestions, the authors well justify why the robot should not refer to/suggest an out-group child's zones more than those in the in-group. However, they do not justify why it makes sense for the robot to suggest a child's *own* zones 50% of the time, given that (i) the primary purpose of the robot is to foster collaboration and prompt children to use others' zones, (ii) the authors note multiple times that pilot testing demonstrated children highly favoured putting cubes in their own zones. Given that the mediator-robot condition resulted in 75% of suggestions being associated with another child's zone, it raises the question of whether the control robot, still picking children at random rather than according to the online collaboration assessment but suggesting explorative actions at a rate higher than 50% might have had similar impact to the mediator condition. 

On (2) I appreciate the authors opting not to run any statistical analysis and instead presenting raw results with interpretation and discussion. Broadly, I agree that the results do suggest the system has potential for having the desired impact on children's group behaviour, but I don't think this is well established in the results/discussion as it currently reads and would offer the following feedback:

- In Section VI-B Equal participation: the authors state, in the first round of the game that variability was lower (listing mean and standard deviation values) for mediation condition groups than the control, therefore suggesting a positive result for the mediation condition. However the reverse is true for the second round of the game (in which no robot is present) with the mean variability of mediation groups actually increasing almost to match that of the control group in round 1. This makes me therefore question the claim made in the results section that 'results show a trend towards the robot being able to achieve more equal participation in the first round...[which] does persist for the majority of groups in the mediating condition'. A closer look at Figure 6 suggests that in round 2 of the game 1/4 of the mediating groups was a lot less equal (much higher COV #placements), another was moderately less equal increased and 2/4 groups were almost the same/slightly more equal - so I'm not sure how this suggests a persistent effect. The first round results do provide initial evidence to support H1 (with the mediation groups being all having more equal participation than 3/4 of the control groups) but I think this is all that can be said at this point.

- In Section VI-C Exploration of distant zones: the authors state that 'the mediation condition shows a trend towards more frequent placement of cubes in the 6 distant zones' - I think any difference between the control and mediation conditions is really hard to see on the radial plot figures presented (which really just overhwhelmingly show much children tended to put cubes in their own zones in both cases) and therefore tabluated raw numbers covering the number of placements in own versus in versus out group member zones might be clearer. The lack of difference between control and mediation condition results is acknowledged by the authors in the discussion, but they still suggest that evidence for H2 is given by 'out-group and in-group children in the mediating condition, especially in the second game round, placed their cubes more often in the distant zones' - this ambiguity combined with the difficulty of visually seeing a real difference (in either game round, in my opinion) in the figures provided, make this somewhat unconvinving. 

- In the discussion the above then becomes particularly an issue because the authors use the above addressing of H1 and H2 to claim that the robot's mediating behvaiour had an 'important role towards a fast inclusion of the out-group member', but they later state that actually whilst the out-group children typically showed an increase in visting distant (in-group members) zones, this was not returned by the in-group children, although the in-group children did show an increase in visiting *other in-group* members distant zones - which seems potentially the opposite of what you are trying to achieve! As such, I think all that can really be said that the online assessment of collaboration/equal participation did seem to work (based on the output actions of the robot, which could be presented as a result in themselves) and it did prompt the out-group children to approach the in-group ones, but the increased collaboration between in-group children (with no such result for in-group to out-group) makes me very much question the statement regarding 'fast inclusion of the out-group member'. 

- Again in the discussion the same issue is true for H3 which the authors initially suggest is supported by H1 and H2, when surely H3 should only really linked with the sticker allocation measure designed to measure prosocial behaviour. In general I think that the link between experimental measures and how they relate to the hypotheses should be much clearer, because at this point this section just feels quite vague/ambigious and potentially actually discredits what positive conclusions might be taken from the results presented. 

In summary, I think the technical implementation of the robot, the experimental protocol and the application use case is well thought out and I think the protocol could certainly be of use to other researchers in the field. Autonomous HRI studies in the wild are always to be applauded and this work targets a complex use case with real potential for impact, and whilst I do think the results show some benefit of the system, I feel they are not well discussed and could be presented more clearly to really do the work credit. ",,"This paper introduces an integrated robotic game and setup. It is a mark of distinction that the experimenters were able to run a user study with actual children from their target population (those in Sweden <2 years). Generally, the research is motivated and presented well well. The authors present a solid and up-to-date overview of recent HRI work on robots promoting/influencing group dynamics. The system description and task are presented clearly. The interaction apparatus strikes me as a good example of interactive group child-robot system design: the game is creative, interesting, appears to be engaging, and is accompanied by a clear algorithmic description of robot behavior.

It is somewhat odd to present 3 clearly stated hypotheses for the experiment, and then either not conduct or not present results of statistical tests, which are generally considered the appropriate scientific way to determine whether the experimental hypotheses are supported by the data. I understand that recruiting sufficient participants (particularly when working with a unique target population, which, as noted earlier, is a strength of this work), but in this case, perhaps the standard approach of listing and evaluating hypotheses framed in the form of H1, H2, and H3 is not the right course. 

That is to say, rather than couching inconclusive results in vague statements that lack mathematical support (e.g., ""results show a trend towards the robot being able to achieve a more equal participation during its presence in the first game round"", among others), the authors might consider accepting that the data as they are presented do not really answer such questions with much clarity or certainty. 

Instead, I recommend the authors focus on the many other valuable aspects of the work. What can HRI researchers, in particular those working with groups of children, learn from this work? The design of the interaction and game seems like a good starting point: what was the process like for developing this system and interaction? The game appears to be successful in engaging groups of children for at least a short while, what changes could be made in future work to encourage more clear demonstrations of in-/out-group co-operation? I am quite certain there are valuable reflections for researchers that can be shared as part of this work; given the small sample size of the experiment, I do not think that the currently presented behavioral hypotheses and measures are the most relevant or interesting parts of this project.

Questions:
How much the system relies on Wizard-of-Oz is unclear, yet important to the overall assessment of the system: how often did the Wizard have to intervene?

Minor Points:
Abstract: ""Results indicate the robot accomplishes to address the inclusive aspect of the group..."" awkward wording

Introduction: ""Today's society is increasingly polarized with growing impressions of 'us' vs 'them'...."". Such a claim deserves a citation.

",Agreement accepted,,,Agreement accepted,07/16 15:00,07/16 17:00,,,,,103.0,nOqDxJxizis
